{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjVyBhNhpC2WMAT2SRbkwY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinsooKwak/RAG/blob/main/test/chunking/chunking_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- langchain 제공\n",
        "  - CharacterTextSplitter() : 문자 기반\n",
        "    - Tiktoken : 토큰 단위\n",
        "  - RecursiveCharacterTextSplitter() : 재귀 + 문자 기반\n",
        "  - TokenTextSplitter() : 토큰 수 기준 분할\n",
        "- Semantic chunk"
      ],
      "metadata": {
        "id": "BpoSlhFAY0qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 청킹시 고려사항\n",
        "  1. 문맥 손실\n",
        "  2. 청크 경계 처리 : 청크 나누는 과정에서 문장 잘리지 않도록 주의\n",
        "  - 해결 방안 : 청크 경계가 자연스러운 언어 단위(문장, 문단)에 맞춰지도록 조정하고, 필요한 경우 청크간 중복 일부 허용"
      ],
      "metadata": {
        "id": "ePOBemDDiWQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Arxiv 문서 로더 > 설치  # pymupdf : arxiv.org 사이트에서 다운로드한 PDF 파일을 텍스트 형식)\n",
        "#!pip install -qU langchain-community arxiv pymupdf"
      ],
      "metadata": {
        "id": "ygLrJ4FPGSrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF 가져올 때\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "\"\"\"\n",
        "# PDF 가져오기\n",
        "loaders = [\n",
        "    PyPDFLoader(\"\"),\n",
        "    PyPDFLoader(\"\"),\n",
        "]\n",
        "\n",
        "docs2 = []\n",
        "for loader in loaders:\n",
        "  docs2.extend(loader.load())\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "InlTfrcQecuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import ArxivLoader\n",
        "\n",
        "# Query 에 검색하고자 하는 논문의 주제를 입력\n",
        "loader = ArxivLoader(\n",
        "    query=\"Agent\",\n",
        "    load_max_docs= 10,              # 최대 문서 수\n",
        "    load_all_available_meta=True,   # 메타데이터 전체 로드 여부\n",
        ")"
      ],
      "metadata": {
        "id": "2oqUfBAxGoOy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서 로드 결과출력\n",
        "docs = loader.load()\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRlE-zEpG1az",
        "outputId": "8df0cbaf-a206-4460-a8cd-41122150668a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.', 'entry_id': 'http://arxiv.org/abs/2406.14228v2', 'published_first_time': '2024-06-20', 'comment': 'Work in process', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.AI', 'categories': ['cs.AI'], 'links': ['http://arxiv.org/abs/2406.14228v2', 'http://arxiv.org/pdf/2406.14228v2']}, page_content='EVOAGENT: Towards Automatic Multi-Agent\\nGeneration via Evolutionary Algorithms\\nSiyu Yuan1∗, Kaitao Song2∗†,\\nJiangjie Chen1, Xu Tan2, Dongsheng Li2, Deqing Yang1†\\nFudan University1, Microsoft Research Asia2\\nsyyuan21@m.fudan.edu.cn, {kaitaosong, xuta, dongsli}@microsoft.com\\n{jjchen19,yangdeqing}@fudan.edu.cn\\nhttps://evo-agent.github.io\\nAbstract\\nThe rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically extend\\nthe specialized agent to multi-agent systems to improve task-solving capability\\nstill remains a significant challenge. In this paper, we introduce EVOAGENT, a\\ngeneric method to automatically extend expert agents to multi-agent systems via\\nthe evolutionary algorithm, thereby improving the effectiveness of LLM-based\\nagents in solving tasks. Specifically, we consider the existing agent frameworks\\nas the initial individual and then apply a series of evolutionary operators (e.g.,\\nmutation, crossover, selection, etc.) to generate multiple agents with diverse agent\\nsettings. EVOAGENT can be generalized to any LLM-based agent framework,\\nand can automatically extend the existing agent framework to multi-agent systems\\nwithout any extra human designs. Experimental results across various tasks have\\nshown that EVOAGENT can automatically generate multiple expert agents and\\nsignificantly enhance the task-solving capabilities of LLM-based agents.\\n1\\nIntroduction\\nRecently, the advent of large language models (LLMs) [1, 2, 3, 4] have shown remarkable capabilities\\nin solving language understanding, reasoning, and generation tasks. Based on the foundation of LLMs,\\nmany research works [5, 6, 7, 8, 9, 10, 11] have discovered that by empowering multiple advanced\\nskills (e.g., planning, tool, memory and so on), we can develop more powerful autonomous agents to\\nsolve more challenging tasks. Therefore, how to design and leverage LLM-based autonomous agents\\nto tackle more diverse and complex real-world applications has attracted enormous interest.\\nGenerally, many real-world scenarios are usually complex, encompassing a variety of challenging\\ntasks that are beyond the capability of a single agent. To address this point, we notice that human\\nsociety is composed of vast individuals, each possessing their unique characteristics. By selecting,\\norchestrating, and cooperating with different individuals, humans can form an efficient team group\\nto handle complicated missions in the real world. Therefore, there has been an increasing trend\\nto develop multi-agent collaboration frameworks (e.g., MetaGPT [10], AutoGen [12], Camel [13],\\n∗The first two authors have equal contributions. This work was done when the first author was an intern at\\nMicrosoft Research Asia.\\n† Corresponding authors.\\nPreprint. Under review.\\narXiv:2406.14228v2  [cs.AI]  11 Jul 2024\\nGenerative Agents [11]) to simulate human behaviors for solving complex tasks. By developing a\\nseries of expert agents with diverse settings, multi-agent systems enable us to reveal emergent abilities\\namong multiple agents and synergize their specialized expertise to achieve superior performance,\\nakin to simulating human populations. Nevertheless, it is worthy noting that, in most of (multi)-agent\\nframeworks, their designs heavily depend on handcrafted settings, including character roles, task\\nscopes, skills, and prompt settings. Although we admit that meticulous human design is quite useful\\nfor instructing LLM-based agents to understand tasks, it also limits scaling up the number of agents to\\nfurther improve performance due to expensive human labor. Considering the increasing popularity of\\nLLM-based autonomous agents, how to create a generic agent generation paradigm to automatically\\nbuild multi-agent systems has emerged as a critical challenge.\\nIn this paper, we introduce a novel method, EVOAGENT, that formulates agent generation as the\\nevolutionary processing [14] in human society. Specifically, to align human society, each agent\\ncan be considered as individuals that can procreate its population across successive generations.\\nMotivated by this mechanism, we can simulate such a human behavior to automatically generate\\nmultiple agents based on any pre-defined agents. Therefore, EVOAGENT can be considered as a\\none-shot agent generation method that starts from a specialized agent as the initial agent, and then\\nconsiders its settings (e.g., role, skills, prompts, and so on) as the variables to be evolved. With a\\nseries operation of EAs (e.g., selection, crossover, mutation), EVOAGENT can automatically create\\nmultiple evolutionary agents based on the initial specialized agent. Moreover, EVOAGENT is not\\nlimited to the infrastructure of agent frameworks, as it is a generic multi-agent generation method.\\nThus, it can be applied to any agent framework and expanded to multi-agent systems without any\\nextra human effort.\\nWe conduct experiments on multiple datasets, including knowledge-based question answering and\\nmulti-modal reasoning (§ 4.1), interactive scientific solving (§ 4.2) and real-world complex planning\\n(§ 4.3). Experimental results indicate that EVOAGENT can generate multiple agents with diverse\\nskills and harness their capabilities to consistently improve model performance in different scenarios.\\nBesides, to validate the scalability of EVOAGENT in creating massive agents, we also apply our\\nmethod to some conversational scenarios (e.g., debate), and the results also indicate the potential of\\nEVOAGENT in generating multiple diverse agents. Overall, the contributions of this paper can be\\nsummarized as below:\\n• We introduce EVOAGENT, a simple and generic multi-agent generation method to improve the\\neffectiveness of LLM-based agents in solving tasks. EVOAGENT can automatically generate new\\nexpert agents and is applicable to any agent framework.\\n• We formulate the agent generation processing as an evolutionary pipeline, that encompasses\\nmultiple operators (e.g., selection, crossover, mutation) to generate agent population without\\nadditional human supervision.\\n• We conduct extensive experiments on various tasks and demonstrate the effectiveness, scalability,\\nand generality of our EVOAGENT. Particularly, EVOAGENT can significantly enhance the perfor-\\nmance of LLM-based agents in both challenging open-world scenarios and complex real-world\\nplanning by generating more specialized agents.\\n2\\nRelated Work\\nLLM-based Autonomous Agents\\nWith the emergence of powerful large language models [1, 2, 3,\\n4], many researchers have endeavored to develop advanced autonomous agents [5, 6, 7] empowered by\\nmultiple high-level LLM skills (e.g., personas [11, 15, 16], planning [9, 17, 18, 19], tool [8, 6, 20, 21]\\nand memory [22, 23]). Some of them also extend agent frameworks to multi-agent collaboration\\n(e.g., MetaGPT [24], Generative Agents [11], AutoGen [12], Camel [13], AgentVerse [25] and so\\non), by designing multiple specific roles. These systems also demonstrate satisfactory performance in\\naddressing massive, challenging tasks. However, it is worth noting that most of the popular agent\\nframeworks heavily relied on handcrafted designs. The abundant human efforts necessitated by these\\nsystems also limit the adaptability and flexibility of agents to handle unexpected challenges [26, 27,\\n25, 10]. In this paper, we propose EVOAGENT, a method that can be applied to any LLM-based\\nagent framework and easily extend to multi-agent systems. By using EA, our method allows us to\\niteratively generate and optimize multiple agents with diverse settings.\\n2\\nAlgorithm 1: Multi-Agent Generation with Evolutionary Algorithm\\nRequire: Initial agent A(0,0), population size N per iteration, number of iterations T,\\nquality-check module LLMQuality(⋅), evolutionary operations EvoCrossover(⋅) and\\nEvoMutation(⋅), EvoUpdate(⋅)\\nInput: Initial result R0 derived from A(0,0)\\nOutput: Final result RT\\n1 for t = 1 to T do\\n2\\nCrossover: Update the settings of parent agents based on their generated results and initial\\nagent: {A\\n′\\n(0,t−1),A\\n′\\n(1,t−1),...,A\\n′\\n(N−1,t−1)} ←\\nEvoCrossover({R(0,t−1),R(1,t−1),...,R(N−1,t−1)},A(0,0));\\n3\\nMutation: Generate N ′(N ′ > N) child agents based on parent agents and initial agent:\\n{A(0,t),A(1,t),...,A(N ′−1,t)} ←\\nEvoMutation({A\\n′\\n(0,t−1),A\\n′\\n(1,t−1),...,A\\n′\\n(N−1,t−1)},A(0,0))\\n4\\nSelection: Select high-quality agents with quality-check module:\\n{At\\n0,...,At\\nN−1} ←LLMQuality({A(0,t),A(1,t),...,A(N ′−1,t)});\\n5\\nResult Update: Generate new result from new agents:\\n{R(0,t),R(1,t),...,R(N−1,t)} ←{A(0,t),A(1,t),...,A(N−1,t)}\\n6\\nIntegrate their results as a natural selection processing:\\nRt ←EvoUpdate({R(0,t),R(1,t),...,R(N−1,t)},Rt−1);\\n7 return RT ;\\nAgent Generation\\nRecent studies have shown that assigning personas or roles to LLM-based\\nautonomous agents can influence their behavior and performance in generation tasks [28, 29, 30, 13].\\nCurrent methods primarily involve manually assigning these personas and limit multi-agent col-\\nlaboration to single or fixed roles, which requires significant human effort and hinders generaliza-\\ntion [13, 12, 24, 10]. To address this, some frameworks like AgentVerse [25] and AutoAgents [31]\\nhave been proposed to automatically generate unlimited agents for collaborative task completion.\\nHowever, these methods still heavily depend on human-designed interventions, which limits their\\nscalability and functionality. For example, AutoAgents requires agent settings to satisfy a “Planner\\n- Agent Observer - Plan Observer” framework. Meanwhile, AgentVerse formulates a pipeline of\\n“Expert Recruitment - Collaborative Decision Making - Action Execution - Evaluation” to build\\nagents. These architectures also limit the task scope of designing agents. In contrast, EVOAGENT\\ncan automatically formulate the current agent frameworks to multi-agent systems with high-quality\\ngenerated expert agents by using EAs, which is flexible and adaptable to various agent frameworks.\\n3\\nMethod\\nGenerally, human society comprises a broad spectrum of individuals from diverse cultures, encom-\\npassing multiple generations. To solve specific tasks, human society usually involves a lot of expert\\nindividuals and aggregates their specialized expertise to achieve better answer. Thus, it can be\\nconsidered as the foundation to facilitate multi-agent collaborations. To fulfill this point, how to\\nautomatically create multiple agents would be very critical. Inspired by evolutionism, we formulate\\nagent generation as an evolutionary process to help us generate multiple agents without any human\\nlabor.\\n3.1\\nPreliminary\\nEvolutionary algorithm (EA) [32, 33], is a general algorithm to simulate the biological behaviors in\\nevolution, including reproduction, mutation, recombination, and selection. By introducing genetic\\nalgorithm [34, 35, 36, 37, 38] of the “survival of the fittest” mechanism, it can also be considered as\\nan optimization method to improve individuals. Therefore, EAs also belong to the non-parametric\\nlearning method, which can be applied to any framework. All we need to do is define which\\nparts should be evolved and the corresponding evolutionary operators. We also note some recent\\n3\\nworks [39, 40] indicate the potential of EAs that can be applied to optimize discrete prompts. So, in\\nthis paper, we explore how to formulate the agent generation problem as an evolutionary task.\\n3.2\\nEVOAGENT\\nBy assigning various settings to specific skills (e.g., role-playing, planning, tools and so on), agents\\ncould exhibit diverse task-solving capabilities. Therefore, our objective is to produce a population\\nof agents with distinct skills, to establish effective multi-agent systems. To fulfill this point, we\\ntreat each specialized agent as an unique individual and denote each skill as the part to be evolved,\\nakin to humans. So, we consider the procedure of agent generation to be evolutionary processing.\\nSpecifically, existing frameworks usually describe agent skills as the language. Thus, we can employ\\nLLM to simulate evolutionary operators to update the system settings of agents and create new agents.\\nHere, we formulate the procedure of EVOAGENT as a four-stage pipeline:\\nQuery: Please create a travel plan where I\\'ll depart from Washington and head to Myrtle \\nBeach for a 3-day trip from March 13th to March 15th, 2022. Can you help me keep this \\njourney within a budget of $1,400? It\\'s vital that my accommodations are pet-friendly.\\nSTEP 1: Initialization\\nInitial \\nAgent\\nHuman \\nWritten\\nSTEP 2: EA Operation Crossover & Mutation\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nLunch: Exotic India\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Cozy Brooklyn Room\\nInitial Agent\\nAccommodation \\nAgent\\nTransportation \\nAgent\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nBreakfast: Exotic India, Myrtle Beach\\nLunch: Catfish Charlie\\'s, Myrtle Beach\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Large sunny park slope apartment, \\npet-friendly\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nTransportation: Flight Number: F3792603\\nBreakfast: Exotic India, Myrtle Beach\\nLunch: Catfish Charlie\\'s, Myrtle Beach\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Cozy Brooklyn Room\\nHotel Agent\\nSTEP 3: Selection\\nQuality Check\\nThis agent has a duplicate type with \\nAccommodation Agent, so it is discarded.\\nSTEP 4: Results Update\\nUpdate \\nOperation\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nTransportation: Flight Number: F3792603\\nBreakfast: Exotic India, Myrtle Beach\\nLunch: Catfish Charlie\\'s, Myrtle Beach\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Large sunny park slope apartment, pet-friendly\\nFigure 1:\\nThe illustration of EVOAGENT. With the\\ngenerated multiple expert agents, EVOAGENT can gen-\\nerate a better travel plan to meet user preferences. For\\nEA operators, Crossover can improve the results of\\nparent agents by adjusting existing details (e.g., the in-\\nformation marked as blue). Mutation can introduce\\nnew variations to refine the results of parent agents by\\ngenerating child agents with new characteristics (e.g.,\\nthe information marked as red).\\nSTEP 1: Initialization\\nTo conduct EAs,\\nwe first need to confirm our initial agents.\\nHere, we enable EVOAGENT to start\\nfrom a pre-defined agent framework (e.g.,\\nMetaGPT [10] and AutoGen [12]), which\\nserves as the initial (parent) agents. More-\\nover, we also define which parts of this\\nagent should be upgraded. Generally, since\\nEAs is a generic algorithm, EVOAGENT\\nis applicable to any agent frameworks and\\nextends them as multi-agent frameworks.\\nWe will then explore how to generate new\\nagents in the next steps.\\nSTEP 2: Crossover & Mutation\\nIn the\\nfirst iteration, we directly use the initial\\nagents as the parents. And then, we design\\ntwo kinds of evolutionary operators, named\\nCrossover and Mutation. For Crossover,\\nwe first enable the parent agents to generate\\nresults based on user requests. Then, based\\non the generated results, we ask LLMs to\\ncheck which skills should be improved and\\nthen update them. This mechanism allows\\nus to generate child agents in new settings\\nwithout requiring any human labor. More-\\nover, we also need to guarantee the diver-\\nsity between the child agents and parents.\\nTo this end, we design a Mutation operation\\nthat requires LLMs to compare the child\\nagents and parent agents and then modify\\nthe child agents to make them distinct from\\ntheir parents while maintaining their task-\\nsolving capability. Based on these evolu-\\ntionary operators, we can generate effective\\nand diverse agents during one iteration. Be-\\nsides, as we also need to conduct multiple iterations, we will append all agents generated in the\\nprevious generation into the next iteration. How to select these agents during each iteration will be\\nintroduced next.\\nSTEP 3: Selection\\nBased on the above steps, we can obtain multiple candidate agents with diverse\\nsettings. To guarantee the quality of each agent, we also introduce a selection mechanism like EAs.\\nHere, we conduct a quality-check module with an LLM to detect whether the generated agents can\\nsatisfy it has inherited the characteristics and maintained differences from parent agents. We will\\nselect N child agents as the evolved agents in each iteration.\\n4\\nTable 1: Results of LLMs with different methods on Logic Grid Puzzle (Logic), Trivia Creative\\nWriting (Writing) and Codenames Collaborative (Codenames). The best results are bolded, and the\\nsecond best ones are underlined.\\nModel\\nMethod\\nLogic Writing Codenames\\nLLama2-13B-Chat\\nDirect\\n4.00\\n28.00\\n0.00\\nCoT\\n26.00\\n46.00\\n18.00\\nSelf-Refine3\\n33.50\\n31.20\\n12.37\\nSPP\\n0.00\\n4.00\\n1.00\\nEVOAGENT(1,3)\\n35.50\\n49.60\\n27.83\\nGPT-3.5\\nDirect\\n48.00\\n56.20\\n76.29\\nCoT\\n47.50\\n51.00\\n71.13\\nSelf-Refine3\\n47.50\\n59.19\\n46.39\\nSPP\\n56.00\\n54.40\\n61.86\\nEVOAGENT(1,3)\\n71.50\\n60.80\\n79.38\\nGPT-4\\nDirect\\n60.50\\n75.40\\n79.38\\nCoT\\n65.50\\n74.00\\n80.41\\nSelf-Refine3\\n64.50\\n74.60\\n79.38\\nSPP\\n64.50\\n79.20\\n78.35\\nEVOAGENT(1,3)\\n77.00\\n84.40\\n84.53\\nSTEP 4: Results Update\\nBased on the above steps, we obtain many new agents that evolved from\\nparent agents, but with diverse settings. To improve task-solving capabilities, we ask each child agent\\nto generate candidate results and then use LLMs to integrate these candidates with the result from the\\nprevious iteration into a new result, akin to a natural selection processing stage. Moreover, we can\\nautomatically generate more agents by repeating the operations from step 2 to step 4 until the number\\nof agents has fulfilled our targets.\\nBy introducing EA, EVOAGENT enables us to automatically extend the existing agent framework\\nto a multi-agent system without any extra human designs. The mechanism also makes EVOAGENT\\ncan be applied to any agent framework without any prerequisites. The entire process is illustrated in\\nFigure 1. And we also present the details of EVOAGENT in Algorithm 1.\\n4\\nExperiment\\nIn this section, we adopt EVOAGENT to multiple applications to illustrate that EVOAGENT can\\nhelp LLM-based agents better accomplish tasks with multi-agent generation.3 Furthermore, we also\\ndemonstrate that EVOAGENT can be applicable in supporting currently widely used multi-agent\\nframeworks, such as MetaGPT [10], AutoGen [12], and Camel [13].\\n4.1\\nNLP and Multi-Modal Tasks\\nBenchmarks\\nTo align previous experiences (e.g., Self-Refine [41] and Solo Performance Prompt-\\ning [42]), we select three NLP knowledge-intensive and reasoning-intensive tasks from [42] and one\\nmulti-modal task:\\n• Logic Grid Puzzle is a reasoning task with 200 puzzles featuring 2 to 5 unique occupants in\\ndifferent houses. The aim is to identify house numbers for one occupant with provided clues.\\n• Trivia Creative Writing is a knowledge-intensive task consisting of 100 instances. This task\\nrequires a model to write a coherent story while incorporating answers to N trivia questions.\\n• Codenames Collaborative is a reasoning-intensive task with 50 instances. It involves a model\\nidentifying target words based on a given hint and a complete list of words.\\n• MMMU [43] is a comprehensive benchmark for college-level, multi-discipline multi-modal\\nunderstanding and reasoning. MMMU has three levels of difficulty: easy, medium, and hard. We\\nevaluate EVOAGENT against baselines using the multiple-choice questions in the validation set of\\nMMMU, which includes 847 questions spanning 30 different domains.\\n3The data examples of EVOAGENT on these tasks are provided in Appendix F.\\n5\\nCoT\\nSR\\nSPP Ours\\n45\\n50\\n55\\n60\\n65\\nAccuracy(%)\\nAll\\nCoT\\nSR\\nSPP Ours\\n55\\n65\\n75\\nEasy\\nCoT\\nSR\\nSPP Ours\\n45\\n50\\n55\\n60\\nMedium\\nCoT\\nSR\\nSPP Ours\\n25\\n35\\n45\\n55\\nHard\\nGPT-4V\\nGemini-Pro\\nFigure 2: Overall results of GPT-4V and Gemini-Pro with different methods on the MMMU validation\\nset. We also compare the performance of GPT-4V and Gemini-Pro across three difficulty levels.\\nBaselines\\nFor NLP tasks, we select LLama2-13B-Chat [3], GPT-3.5 [44] and GPT-4 [1] as our\\nbackbone networks. We compare EVOAGENT with 0-shot learning (Direct), Chain-of-thought (CoT)\\nprompting [45] and Self-Refine [41] and Solo Performance Prompting (SPP) [42]. For Self-Refine,\\nwe follow [41] to design feedback and refine prompts with three iterations. SPP is not a multi-agent\\ncollaboration framework but a prompting strategy that asks a single LLM to identify and discuss with\\nmultiple personas with few-shot learning. For SPP, we follow the original setting [42] to make a fair\\ncomparison. For MMMU, we select GPT-4V [46] and Gemini-Pro as the backbone and compare\\nEVOAGENT with CoT prompting, Self-Refine, and SPP. 4\\nEvaluation Metrics\\nFor all benchmarks, we adhere to the evaluation metrics specified in the\\noriginal setting. Specifically, for Logic Grid Puzzle and MMMU tasks, we report the accuracy of\\nall questions. For Trivia Creative Writing, we measure the ratio of correctly mentioned answers in\\nthe trivia questions. For Codenames Collaborative, we calculate the overlapping ratio between the\\npredicted words from the Guesser and the target words as the metric.\\nResult & Analysis\\nIn our experiments, we adopt the agent settings of [42] (for NLP tasks) and\\n[43] (for MMMU) as the initial agent. For our method, we denote it as EVOAGENT(N,T ), where N\\nis the population size generated in each iteration, and T is the number of iterations. Here, to align\\nwith Self-Refine, we set N as 1 and T as 3, which means we conduct three iterations, each of which\\ngenerates a new expert agent. Our results are reported in Table 1, and we can observe:\\n1. By utilizing multiple generated agents, EVOAGENT can greatly improve LLM performances in\\nboth NLP knowledge and reasoning tasks. We also compare EVOAGENT with some pre-defined\\nagent generation frameworks, e.g., AgentVerse [25] and AutoAgent [31]. The results shown in\\nAppendix B prove that the EVOAGENT is even better than these agent generation frameworks.\\n2. When using weaker LLMs (e.g., LLama2-13B-Chat), SPP usually produces poor performances,\\nconsistent with the findings in [42]. This suggests the limited effectiveness of SPP in smaller and\\nless capable models. However, EVOAGENT can provide consistent improvements among each\\nLLM, proving its strong generalization by using diverse generated agents.\\nIn addition, Figure 2 shows that Self-Refine (SR) and SPP degrade performance compared to CoT\\nprompting in MMMU task. However, EVOAGENT can generate multiple domain-specific agents and\\nthus improve multi-modal models in addressing scientific questions across various difficulty levels.\\n4.2\\nInteractive Scientific Solving Simulation\\nBenchmark\\nCompared with traditional NLP or multi-modal tasks, autonomous agents usually need\\nto perform problem-solving abilities akin to humans in interactive and open-world environments.\\nCurrently, we choose ScienceWorld [47], a complex interactive environment requiring skills in\\nlong-term memory, sub-task decomposition, and scientific and commonsense knowledge. Here, we\\nevaluate 30 scientific tasks in ScienceWorld to demonstrate the capability of EVOAGENT in solving\\ntasks in more challenging open-world environments.\\n4The detailed model parameters and versions and full prompts for these methods can be found in Appendix A.\\n6\\nTable 2: Average Scores of different methods on Sci-\\nenceWorld. We also report performance on three\\ndifficult-level groups based on the average length of\\nthe oracle agent’s trajectories [48].\\nModel\\nOverall\\nLong Medium Short\\nGPT-3.5\\n17.12\\n6.28\\n19.91\\n27.90\\nw/ EVOAGENT(1,1)\\n19.02\\n7.25\\n18.87\\n33.26\\nGPT-4\\n27.97 10.58\\n36.00\\n42.41\\nw/ EVOAGENT(1,1)\\n30.42 11.38\\n36.17\\n48.67\\nBaseline and Evaluation Metrics\\nFollow-\\ning [48], we require LLMs to perform an\\naction at each step by using in-context learn-\\ning 5. For evaluation, each task in Science-\\nWorld includes some sub-tasks, and we re-\\nport the results by calculating the completed\\nsub-tasks for the whole task.\\nResult & Analysis\\nFor EVOAGENT, we\\nadopt the agent framework with original set-\\ntings in [48] as the initial agent. Since each\\nstep in ScienceWorld requires using EA, we\\nset the population size N as 1 and the itera-\\ntions T as 1 for efficiency, denoted as EVOA-\\nGENT(1,1). Results in Table 2 show that:\\n1. EVOAGENT can also extend interactive agents to multi-agent systems in solving complete scientific\\ntasks in dynamic, open-world environments and consistently improve the performance of LLMs.\\n2. Our method exhibits the most substantial improvement in short-trajectory tasks, with less sig-\\nnificant gains in medium and long-trajectory tasks. We argue that the capability of multi-agent\\nsystems will also be affected by a longer context. We also expect to investigate the effect of long\\ncontext on multi-agent systems in the future.\\nGenerally, these results also demonstrate the generalization of EVOAGENT, which can also be used\\nfor solving interactive tasks in an open-world environment.\\n4.3\\nReal-World Scenarios\\nBenchmark\\nMoreover, in addition to performing actions in interactive environments, planning in\\ncomplex and realistic environments is also a crucial skill for building autonomous agents. To validate\\nthis point, we also select TravelPlanner [49], a benchmark designed to evaluate language agents in\\nreal-world complex planning with multiple constraints.\\nBaseline and Evaluation Metrics\\nFollowing [49], we select Mistral-7B [50], GPT-3.5, Gemini-\\nPro [2] and GPT-4 as our backbone models. We compare EVOAGENT with 0-shot learning (Direct),\\nCoT prompting, SPP, and Self-Refine within each backbone model. Furthermore, we also attempt\\nthe ReAcT method [51] for GPT-3.5, which introduces a virtual ‘think’ action to generate sub-\\ntasks during the action planning process. For evaluation, we adhere to the original metrics from\\nTravelPlanner, reporting the delivery rate, commonsense constraint pass rate, hard constraint pass\\nrate, and final pass rate for all methods 6.\\nResult & Analysis\\nFor EVOAGENT, we adopt the original settings in TravelPlanner as the initial\\nagent. Results in Table 3 show that:\\n1. EVOAGENT can generate specialized agents, such as those focused on culinary experiences,\\ntransportation, and attractions. Therefore, the generated travel plans are more aligned with user\\npreferences (hard constraints) and commonsense rules;\\n2. Although existing paradigms (e.g., CoT, ReAct, Self-Refine, SPP) have demonstrated decent re-\\nsults in some conventional NLP tasks, they still lack capability in handling complex planning tasks\\nwithin TravelPlanner. These results also demonstrate that only using human-design prompting\\nstrategies is insufficient to handle complex planning tasks.\\n3. By using EVOAGENT to automatically generate multiple agents and forming a multi-agent\\ncollaboration paradigm, we can develop higher-quality plans that better meet user preferences.\\nThat also indicates the significance of multi-agent systems for complex planning tasks.\\n5The introduction of the settings of LLMs are shown in Appendix C.\\n6Detailed introduction of experiment settings is provided in Appendix D.\\n7\\nTable 3: Main results of different LLMs and planning strategies on the TravelPlanner validation set.\\nEVOAGENT(N,T ) indicates that the population size per iteration is N and the number of iterations is\\nT. The best results are bolded, and the second best ones are underlined.\\nModel\\nMethod\\nDelivery Commonsense Hard Constraint Final\\nRate\\nMicro Macro Micro\\nMacro\\nMistral-7B\\nDirect\\n100.0\\n64.7\\n2.2\\n3.1\\n0.0\\n0.0\\nCoT\\n100.0\\n60.5\\n1.1\\n1.0\\n0.0\\n0.0\\nSPP\\n100.0\\n55.1\\n0.0\\n0.7\\n0.6\\n0.0\\nSelf-Refine3\\n100.0\\n58.3\\n0.0\\n0.7\\n0.0\\n0.0\\nEVOAGENT(1,3)\\n100.0\\n60.1\\n2.2\\n4.5\\n0.6\\n0.0\\nGPT-3.5\\nDirect\\n100.0\\n57.3\\n3.9\\n11.0\\n3.3\\n0.0\\nCoT\\n100.0\\n61.0\\n2.8\\n10.0\\n3.3\\n0.0\\nReAct\\n82.2\\n42.3\\n0.6\\n11.9\\n4.6\\n0.0\\nSPP\\n99.4\\n54.6\\n1.7\\n3.8\\n1.1\\n0.0\\nSelf-Refine3\\n100.0\\n56.0\\n1.7\\n3.1\\n1.1\\n0.0\\nEVOAGENT(1,3)\\n100.0\\n64.2\\n7.8\\n11.0\\n4.4\\n1.1\\nEVOAGENT(1,5)\\n100.0\\n61.0\\n5.0\\n12.6\\n5.0\\n0.5\\nGemini-Pro\\nDirect\\n90.0\\n61.7\\n7.8\\n16.4\\n7.8\\n0.6\\nCoT\\n90.0\\n61.4\\n7.2\\n10.0\\n6.1\\n1.7\\nSPP\\n100.0\\n67.6\\n7.8\\n10.2\\n3.9\\n1.1\\nSelf-Refine3\\n95.6\\n65.8\\n6.1\\n15.0\\n4.4\\n0.6\\nEVOAGENT(1,3)\\n100.0\\n73.5\\n12.8\\n16.9\\n7.2\\n1.7\\nEVOAGENT(1,5)\\n100.0\\n74.0\\n8.9\\n21.2\\n11.7\\n2.2\\nGPT-4\\nDirect\\n100.0\\n79.4\\n15.8\\n27.5\\n16.1\\n2.2\\nCoT\\n100.0\\n76.7\\n11.7\\n22.4\\n12.8\\n2.2\\nSPP\\n96.7\\n70.6\\n5.6\\n11.4\\n7.8\\n0.6\\nSelf-Refine3\\n98.9\\n75.3\\n7.2\\n12.4\\n7.2\\n1.1\\nEVOAGENT(1,3)\\n100.0\\n81.5\\n21.1\\n31.4\\n18.9\\n7.2\\n4.4\\nAblation Studies\\nTo better understand the value of EVOAGENT, we conduct detailed analyses on TravelPlanner,\\nfocusing on the impact of population size and the effectiveness of the quality-check module in the\\nselection stage.\\nTable 4: Average commonsense constraint\\npass rate (Com.) and hard constraint pass rate\\n(Hard) of ablated variants on TravelPlanner.\\nMethod\\nw/o QC\\nw/ QC\\nCom. Hard Com. Hard\\nDirect\\n-\\n-\\n59.5\\n13.7\\nSuggest3\\n-\\n-\\n61.7\\n8.4\\nOvergen3\\n-\\n-\\n61.4\\n10.7\\nPromptRefine3\\n-\\n-\\n63.0\\n13.8\\nDifferent Population Size\\nEVOAGENT(1,3)\\n68.9\\n14.0\\n68.9\\n14.0\\nEVOAGENT(2,3)\\n62.8\\n12.7\\n67.0\\n15.2\\nEVOAGENT(3,3)\\n62.7\\n13.7\\n66.8\\n15.8\\nDifferent Selection Stategies\\nRandom\\n62.9\\n12.7\\n67.1\\n15.0\\nPK\\n63.5\\n13.6\\n66.4\\n14.5\\nAll-in\\n61.9\\n13.2\\n67.1\\n17.0\\nExperiment Settings\\nWe evaluate the performance\\nof different LLMs at varying population sizes N\\nwith fixed iteration number 3, denoted as EVOA-\\nGENT(N,3), both with and without the quality-check\\nmodule (QC). We employ an LLM that shares the\\nsame backbone as the initial agent for updates. To\\nselect results from candidates for this LLM to update,\\nwe adopt three different selection strategies: 1) Ran-\\ndom: one result is selected randomly from the pool\\nof candidates; 2) PK: we ask an agent with the same\\nbackbone as the initial agent to identify the optimal\\nresults from the pool of candidates; 3) All-in: Rather\\nthan selecting a single result, we update using all\\ncandidates.\\nMoreover, we also attempt Suggest3, Overgen3 and\\nPromptRefine3 as variants to prove the effectiveness\\nof our method. For Suggest3, instead of generating\\nnew results, we ask new generated agents to only give\\nsuggestions for initial agents to revise their results.\\nFor Overgen3, we first ask initial agents to generate\\n3 different results at one time, and then these agents\\n8\\nEvoAgent\\nInvesting in clean energy not only addresses the climate \\ncrisis but also creates jobs and strengthens our economy.\\n...transitioning to renewable energy can create \\nmillions of good-paying, union jobs without \\nsignificant unemployment or economic fallout...\\nLabor \\nEconomist \\nAgent\\nPresident Opinion: Support\\n...Thrusting forward with renewable energy \\nstrengthens our international ties and \\npropels economies reliant on fossil fuel \\nexports towards clean energy transitions...\\nGeopolitical \\nAnalyst \\nAgent\\n...Every moment we delay increases the \\nseverity of climate-related illnesses, straining \\nour health infrastructure and costing us $820 \\nbillion annually...\\nPublic Health \\nAgent\\nMetaGPT Framework\\nTopic: The U.S. should commit more in climate change fighting\\nEvoAgent\\nEnergy Sector \\nAnalyst Agent\\nThe promises of ample job creation overlook \\nthe reality that many displaced workers \\nfrom conventional sectors may struggle to \\nfind roles in the nascent green economy. \\nAn abrupt transition to renewable energy \\ncould cause economic tremors and job \\nlosses.\\nRisk \\nManagement \\nAgent\\nThis isn‘t about alarmism or denial, it’s \\nabout carefully leading our nation \\ntowards a sustainable, prosperous future. \\nAn abrupt shift spells risk!\\nTransition \\nStrategist \\nAgent\\nThe real crisis is the economic disaster under His policies. He \\ntalks about investments, but it\\'s your tax dollars he\\'s spending. \\nPresident Opinion: Oppose\\nFigure 3: The adaption of EVOAGENT on MetaGPT framework. With the EA, we can extend the\\noriginal role in the debate scenario to different expert agents to enrich the opinions.\\ncan output the final results based on these multiple\\ncandidates. For PromptRefine3, instead of generating agents, we ask the initial agent to refine its\\nprompts three times to better answer the query. 7\\nResult & Analysis\\nTo obtain stable findings, we first obtain results from GPT-3.5 and Gemini-Pro\\nacross different population sizes and selection strategies. We then average their results over various\\nmetrics to clearly compare the strengths and weaknesses of these variants.\\nThe results are shown in Table 4.8 We find that EVOAGENT significantly outperforms the Overgen,\\ndemonstrating the effectiveness of generating specialized agents to assist with complex planning.\\nAlthough obtaining suggestions from new generated agents can improve the performance on com-\\nmonsense constraints, these methods greatly harm the agents to meet the user preference. Modifying\\nthe prompt can improve the performance of agents, yet it remains less effective than EVOAGENT.\\nWhen the population size exceeds one, agents may generate similar agents. Thus, lacking a quality-\\ncheck module leads to reduced travel plan quality. Furthermore, when population size increases,\\nthe model aligns travel plans more closely with user preferences but diminishing adherence to\\ncommonsense rules, consistent with the findings in Table 3. Remarkably, the PK strategy initially\\nyields superior results without the quality-check module, but this trend reverses once quality checks\\nare implemented. We speculate that, without the quality-check module, PK partially fulfills this role,\\naiding in selecting better candidates. However, with the quality-check module, PK introduces bias by\\nfavoring specific fields of expertise while neglecting others, resulting in a less effective than random\\nstrategy. Meanwhile, the All-in strategy performs optimally when a quality-check module is included.\\nFuture research can leverage long-context LLMs to expand more agents with EVOAGENT to better\\nsolve complex real-world tasks.\\n4.5\\nEVOAGENT Application\\nPrevious experiments have demonstrated that our method can automatically extend existing agent\\nframeworks to multi-agent systems, thus greatly improving LLM-based agents in various scenarios.\\nWe also attempt to extend our work to real-world multi-agent applications (e.g., MetaGPT [10],\\nCamel [13] and AutoGen [12]), to verify it can scale up the number of agents in building multi-agent\\nscenarios, just as shown in Figure 3. Here, we choose the debate scenario used in MetaGPT, which\\nincludes two debaters with different opinions, leading to dull and repetitive content generation. Here,\\ninstead of manually assigning new roles, we applied EVOAGENT to extend each debate team to more\\nagents with diverse settings, increasing the variety of opinions and the quality of the debate 9.\\n7The full prompts of different ablation settings are shown in Appendix A.1.\\n8The complete results with further analysis are shown in Appendix E\\n9The details of MetaGPT, and the adaption of EVOAGENT on Camel and AutoGen are shown in Appendix G.\\n9\\n5\\nConclusion\\nIn this paper, we propose EVOAGENT, an automatic multi-agent generation system by leveraging\\nevolutionary algorithms. Different from previous methods, EVOAGENT is suitable to any existing\\nagent framework and extends it to multi-agent systems with diverse and effective agents by using\\na series of evolutionary operations, including mutation, crossover, and selection. Experiments on\\nmultiple tasks show that EVOAGENT can significantly improve the capabilities of LLM-based agents\\nin solving complex tasks.\\nReferences\\n[1] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.\\n[2] Gemini Team. Gemini: A family of highly capable multimodal models, 2023.\\n[3] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\\nKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-\\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\\nand fine-tuned chat models. CoRR, abs/2307.09288, 2023.\\n[4] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.\\n[5] Significant Gravitas. Auto-gpt: An autonomous gpt-4 experiment. https://github.com/\\nSignificant-Gravitas/Auto-GPT, 2023.\\n[6] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-\\nginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580,\\n2023.\\n[7] Yohei Nakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023.\\n[8] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle-\\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\\nthemselves to use tools. CoRR, abs/2302.04761, 2023.\\n[9] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\\nmodels. In NeurIPS, 2022.\\n[10] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang,\\nCeyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng\\nXiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent\\ncollaborative framework. In The Twelfth International Conference on Learning Representations,\\n2024.\\n[11] Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang,\\nand Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In\\nProceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,\\nUIST 2023, San Francisco, CA, USA, 29 October 2023- 1 November 2023, pages 2:1–2:22.\\nACM, 2023.\\n10\\n[12] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\\nLi Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via\\nmulti-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.\\n[13] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\\nGhanem. CAMEL: Communicative agents for ”mind” exploration of large language model\\nsociety. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n[14] Thomas Bäck and Hans-Paul Schwefel. An overview of evolutionary algorithms for parameter\\noptimization. Evol. Comput., 1(1):1–23, 1993.\\n[15] Xintao Wang, Yunze Xiao, Jen tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei,\\nZiang Leng, Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. Incharacter: Evaluating\\npersonality fidelity in role-playing agents through psychological interviews, 2024.\\n[16] Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li,\\nRuihan Yang, Tinghui Zhu, et al. From persona to personalization: A survey on role-playing\\nlanguage agents. arXiv preprint arXiv:2404.18231, 2024.\\n[17] Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, and Kyle Richardson. Put\\nyour money where your mouth is: Evaluating strategic planning and execution of llm agents in\\nan auction arena. arXiv preprint arXiv:2310.05746, 2023.\\n[18] Yikai Zhang, Siyu Yuan, Caiyu Hu, Kyle Richardson, Yanghua Xiao, and Jiangjie Chen.\\nTimearena: Shaping efficient multitasking language agents in a time-aware simulation. arXiv\\npreprint arXiv:2402.05733, 2024.\\n[19] Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski,\\nYanghua Xiao, and Deqing Yang. Distilling script knowledge from large language models for\\nconstrained language planning. arXiv preprint arXiv:2305.05252, 2023.\\n[20] Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu,\\nDongsheng Li, and Yueting Zhuang. Taskbench: Benchmarking large language models for task\\nautomation. arXiv preprint arXiv:2311.18760, 2023.\\n[21] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li,\\nand Deqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv\\npreprint arXiv:2401.06201, 2024.\\n[22] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In 3rd International\\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\\nConference Track Proceedings, 2015.\\n[23] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\\ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\\n[24] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human\\nbehaviors for llm-based task-oriented coordination via collaborative generative agents. arXiv\\npreprint arXiv:2310.06500, 2023.\\n[25] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu,\\nYaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong\\nSun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent\\nbehaviors. In The Twelfth International Conference on Learning Representations, 2024.\\n[26] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu,\\nand Maosong Sun.\\nCommunicative agents for software development.\\narXiv preprint\\narXiv:2307.07924, 2023.\\n[27] Zhitao He, Pengfei Cao, Yubo Chen, Kang Liu, Ruopeng Li, Mengshu Sun, and Jun Zhao.\\nLEGO: A multi-agent collaborative framework with role-playing and iterative feedback for\\ncausality explanation generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,\\nFindings of the Association for Computational Linguistics: EMNLP 2023, pages 9142–9163,\\nSingapore, December 2023. Association for Computational Linguistics.\\n11\\n[28] Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong\\nMao. Expertprompting: Instructing large language models to be distinguished experts. arXiv\\npreprint arXiv:2305.14688, 2023.\\n[29] Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik\\nNarasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. In Houda\\nBouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational\\nLinguistics: EMNLP 2023, pages 1236–1270, Singapore, December 2023. Association for\\nComputational Linguistics.\\n[30] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceed-\\nings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST\\n’23, New York, NY, USA, 2023. Association for Computing Machinery.\\n[31] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu,\\nand Yemin Shi. Autoagents: A framework for automatic agent generation. arXiv preprint\\narXiv:2309.17288, 2023.\\n[32] Thomas Bartz-Beielstein, Jürgen Branke, Jörn Mehnen, and Olaf Mersmann. Evolutionary\\nalgorithms. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 4(3):178–\\n195, 2014.\\n[33] Agoston E Eiben, James E Smith, AE Eiben, and JE Smith. What is an evolutionary algorithm?\\nIntroduction to evolutionary computing, pages 25–48, 2015.\\n[34] Jeffrey R Sampson. Adaptation in natural and artificial systems (john h. holland), 1976.\\n[35] John H Holland. Adaptation in natural and artificial systems: an introductory analysis with\\napplications to biology, control, and artificial intelligence. MIT press, 1992.\\n[36] Melanie Mitchell. An introduction to genetic algorithms. MIT press, 1998.\\n[37] Lothar M Schmitt. Theory of genetic algorithms. Theoretical Computer Science, 259(1-2):1–61,\\n2001.\\n[38] Seyedali Mirjalili, Jin Song Dong, Ali Safa Sadiq, and Hossam Faris. Genetic algorithm:\\nTheory, literature review, and application in image reconstruction. Nature-inspired optimizers:\\nTheories, literature reviews and applications, pages 69–85, 2020.\\n[39] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang\\nBian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields\\npowerful prompt optimizers. CoRR, abs/2309.08532, 2023.\\n[40] Angelica Chen, David Dohan, and David R. So. Evoprompting: Language models for code-level\\nneural architecture search. In Advances in Neural Information Processing Systems 36: Annual\\nConference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,\\nUSA, December 10 - 16, 2023, 2023.\\n[41] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine:\\nIterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information\\nProcessing Systems, 2023.\\n[42] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing\\ncognitive synergy in large language models: A task-solving agent through multi-persona\\nself-collaboration. arXiv preprint arXiv:2307.05300, 2023.\\n[43] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\\nunderstanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.\\n12\\n[44] OpenAI. Chatgpt, 2022.\\n[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\\nQuoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language\\nmodels. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,\\nAdvances in Neural Information Processing Systems, 2022.\\n[46] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\\nLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\\narXiv:2309.17421, 9(1):1, 2023.\\n[47] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Science-\\nWorld: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa Kozareva, and\\nYue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 11279–11298, Abu Dhabi, United Arab Emirates, December 2022.\\nAssociation for Computational Linguistics.\\n[48] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula,\\nPrithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with\\nfast and slow thinking for complex interactive tasks. In Thirty-seventh Conference on Neural\\nInformation Processing Systems, 2023.\\n[49] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao,\\nand Yu Su. Travelplanner: A benchmark for real-world planning with language agents. arXiv\\npreprint arXiv:2402.01622, 2024.\\n[50] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\\nSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\\nLavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.\\n[51] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and\\nYuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh\\nInternational Conference on Learning Representations, 2023.\\nA\\nExperiment Settings\\nA.1\\nPrompt for Baselines and EVOAGENT\\nListing 1 and 2 shows the full prompt for 0-shot learning (Direct), Chain-of-thought (CoT) prompt-\\ning [45] and Self-Refine [41] and Solo Performance Prompting, i.e., SPP [42]. Listing 3 and 4 show\\nthe prompt of EVOAGENT and different ablation settings.\\nA.2\\nModel Selection\\nFor OpenAI models,\\nwe use gpt-35-turbo and gpt-4-32k with the version of\\n2024-02-15-preview in Azure.10 For Gemini-pro, we use Google Gemini-Pro APIs to ob-\\ntain results. We set the temperature to 0 for all models.\\nB\\nEVOAGENT v.s. Human-designed Agent Framework\\nAgentVerse [25] and AutoAgent [31] are frameworks designed to generate an unlimited number\\nof agents for collaborative tasks automatically. Despite this automation, they still rely on human-\\ndesigned interventions. AutoAgents requires agent settings to satisfy a “Planner - Agent Observer\\n- Plan Observer” framework, while AgentVerse formulates a pipeline of “Expert Recruitment -\\nCollaborative Decision Making - Action Execution - Evaluation” to build agents. We argue that these\\nhuman-designed architectures limit their scalability and functionality.\\n10https://azure.microsoft.com/en-us/products/ai-services/openai-service\\n13\\nTable 5: Comparison of EVOAGENT\\nwith human-designed agent frameworks\\nin Logic Grid Puzzle, Trivia Creative\\nWriting tasks.\\nFramework Logic Writing\\nEVOAGENT\\n77.00\\n84.40\\nAgentVerse\\n66.50\\n-\\nAutoAgents\\n-\\n82.00\\nIn the original papers, AgentVerse [25] and AutoA-\\ngent [31] also conduct experiments on the Logic Grid\\nPuzzle task and Trivia Creative Writing task, respectively.\\nWe follow their experimental settings and compared them\\nwith our method. As demonstrated in Table 5, EVOAGENT\\noutperforms both AgentVerse and AutoAgent, highlight-\\ning the effectiveness and generality of EVOAGENT.\\nC\\nExperimental Details of ScienceWorld\\nFollowing [48], we adopt the REACT [51] method for\\neach LLM, which introduces a virtual ’think’ action. This\\naction allows LLMs to generate subgoals during the action\\nplanning process.\\nD\\nEvaluation Details of TravelPlanner\\nGrounding to travel planning, a real-world use-case that inherently involves various constraints like\\nuser preferences and commonsense rules, TravelPlanner evaluates whether agents can formulate\\nflexible travel plans using gathered information to meet these constraints. We test EVOAGENT and all\\nbaselines on the TravelPlanner validation set, which consists of 180 user queries with the collected\\ninformation. To evaluate the travel plans generated by agents, TravelPlanner adopts the following\\nevaluation metrics:\\n• Delivery Rate: Assesses if agents can complete a plan within a limited number of steps (30 in our\\nexperimental setting). Failures are due to dead loops, numerous failed attempts, or exceeding the\\nstep limit.\\n• Commonsense Constraint Pass Rate: Evaluates if an agent can incorporate commonsense into their\\nplan.\\n• Hard Constraint Pass Rate: Measures if a plan meets all explicit hard constraints in the query,\\ntesting the agent’s ability to adapt to diverse user preferences.\\n• Final Pass Rate: Indicates the proportion of viable plans that meet all criteria, reflecting the agent’s\\nproficiency in creating practical plans.\\nFurthermore, TravelPlanner uses micro and macro strategies to assess the Commonsense and Hard\\nConstraint Pass Rates. The micro strategy calculates the ratio of met constraints to the total. The\\nmacro strategy measures the proportion of plans that meet all commonsense or hard constraints.\\nTogether, these strategies assess an agent’s ability to satisfy individual constraints and all constraints\\ncomprehensively.\\nE\\nMore Analysis of Ablation Studies\\n0\\n1\\n3\\n5\\n7\\nIteration\\n56\\n58\\n60\\n62\\nAnswer Ratio (%)\\nFigure 4: The performance of\\nGPT-3.5 with EVOAGENT under\\ndifferent iterations on Trivia Cre-\\native Writing task.\\nThe complete results of ablation studies on TravelPlanner are\\nshown in Table 6. This result indicates that the absence of the\\nquality-check module significantly lowers the delivery pass rate\\nwhen the All-in strategy is applied. To explore the reasons, we\\nrevisit the results and discover that sometimes unsuitable agents\\ncreate overly lengthy travel plans that fail to meet the criteria.\\nFor example, the model might erroneously assign a nutritionist to\\ndevise travel plans, resulting in excessively detailed meal arrange-\\nments and nutritional breakdowns. Therefore, the input length\\nsurpasses the context window of LLMs, preventing the final result\\ngeneration.\\nMoreover, we also conduct experiments on the Trivia Creative\\nWriting task to investigate the impact of the number of iterations\\non model performance in traditional NLP tasks. As shown in\\n14\\nTable 6: Comparison of different popularity selection strategies for LLMs on TravelPlanner. The best\\nresults are bolded, and the second best ones are underlined.\\nModel\\nStrategy Method\\nw/o Quality Check\\nw/ Quality Check\\nDelivery Com. Hard Delivery Com. Hard\\nGPT-3.5\\nDirect\\n-\\n-\\n-\\n100.0\\n57.3\\n11.0\\nSuggest3\\n-\\n-\\n-\\n100.0\\n57.5\\n5.7\\nOvergen3\\n-\\n-\\n-\\n98.3\\n56.3\\n9.0\\nPromptRefine3\\n-\\n-\\n-\\n100.0\\n61.2\\n11.0\\nEVOAGENT(1,3)\\n100.0\\n64.2\\n11.0\\n100.0\\n64.2\\n11.0\\nRandom\\nEVOAGENT(2,3)\\n100.0\\n59.4\\n10.2\\n100.0\\n65.4\\n13.8\\nEVOAGENT(3,3)\\n98.9\\n59.2\\n11.4\\n100.0\\n65.8\\n14.0\\nPK\\nEVOAGENT(2,3)\\n99.4\\n59.4\\n7.1\\n100.0\\n66.0\\n11.7\\nEVOAGENT(3,3)\\n98.9\\n58.5\\n11.2\\n100.0\\n61.3\\n12.4\\nAll-in\\nEVOAGENT(2,3)\\n97.2\\n59.4\\n10.0\\n100.0\\n64.2\\n15.5\\nEVOAGENT(3,3)\\n93.3\\n56.0\\n8.3\\n100.0\\n65.2\\n12.6\\nGemini-Pro\\nDirect\\n-\\n-\\n-\\n90.0\\n61.7\\n16.4\\nSuggest3\\n-\\n-\\n-\\n100.0\\n65.8\\n11.0\\nOvergen3\\n-\\n-\\n-\\n100.0\\n66.5\\n12.4\\nPromptRefine3\\n-\\n-\\n-\\n96.7\\n64.9\\n16.7\\nEVOAGENT(1,3)\\n100.0\\n73.5\\n16.9\\n100.0\\n73.5\\n16.9\\nRandom\\nEVOAGENT(2,3)\\n96.7\\n65.9\\n13.1\\n99.4\\n67.3\\n14.0\\nEVOAGENT(3,3)\\n97.2\\n67.0\\n16.0\\n100.0\\n70.0\\n18.1\\nPK\\nEVOAGENT(2,3)\\n97.2\\n67.4\\n19.0\\n99.4\\n69.8\\n17.1\\nEVOAGENT(3,3)\\n97.2\\n68.5\\n17.1\\n99.4\\n68.4\\n16.7\\nAll-in\\nEVOAGENT(2,3)\\n95.0\\n65.1\\n16.7\\n99.4\\n69.0\\n19.0\\nEVOAGENT(3,3)\\n95.0\\n66.9\\n17.9\\n100.0\\n70.1\\n20.7\\nFigure 4, model performance improves with increasing iterations.\\nHowever, the improvement plateaus when the iteration count exceeds three. We suggest that traditional\\nNLP tasks are relatively simple, and beyond a certain iteration number, even with a quality-check\\nmodule in place, the generated agents tend to be similar and thus converge.\\nF\\nExamples of EVOAGENT\\nF.1\\nEVOAGENT Examples of NLP reasoning and knowledge tasks\\nListing 5, 6 and7 presents some multi-agent generation examples generated by GPT-4 based EVOA-\\nGENT in Logic Grid Puzzle, Trivia Creative Writing and Codenames Collaborative for a better\\nunderstanding.\\nF.2\\nEVOAGENT Examples of MMMU\\nListing 8 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nMMMU dataset for a better understanding.\\nF.3\\nEVOAGENT Examples of ScienceWorld\\nListing 9 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nScienceWorld for a better understanding.\\nF.4\\nEVOAGENT Examples of TravelPlanner\\nListing 10 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nTravelPlanner for a better understanding.\\n15\\nCamel Framework\\nAI Assistant\\nAI User\\nAI User\\nAI User\\nEvoAgent\\nAutoGen Framework\\nAssistant Agent\\nUserProxy \\nAgent\\nGeopolitical Analyst \\nAgent\\nLabor Economist \\nAgent\\nEvoAgent\\nPublic Health \\nAgent\\nGroup Chat Manager\\nTopic: The U.S. should commit more in climate change fighting\\nLabor Economist \\nAgent\\nGeopolitical Analyst \\nAgent\\nFigure 5: The adaption of EVOAGENT on Camel and AutoGen frameworks.\\nG\\nExamples of EVOAGENT’s Adaption to Multi-agent Collaboration\\nFrameworks\\nG.1\\nEVOAGENT for MetaGPT\\nMetaGPT [10] is a meta-programming framework that enhances LLM-based multi-agent collabo-\\nrations by integrating efficient human workflows. It employs an assembly line approach to assign\\ndiverse roles to agents, effectively simplifying complex tasks into manageable subtasks that multi-\\nple agents can execute collaboratively. As shown in Figure 3, instead of manually creating roles,\\nEVOAGENT can be used to automatically generate specialized agents for effective collaboration.\\nG.2\\nEVOAGENT for Camel\\nCamel [13] is recognized for its framework that supports communicative role-playing agents. Initially,\\nhumans establish this framework by conceptualizing an idea and designing specific roles, such as the\\nAI assistant role and the AI user role. These roles are then assigned to the assistant and user agents,\\nrespectively, enabling them to fulfill the task. As illustrated in Figure 5, EVOAGENT can be utilized\\nto automatically produce agents from AI assistants for interaction with AI users, bypassing the need\\nfor manual role design.\\nG.3\\nEVOAGENT for AutoGen\\nAutoGen [12] offers a framework that enables the creation of customizable and conversable agents\\nby integrating various LLMs. Initially, humans configure the assistant agents along with a user proxy\\nagent. Then, a group chat manager is responsible for selecting a speaker, gathering responses, and\\ndisseminating the message. As depicted in Figure 5, EVOAGENT facilitates the creation of multiple\\nexpert roles from a single assistant agent, thereby increasing the agent number in group chats without\\nthe need for manual design.\\n16\\nListing 1: Instruction templates for for 0-shot learning (Direct), Chain-of-thought (CoT) prompting\\nand Self-Refine method\\nDirect Method:\\n{question}\\nAnswer:\\nCoT Method:\\n{question}\\nYou need to give reasons first and then give the answer.\\nAnswer:\\nSelf-Refine Method:\\nStep One: Feedback Generation:\\nYou are a helpful assistant that provides feedback on {task}\\n{question}\\nThis is the answer from a student: {answer}.\\nPlease do not refine the answer but give some insightful suggestions for\\nthe student to help him better answer the question.\\nSuggestion:\\nStep Two: Result Refine:\\n{question}\\nThis is your answer:\\n{answer}\\nThere is the suggestion from an assistant:\\nSuggestion: {feedback}\\nNow you can refine your answer with his suggestion to better answer the\\nquestion.\\nKeep in mind that his suggestion may not be correct, so critically decide\\nwhether to accept his response or stick with your original one.\\nYou need to give reasons first and then give the answer.\\nRevised Answer:\\n17\\nListing 2: Instruction templates for SPP method\\nWhen faced with a task, begin by identifying the participants who will\\ncontribute to solving the task. Then, initiate a multi-round\\ncollaboration process until a final solution is reached. The participants\\nwill give critical comments and detailed suggestions whenever necessary.\\nHere are some examples:\\n---\\nExample Task 1: Use numbers and basic arithmetic operations (+ - * /) to\\nobtain 24. You need to use all numbers, and each number can only be used\\nonce.\\nInput: 6 12 1 1\\nParticipants: AI Assistant (you); Math Expert\\nStart collaboration!\\nMath Expert: Let’s analyze the task in detail. You need to make sure that\\nyou meet the requirement, that you need to use exactly the four numbers\\n(6 12 1 1) to construct 24. To reach 24, you can think of the common\\ndivisors of 24 such as 4, 6, 8, 3 and try to construct these first. Also\\nyou need to think of potential additions that can reach 24, such as 12 +\\n12.\\nAI Assistant (you): Thanks for the hints! Here’s one initial solution:\\n(12 / (1 + 1)) * 6 = 24\\nMath Expert: Let’s check the answer step by step. (1+1) = 2, (12 / 2) =\\n6, 6 * 6 = 36 which is not 24! The answer is not correct. Can you fix\\nthis by considering other combinations? Please do not make similar\\nmistakes.\\nAI Assistant (you): Thanks for pointing out the mistake. Here is a\\nrevised solution considering 24 can also be reached by 3 * 8: (6 + 1 + 1)\\n* (12 / 4) = 24.\\nMath Expert: Let’s first check if the calculation is correct. (6 + 1 + 1)\\n= 8, 12 / 4 = 3, 8 * 3 = 24. The calculation is correct, but you used 6\\n1 1 12 4 which is not the same as the input 6 12 1 1. Can you avoid using\\na number that is not part of the input?\\nAI Assistant (you): You are right, here is a revised solution considering\\n24 can be reached by 12 + 12 and without using any additional numbers: 6\\n* (1 - 1) + 12 = 24.\\nMath Expert: Let’s check the answer again. 1 - 1 = 0, 6 * 0 = 0, 0 + 12 =\\n12. I believe you are very close, here is a hint: try to change the \"1 -\\n1\" to \"1 + 1\".\\nAI Assistant (you): Sure, here is the corrected answer:\\n6 * (1+1) + 12 =\\n24\\nMath Expert: Let’s verify the solution. 1 + 1 = 2, 6 * 2 = 12, 12 + 12 =\\n12. You used 1 1 6 12 which is identical to the input 6 12 1 1.\\nEverything looks good!\\nFinish collaboration!\\nFinal answer: 6 * (1 + 1) + 12 = 24\\n---\\n{question}\\n18\\nListing 3: Instruction templates for EVOAGENT\\nCrossover and Mutation:\\n{question}\\nThis is your result:\\n{answer}\\nNow, you can create and collaborate with multiple experts to improve your\\nresult. Therefore, please describe in as much detail as possible the\\ndifferent skills and focuses you need from multiple experts individually.\\nWe will provide each expert with the same information and query. However\\n, please note that each profession has its own specialization, so you can\\nassign each expert to just one sub-task to ensure a more refined\\nresponse. We will relay their responses to you in turn, allowing you to\\nreorganize them into a better answer. Please note that the description\\nshould be narrated in the second person, for example: You are a XXX.\\nThese are the descriptions of the experts you have created before for\\nthis task:\\n{description}\\nTherefore, please remember you should not repeatedly create the same\\nexperts as described above.\\nNow, you can give the description for a new expert (Please note that only\\nbe one, do not give multiple at one time):\\nQuality Check:\\n{question}\\nWe employ mulitple experts to answer this query. The following is a\\nsecond-person introduction to the experts we have hired:\\n{description_ls}\\nNow, we will hire a new expert to help better respond to user query. Here\\nis a second person description of the new expert: {description}\\nPlease evaluate the new expert based on the following criteria to decide\\nwhether they should be retained or not:\\n1. The new expert is distinct and does not duplicate any previously hired\\nexperts.\\n2. Based on the new expert’s description, determine if they can\\neffectively assist in answering users’ questions.\\nGive the reason first and then give the choice. If retaining, please\\nreply with: Retain. If discarding, please reply with: Discard.\\nResult Update:\\n{question}\\nThis is your result:\\n{old_answer}\\nYou invite an expert whose description is: {description}\\nThis expert also give his answer based on his own professional knowledge:\\n{new_answer}.\\nNow you can refine your result with his answer to better answer the\\nquestion.\\nKeep in mind that his answer may not be correct, so critically decide\\nwhether to accept his response or stick with your original one.\\nRevised Answer:\\n19\\nListing 4: Instruction templates of different ablation settings in EVOAGENT.\\nPK:\\n{question}\\nWe invite {n} experts. They give the results based on their own\\nprofessional knowledge:\\nHere are second-person descriptions of these experts with their answers:\\n{select}\\nNow you can should help us select the best result which can meet the\\nquery.\\nYou need to give reasons first and then give the answer with the format:\\n\"Final Answer: Expert #XX\"\\nAll-in:\\n{question}\\nThis is your answer: {old_answer}.\\nFurthermore, you also invite {n} experts. They also give answers based on\\ntheir own professional knowledge:\\nHere are second person descriptions of these experts with their answers:\\n{description_ls}\\nNow you can refine your answer with these answers to better meet the\\nquery.\\nSuggest:\\n{Expert_Agent_description}\\n{question}\\nThis is the result from an AI assistant: {answer}.\\nPlease do not refine the plan but give some insightful suggestions for\\nthe AI assistant to help it better meet the user’s query.\\nSuggestion:\\nOverGen:\\n{question}\\nPlease generate three different results at one time for user to choose\\nfrom. The format can be:\\nResult #1:\\nResult #2:\\nResult #3:\\nThree Different Candidate Results:\\nPromptRefine:\\n{question}\\nThis is the result from an AI assistant, which description is\\n\"{original_description}\":\\n{answer}.\\nPlease do not refine the result but refine the description of the AI\\nassistant to help it better answer the user’s query.\\nPlease note that the description should be narrated in the second person,\\nfor example: You are a XXX.\\nDescription:\\n20\\nListing 5: An example of multi-agent generation from EVOAGENT for Logic Grid Puzzle\\nQuestion: There are 5 houses in a row, numbered 1 on the left to 5 on the\\nright. There is one person living in each house. The people in these\\nhouses have different characteristics:\\n- Each person has a different kind of car: one owns an SUV, one owns a\\nsedan, one drives a convertible, one drives a minivan, and one drives a\\ntruck\\n- Each person has a favorite color: one likes red, one likes yellow, one\\nlikes green, one likes white, and one likes blue\\n- Each person is wearing different shoes: one is wearing running shoes,\\none is wearing boots, one is wearing high heels, one is wearing sandals,\\nand one is wearing loafers\\nClue(s):\\n1. There is one house between where the person who is wearing loafers\\nlives and where the person who drives a minivan lives. 2. The person who\\ndrives a truck does not live in the fourth house. 3. The person who\\ndrives a truck lives directly left of the person who is wearing sandals.\\n4. The person who likes red lives somewhere to the right of the person\\nwho likes yellow. 5. The person who is wearing loafers lives somewhere to\\nthe left of the person who drives a truck. 6. The person who likes blue\\nlives in the fifth house. 7. There are two houses between where the\\nperson who owns a sedan lives and where the person who is wearing running\\nshoes lives. 8. The person who is wearing boots lives somewhere to the\\nleft of the person who drives a truck. 9. The person who likes white\\nlives in the fourth house. 10. There is one house between where the\\nperson who likes green lives and where the person who drives a truck\\nlives. 11. The person who owns an SUV does not live in the first house.\\nWhat is the number of the house where the person who is wearing boots\\nlives? choice: 2, choice: 5, choice: 3, choice: 1, choice: 4.\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a logician, specializing in logical reasoning and problem-solving\\n. Your task is to analyze the given clues and use deductive reasoning to\\ndetermine the correct sequence of houses, their occupants, and their\\ncharacteristics. You will need to consider each clue carefully,\\nunderstanding how it relates to the others and how it narrows down the\\npossibilities. Your focus should be on ensuring the logical consistency\\nof the solution, making sure that all clues are satisfied without\\ncontradiction.\\nReason: From clue 6 and 9, we know that the person who likes blue lives\\nin the fifth house and the person who likes white lives in the fourth\\nhouse. From clue 4, the person who likes red lives somewhere to the right\\nof the person who likes yellow. Since the fourth and fifth houses are\\noccupied by the people who like white and blue, the person who likes red\\nmust live in the third house and the person who likes yellow must live in\\nthe first or second house.From clue 10, there is one house between where\\nthe person who likes green lives and where the person who drives a truck\\nlives. Since the third, fourth, and fifth houses are occupied by the\\npeople who like red, white, and blue, the person who likes green must\\nlive in the first or second house. Therefore, the person who likes yellow\\nmust live in the first house and the person who likes green must live in\\nthe second house.From clue 11, the person who owns an SUV does not live\\nin the first house. So, the person who owns an SUV must live in the\\nsecond, third, or fourth house.From clue 2, the person who drives a truck\\ndoes not live in the fourth house. So, the person who drives a truck\\nmust live in the second or third house.From clue 3, the person who drives\\na truck lives directly left of the person who is wearing sandals. So,\\nthe person who drives a truck must live in the second house and the\\nperson who is wearing sandals must live in the third house.From clue 8,\\nthe person who is wearing boots lives somewhere to the left of the person\\nwho drives a truck. So, the person who is wearing boots must live in the\\nfirst house.\\nAnswer: choice: 1\\n21\\nListing 6: An example of multi-agent generation from EVOAGENT for Trivia Creative Writing\\nWrite a short and coherent story about Zootopia that incorporates the\\nanswers to the following 5 questions:\\n1. Following success at the 1979 election whose party conference speeech\\nincluded the lines ’you turn if you want to, the lady’s not for turning’?\\n2. Which snooker player was simply known as ’The Grinder’?\\n3. Which former West Indian fast bowler was known as ’Whispering Death’?\\n4. Which Eastenders character had a dog called Willy?\\n5. In which year did the first episode of Minder take place?\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a sports historian with a deep understanding of snooker and\\ncricket from the 1970s and 1980s. You are familiar with the careers and\\nplaying styles of Cliff Thorburn, known as ’The Grinder’, and Michael\\nHolding, known as ’Whispering Death’. Your role in this task will be to\\nprovide context and insight into these sports figures and their\\nsignificance in the story about Zootopia.\\nExpert Agent #2 Description:\\nYou are a political historian with a deep understanding of the political\\nclimate and key figures of the 1970s and 1980s, particularly in the\\nUnited Kingdom. You have extensive knowledge about Margaret Thatcher, her\\npolicies, and her impact on society. Your role in this task will be to\\nprovide context and insight into the significance of the reference to\\nThatcher’s famous speech in the story about Zootopia.\\nStory: In the vibrant city of Zootopia, the year was 1979, a time of\\nsignificant change and excitement. The city had just elected its first\\nfemale mayor, a lioness named Margaret Thatcher, known for her unwavering\\ndetermination and strong leadership. Her victory speech at the party\\nconference echoed the famous line from the human world’s Margaret\\nThatcher, \"You turn if you want to, the lady’s not for turning.\" This\\nphrase resonated with the citizens of Zootopia, who admired her\\nsteadfastness and determination. In the heart of the city, a meticulous\\nbadger known as ’The Grinder’ ran a popular snooker club. His nickname\\nwas a tribute to the legendary human snooker player Cliff Thorburn, known\\nfor his relentless and meticulous style of play. The Grinder’s snooker\\nclub was a hub of social activity, where animals from all walks of life\\ncame to unwind and engage in friendly competition. Meanwhile, a new\\nrecruit had joined the Zootopia Police Department, a cheetah named\\nMichael Holding. Known as ’Whispering Death’ during his cricketing days,\\na moniker borrowed from the former West Indian fast bowler, he was now a\\nrespected figure in the community, using his speed and stealth to uphold\\nthe law. In another part of the city, a popular soap opera was being\\nfilmed. The show was called \"Eastenders of Zootopia,\" and one of the\\nbeloved characters was a rabbit named Ethel Skinner, who had a dog named\\nWilly. This was a charming reference to the human Eastenders character of\\nthe same name, symbolizing the enduring friendships that were a\\ncornerstone of Zootopian society. The year 1979 was also significant as\\nit marked the airing of the first episode of the Zootopian version of the\\nhuman show \"Minder\". The show was a reflection of the city’s vibrant and\\ndiverse culture, much like the original British series. It was a\\ntestament to the city’s ability to adapt and evolve, while still holding\\nonto its rich history and traditions. In conclusion, Zootopia was a\\nmelting pot of cultures and histories, much like the television shows and\\ncharacters it referenced. These references served as a bridge between\\nthe past and the present, reminding the citizens of their roots while\\nencouraging them to embrace the future.\\nEvaluation: correct_count=5/5\\n22\\nListing 7: An example of multi-agent generation from EVOAGENT for Codenames Collaborative\\nTry to identify the 3 words best associated with the word \"fruit\" from\\nthe following word list:\\n[\"bowl\", \"einstein\", \"banana\", \"brain\", \"judge\", \"groom\", \"crusader\", \"\\nstamp\", \"peach\", \"halloween\", \"troll\", \"slipper\", \"earthquake\", \"tip\", \"\\nsun\", \"quarter\", \"pad\", \"nude\", \"helmet\", \"laundry\", \"wedding\", \"mirror\",\\n\"sleep\", \"minotaur\", \"wish\"].\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a cultural anthropologist with a focus on symbolism and rituals.\\nYour expertise in understanding how different objects, including fruits,\\nare used and symbolized in various cultures will be beneficial in this\\ntask. You can provide insights into how certain words might be associated\\nwith \"fruit\" from a cultural or symbolic perspective. Your focus will be\\non the cultural and symbolic aspects of the task.\\nReason:\\n1. ’Banana’: This is a type of fruit, rich in potassium and dietary fiber\\n. It is commonly consumed raw and is used in a variety of dishes like\\ndesserts, smoothies, and baked goods.\\n2. ’Peach’: This is also a type of fruit, known for its sweet and juicy\\nnature. Peaches are rich in vitamins A and C and are often eaten raw or\\nused in dishes like pies, jams, and salads.\\n3. ’Bowl’: This word is associated with fruit in the context of a fruit\\nbowl, which is a common way to store and display various types of fruit.\\nA bowl of fruit can also be a healthy snack or dessert option.\\nAnswer: Banana, Peach, Bowl\\nExpert Agent #2 Description:\\nYou are a nutritionist with a deep understanding of different types of\\nfood, including fruits. Your knowledge about the nutritional value,\\ncommon uses, and cultural significance of various fruits will be helpful\\nin this task. You can provide insights into how certain words might be\\nassociated with \"fruit\" from a dietary or nutritional perspective. Your\\nfocus will be on the nutritional and dietary aspects of the task.\\nReason:\\n1. ’Banana’: This is a direct association as a banana is a type of fruit.\\nIn many cultures, bananas symbolize fertility and prosperity. In\\nHinduism, for example, bananas are considered a sacred fruit and are used\\nin religious rituals.\\n2. ’Peach’: Similar to banana, a peach is also a type of fruit. In\\nChinese culture, peaches are symbols of immortality and unity. They are\\noften used in traditional rituals and ceremonies.\\n3. ’Bowl’: A bowl is often used to hold fruits. In many cultures, a bowl\\nof fruit is a symbol of abundance, fertility, and prosperity. It is often\\nused in rituals and ceremonies as an offering or a centerpiece.\\nAnswer: banana, peach, bowl\\nFinal Answer:\\nAfter considering the expert’s input, I find that their reasoning aligns\\nwith my initial thoughts. The words ’banana’ and ’peach’ are indeed types\\nof fruit, and their nutritional value and common uses further strengthen\\ntheir association with the word ’fruit’. The word ’bowl’ is also\\nassociated with fruit in the context of a fruit bowl, which is a common\\nway to store and display various types of fruit. Therefore, I agree with\\nthe expert’s choices.\\nFinal Answer: banana, peach, bowl.\\n23\\nListing 8: An example of multi-agent generation from EVOAGENT for MMMU dataset.\\nQuestion: <image 1> The elliptical orbit of a planet around the Sun is\\nshown on the diagram. Which of the following statements is true?\\nA. the eccentricity of the orbit is less than zero\\nB. the eccentricity of the orbit is greater than 1\\nC. the sun might be at point C\\nD. the sun might be at point D\\nE. the sun might be at point B (Correct)\\nCoT:\\nReason: In an elliptical orbit, the Sun is located at one of the two foci\\nof the ellipse. The eccentricity of an orbit is a measure of how much\\nthe orbit deviates from a perfect circle, with 0 being a perfect circle\\nand 1 being a parabola. Since the orbit shown is an ellipse, the\\neccentricity must be between 0 and 1. Points A and E are the farthest\\npoints from the center of the ellipse, so the Sun cannot be located at\\neither of those points. Point B is at the center of the ellipse, so the\\nSun cannot be located there either. This leaves points C and D as\\npossible locations for the Sun. However, since the Sun must be at one of\\nthe foci and point D is closer to the center of the ellipse than point C,\\nit is more likely that the Sun is located at point C.\\nAnswer: C. the sun might be at point C\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a mathematician with a specialization in geometry, particularly\\nin the study of conic sections which include ellipses. Your expertise\\nallows you to understand the properties and definitions of ellipses,\\nincluding the concept of foci. You can provide a mathematical perspective\\non why the Sun must be located at one of the foci of the elliptical\\norbit and not at any other point. Your knowledge will help explain the\\ngeometric reasons behind the positioning of the Sun in relation to the\\nplanet’s orbit, and why the given options in the statements do not align\\nwith the mathematical definition of an ellipse.\\nReason: An ellipse is defined as the set of all points for which the sum\\nof the distances to two fixed points, called foci, is constant. The Sun,\\nbeing one of the massive bodies in the solar system, must be located at\\none of the foci of the elliptical orbit of a planet due to the\\ngravitational forces that govern the motion of the planets. Option E is\\ncorrect because the Sun might be at point B, which is one of the foci of\\nthe ellipse. The other focus would be at a point symmetrically opposite\\nto B along the major axis.\\nAnswer: E. the sun might be at point B.\\nExpert Agent #2 Description:\\nYou are an astronomer with a specialization in observational astronomy.\\nYour expertise lies in the practical observation and data collection of\\ncelestial bodies, including the use of telescopes and other instruments\\nto study the positions and movements of planets and stars. You have a\\ndeep understanding of how the apparent motion of celestial bodies can be\\nused to infer their actual motion in space. You can provide insight into\\nhow observations of planetary orbits, such as the one depicted in the\\ndiagram, have historically been used to deduce the laws of planetary\\nmotion and the positions of stars like the Sun. Your knowledge will help\\nexplain how practical observations support the theoretical understanding\\nthat the Sun must be located at one of the foci of the elliptical orbit.\\nReason: The eccentricity of an orbit is a measure of how much it deviates\\nfrom being a perfect circle. It ranges from 0 (a perfect circle) to 1 (a\\nparabola). An eccentricity less than zero or greater than 1 is not\\npossible for an elliptical orbit. Therefore, options A and B are\\nincorrect. According to Kepler’s laws of planetary motion, the Sun must\\nbe located at one of the foci of the elliptical orbit. Point B is located\\nat a position that could be one of the foci of the ellipse.\\nAnswer: E. the sun might be at point B.\\n24\\nListing 9: An example of multi-agent generation from EVOAGENT for ScienceWorld.\\nTask 1: Your task is to determine if a metal fork is electrically\\nconductive. The metal fork is located around the kitchen. First, focus on\\nthe metal fork. If conductive, place it in the blue box. If\\nnonconductive, place it in the green box.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a thermodynamics expert. Your expertise in heat transfer and\\ntemperature measurement is crucial for this task. You understand how to\\nproperly use a thermometer and interpret its readings. You also have\\nknowledge about the properties of various substances, including their\\ntypical temperature ranges. Your task is to guide the assistant in using\\nthe thermometer to accurately measure the temperature of the unknown\\nsubstance B.\\nTask 2: Your task is to boil gallium. For compounds without a boiling\\npoint, combusting the substance is also acceptable. First, focus on the\\nsubstance. Then, take actions that will cause it to change its state of\\nmatter.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a metallurgist with a deep understanding of the properties and\\nbehaviors of metals, including gallium. Your expertise includes knowledge\\nof the melting and boiling points of various metals, as well as the\\nprocesses and equipment needed to heat them to these temperatures. You\\nare also familiar with safety procedures and precautions when handling\\nand heating metals. Your task is to provide detailed instructions on how\\nto safely and effectively boil gallium in the foundry.\\nTask 3: Your task is to change the state of matter of gallium. First,\\nfocus on the substance. Then, take actions that will cause it to change\\nits state of matter.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a chemist with a specialization in the properties and behaviors\\nof substances. You have a deep understanding of the state changes of\\nmatter, specifically how substances like gallium can transition from\\nsolid to liquid or gas. You are familiar with the conditions required for\\nthese changes to occur, such as temperature and pressure. You also have\\nknowledge of the equipment and procedures used in a foundry, which will\\nbe useful in this task. Your role is to provide guidance on how to\\nmanipulate the conditions in the foundry to achieve the desired state\\nchange in gallium.\\n25\\nListing 10: An example of multi-agent generation from EVOAGENT for TravelPlanner dataset.\\nQuestion: Can you help me with a travel plan departing from Buffalo to\\nAtlanta for a duration of 3 days, specifically from March 2nd to March 4\\nth, 2022? I plan to travel alone and my planned budget for the trip is\\naround $1,100.\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a culinary expert with a deep understanding of food culture in\\nAtlanta. Your expertise includes knowledge of the best local restaurants,\\npopular dishes, and dietary restrictions. You can provide\\nrecommendations for breakfast, lunch, and dinner spots that offer a\\nunique taste of Atlanta’s food scene. Your suggestions should consider\\nthe traveler’s budget and the proximity of the restaurants to the\\nattractions and accommodation listed in the plan.\\nExpert Agent #2 Description:\\nYou are a travel logistics expert with extensive knowledge of\\ntransportation options and costs between Buffalo and Atlanta. Your\\nexpertise includes understanding flight schedules, costs, and durations,\\nas well as alternative transportation methods such as self-driving and\\ntaxi services. You can provide the most cost-effective and time-efficient\\ntransportation options for the traveler, considering their budget and\\nthe travel dates provided in the plan.\\nExpert Agent #3 Description:\\nYou are a local tourism expert with comprehensive knowledge of Atlanta’s\\nattractions. Your expertise includes understanding the city’s popular\\ntourist spots, historical sites, museums, parks, and unique local\\nexperiences. You can provide recommendations for attractions that the\\ntraveler can visit during their stay, considering their budget and the\\nproximity of these attractions to the traveler’s accommodation and meal\\nspots. Your suggestions should also take into account the operating hours\\nof these attractions to ensure they align with the traveler’s schedule.\\nAnswer:\\nDay 1:\\nCurrent City: from Buffalo to Atlanta\\nTransportation: Flight Number: F3502691, from Buffalo to Atlanta,\\nDeparture Time: 18:48, Arrival Time: 20:51\\nBreakfast: Not applicable as the flight is in the evening.\\nAttraction: Not applicable as the flight is in the evening.\\nLunch: Not applicable as the flight is in the evening.\\nDinner: Saffron, Atlanta\\nccommodation: Spacious private room close St. Barnabas Hospital, Atlanta\\nDay 2:\\nCurrent City: Atlanta\\nTransportation: -\\nBreakfast: Adda, Atlanta\\nAttraction: Atlanta Botanical Garden, Atlanta; World of Coca-Cola,\\nAtlanta.\\nLunch: Baba Au Rhum, Atlanta\\nDinner: Asian Bistro, Atlanta\\nAccommodation: Spacious private room close St. Barnabas Hospital, Atlanta\\nDay 3:\\nCurrent City: from Atlanta to Buffalo\\nTransportation: Flight Number: F3500648, from Atlanta to Buffalo,\\nDeparture Time: 21:24, Arrival Time: 23:26\\nBreakfast: Chef Style, Atlanta\\nAttraction: Georgia Aquarium, Atlanta; Martin Luther King, Jr. National\\nHistorical Park, Atlanta.\\nLunch: Pizza Central, Atlanta\\nDinner: Daawat-e-Kashmir, Atlanta\\nAccommodation: -.\\n26\\n'),\n",
              " Document(metadata={'Published': '2020-11-02', 'Title': 'Cooperative Heterogeneous Deep Reinforcement Learning', 'Authors': 'Han Zheng, Pengfei Wei, Jing Jiang, Guodong Long, Qinghua Lu, Chengqi Zhang', 'Summary': 'Numerous deep reinforcement learning agents have been proposed, and each of\\nthem has its strengths and flaws. In this work, we present a Cooperative\\nHeterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn a\\npolicy by integrating the advantages of heterogeneous agents. Specifically, we\\npropose a cooperative learning framework that classifies heterogeneous agents\\ninto two classes: global agents and local agents. Global agents are off-policy\\nagents that can utilize experiences from the other agents. Local agents are\\neither on-policy agents or population-based evolutionary algorithms (EAs)\\nagents that can explore the local area effectively. We employ global agents,\\nwhich are sample-efficient, to guide the learning of local agents so that local\\nagents can benefit from sample-efficient agents and simultaneously maintain\\ntheir advantages, e.g., stability. Global agents also benefit from effective\\nlocal searches. Experimental studies on a range of continuous control tasks\\nfrom the Mujoco benchmark show that CHDRL achieves better performance compared\\nwith state-of-the-art baselines.', 'entry_id': 'http://arxiv.org/abs/2011.00791v1', 'published_first_time': '2020-11-02', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.AI'], 'links': ['http://arxiv.org/abs/2011.00791v1', 'http://arxiv.org/pdf/2011.00791v1']}, page_content='Cooperative Heterogeneous Deep Reinforcement\\nLearning\\nHan Zheng\\nAAII,University of Technology Sydney\\nHan.Zheng-1@student.uts.edu.au\\nPengfei Wei\\nNational University of Singapore\\nwpf89928@gmail.com\\nJing Jiang\\nAAII,University of Technology Sydney\\njing.jiang@uts.edu.au\\nGuodong Long\\nAAII,University of Technology Sydney\\nguodong.long@uts.edu.au\\nQinghua Lu\\nData61, CSIRO\\nqinghua.lu@data61.csiro.au\\nChengqi Zhang\\nAAII,University of Technology Sydney\\nChengqi.Zhang@uts.edu.au\\nAbstract\\nNumerous deep reinforcement learning agents have been proposed, and each\\nof them has its strengths and ﬂaws. In this work, we present a Cooperative\\nHeterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn\\na policy by integrating the advantages of heterogeneous agents. Speciﬁcally, we\\npropose a cooperative learning framework that classiﬁes heterogeneous agents into\\ntwo classes: global agents and local agents. Global agents are off-policy agents\\nthat can utilize experiences from the other agents. Local agents are either on-policy\\nagents or population-based evolutionary algorithms (EAs) agents that can explore\\nthe local area effectively. We employ global agents, which are sample-efﬁcient,\\nto guide the learning of local agents so that local agents can beneﬁt from sample-\\nefﬁcient agents and simultaneously maintain their advantages, e.g., stability. Global\\nagents also beneﬁt from effective local searches. Experimental studies on a range of\\ncontinuous control tasks from the Mujoco benchmark show that CHDRL achieves\\nbetter performance compared with state-of-the-art baselines.\\n1\\nIntroduction\\nDeep reinforcement learning (DRL) integrates deep neural networks with reinforcement learning\\nprinciples, e.g.,Q-learning and policy-gradient, to create a more efﬁcient agent. Recent studies have\\nshown a great success of DRL in numerous challenging real-world problems, e.g., video games and\\nrobotic control [1]. Although promising, existing DRL algorithms still suffer from several challenges\\nincluding sample complexity, instability, and temporal credit assignment problems [2, 3].\\nOne popular research line of DRL is policy-gradient based on-policy methods attempting to evaluate\\nor improve the same policy that is used to make decisions [4], e.g., trust region policy optimization\\n(TRPO) [5] and proximal policy optimization (PPO) [6]. Recent works [7, 8] have proved that\\npolicy-gradient based methods can converge to a stationary point under some conditions, which\\ntheoretically guarantees their stability. However, they are extremely sample-expensive since they\\nrequire new samples to be collected in each gradient step [4].\\nOn the contrary, Q-learning based off-policy methods, which is another research line evaluating or\\nimproving a policy different from the one that is used to generate the behavior, can improve sample\\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\\narXiv:2011.00791v1  [cs.LG]  2 Nov 2020\\nFigure 1: The high-level structure of CHDRL for one iteration\\nefﬁciency by reusing past experiences [4]. Existing off-policy based methods include deep Q-learning\\nnetwork (DQN) [1] and Soft Actor-Critic (SAC) [9] etc. These methods involve the approximation of\\nsome high-dimensional and nonlinear functions, usually through deep neural networks, which poses\\na signiﬁcant challenge on convergence and stability [10, 3]. It is also well known that off-policy Q\\nlearning is not to converge even with linear function approximation [11]. Moreover, recent studies\\n[12, 13] identify some other key sources of instability for off-policy methods, i.e., bootstrapping and\\nextrapolation errors. As shown in [12], off-policy methods are highly sensitive to data distribution,\\nand can only make limited progress without exploiting additional on-policy data.\\nIn addition to the pros and cons discussed above, on-policy and off-policy methods based on temporal\\ndifference learning suffer from some common issues. The one that received much research attention\\nis the so-called temporal credit assignment problem [2]. When rewards become sparse or delayed,\\nwhich is quite common in real-world problems, DRL algorithms may yield an inferior performance\\nas reward sparsity downgrades the learning efﬁciency and hinders exploration. To alleviate this issue,\\nevolutionary algorithms (EAs) [14, 15] have recently been introduced to DRL [16, 17]. The usage of\\na ﬁtness metric that consolidates returns across the entire episode makes EAs indifferent to reward\\nsparsity and robust to long time horizons [18]. However, EAs suffer from high sample complexity\\nand struggle to solve high-dimension problems involving massive parameters.\\nIn this paper, we are interested in an algorithm that takes the essence and discards the dross of\\ndifferent DRL algorithms to achieve high sample efﬁciency and maintain good stability in various\\ncontinuous control tasks. To do so, we propose a framework called CHDRL. Speciﬁcally, CHDRL\\nworks on an agent pool containing three classes of agents: an off-policy agent, an on-policy agent,\\nand a opulation-based EAs agent. All the agents cooperate based on the following three mechanisms.\\nFirstly, all agents collaboratively explore the solution space following a hierarchical policy transfer\\nrule. As the off-policy agent is sample-efﬁcient, we take it as the global agent to obtain a relatively\\ngood policy or value function at the beginning. The on-policy agent and the population-based EAs\\nagent are taken as local agents and start their exploration with the prior knowledge transferred from\\nthe global agent. As the EAs agent is population-based, we further allow it to accept policies from\\nthe on-policy agent.\\nSecondly, we employ a local-global memory replay to enable global (off-policy) agents to replay the\\nnewly generated experiences by local (on-policy) agents more frequently so that global agents can\\nbeneﬁt from local search. Note that, with policy transfer as stated above, local agents start exploration\\nwith a policy transferred from global agents, and thus their generated experiences can be taken as\\nclose to the on-policy data of global agents’ current policy [12, 13]. By allowing global agents\\nexploits more often from these local experiences, we can alleviate the bootstrapping or extrapolation\\nerror and further boost global agents’ learning. Consequently, global agents provide a better starting\\npoint for local agents who in turn generate more diverse local experiences for global agents’ replay,\\nwhich forms a good win-win cycle.\\nThirdly, although we encourage the cooperation among agents in exploration, we also tend to\\nmaintain the independence of each agent; that is, we do not want the learning of local agents to be\\ncompletely dominated by that of global agents. This is to enable each agent to still maintain its policy\\nupdating scheme and preserve its learning advantage. To do so, we ﬁrstly develop a loosely coupled\\nhierarchical framework with global agents at the upper-level and local agents at the lower-level1.\\nSuch a framework not only makes each agent generally run in a relatively independent environment\\n1Policy transfer only happens from upper-level agents to lower-level agents.\\n2\\nwith different random settings, but also achieves the easy and ﬂexible deployment or replacement\\nof the agent candidates used in the framework. Secondly, to avoid over-policy-transfer, i.e., policy\\ntransfer happening too frequently thus interrupting the learning stability of local agents, we set a\\nthreshold to control the frequency of policy transfer.\\nThe high-level structure of CHDRL is shown in Figure 1. In this work, we instantiated a CHDRL\\nwith PPO, SAC, and Cross-Entropy-Method (CEM) based EA [19], named CPSC. Experimental\\nstudies showed the superiority of CPSC to several state-of-the-art baselines in a range of continuous\\ncontrol benchmarks. We also conducted ablation studies to verify the three mechanisms.\\n2\\nPreliminaries\\nIn this section, we review the representation of on-policy method, off-policy method, and EAs,\\nnamely, PPO [6], SAC [9], and Cross-Entropy based EA [19].\\n2.1\\nProximal Policy Optimization (PPO)\\nPPO is an on-policy algorithm that trains a stochastic policy. It explores by sampling actions\\naccording to the latest version of its stochastic policy. During training, the policy typically becomes\\nprogressively less random, as the update rule encourages it to exploit rewards that it has already\\nfound. PPO tries to keep new policies close to old.\\n2.2\\nSoft Actor-critic (SAC)\\nSAC is an off-policy algorithm that incorporates an entropy measure of the policy into the reward\\nto encourage exploration. The idea is to learn a policy that acts as randomly as possible while still\\nbeing able to succeed in the task. It is an off-policy actor-critic model that follows the maximum\\nentropy RL framework. The policy is trained with the objective of maximizing the expected return\\nand entropy at the same time.\\n2.3\\nEvolutionary Algorithms and CEM-ES\\nEAs [14, 15] are a class of black-box search algorithms that apply heuristic search procedures inspired\\nby natural evolution. Among EAs, Estimation of Distribution Algorithms (EDAs) are a speciﬁc\\nfamily where the population is represented as a distribution using a covariance matrix [20]. CEM is a\\nsimple EDA where the number of elite individuals is ﬁxed at a certain value. After all individuals of\\na population are evaluated, the top ﬁttest individuals are used to compute the new mean and variance\\nof the population.\\n3\\nRelated Works\\nExperience replay mechanism [21] is widely used in off-policy reinforcement learning to improve\\nsample efﬁciency. DQN [1] randomly and uniformly samples experience from a replay memory. [22]\\nsubsequently expands DQN to develop a prioritized experience replay (PER), which uses a temporal\\ndifference error to prioritize experiences. Zhizheng Zhang et al. [23] introduce an episodic control\\nexperience replay method to quickly latch on to good trajectories. Our local-global memory uses a\\ndifferent strategy: let the off-policy agent learn more from effective on-policy experiences.\\nCHDRL’s cooperative learning mechanism can be discussed in terms of guided policy search (GPS)\\n[24, 25] or evolutionary reinforcement learning (ERL) [16, 17, 26]. For GPS, they generally need to\\nuse the KL divergence to guide how policies are improved. ERL [17] directly transfers the RL agent’s\\npolicy to the EA population, while Pourchot et al. [16] uses the RL’s critic to update half of the EA\\npopulation using the gradient-based technique. The proposed CHDRL is related to GPS and ERL in\\nthe sense that multiple polices work in a hybrid way. However, the main difference between CHDRL\\nand other similar methods is how heterogeneous agents cooperate. Moreover, CHDRL can beneﬁt\\nnot just from off-policy and EA learning schemes but also from the on-policy learning scheme.\\nAnother related area of work is in the training architectures. A3C [27] introduce an asynchronous\\ntraining framework for deep reinforcement learning, showing parallel actor-learners have a stabilizing\\n3\\neffect on training. Babaeizadeh et al. [28] adapt this approach to make efﬁcient use of GPUs.\\nIMPALA [29] uses a central learner to run SGD while asynchronously pulling sample batches from\\nmany actor processes. Horgan et al. [30] proposes a distributed architecture for training DRL that\\nemploys many actors to explore using different policies and prioritizing the generated experiences.\\nHan Zheng et al. [31] introduces a training method to select the best agent for different tasks. All\\nthese methods only focus on one learning scheme, and/or all actors involved are treated equally. On\\nthe contrary, CHDRL distinguishes actors as global actors and local actors that serve for different\\npurposes respectively. Moreover, CHDRL focuses on the cooperation of diverse learning schemes.\\n4\\nCooperative Heterogeneous Deep Reinforcement Learning(CHDRL)\\nIn this section, we ﬁrstly introduce the proposed CHDRL framework and then suggest a practical\\nalgorithm based on it. Our CHDRL mainly follows three mechanisms to achieve cooperative learning\\nof heterogeneous agents: cooperative exploration (CE), local-global memory relay (LGM) and\\ndistinctive update (DU).\\nCooperative Exploration (CE). The key idea of CE is to utilize a sample-efﬁcient agent, such as\\nan off-policy agent, to guide the exploration of the agent with a relatively lower sample efﬁciency,\\ne.g., an on-policy agent. This is done by transferring policies across agents. More precisely, the\\nsample-efﬁcient agent acts as a global agent and conducts a global search ﬁrst. In every iteration, we\\nwant to use the policy and/or value function obtained by the global agent as the prior knowledge to\\nre-initialize local agents so that they can start to exploit from a relatively better position. To do so, we\\nneed to solve three key points: what to transfer, how to transfer, and when to transfer, following the\\nbasic mechanism of transfer learning[32, 33].\\nWhat to Transfer. Different agents may have different policy architectures. The policy could be\\ndeterministic, where it is denoted by a .= µφ(s), or stochastic, where it is denoted by a ∼πφ(·|s). In\\ncontinuous control tasks, the stochastic policy is usually assumed to be sampled from a Gaussian\\ndistribution, and thus it can be represented as:\\na .= µφ(s) + Σ\\nwhere µφ(s) is the mean action, Σ represent a covariance matrix. Typically, Σ may have different\\nforms, e.g., PPO uses a state-independent Σ while SAC utilizes a state-dependent one. However, a\\nsimilar mean policy architecture µφ(s) is used in different methods. Inspired by this, we propose to\\nuse the structurally identical mean function µφ(s) to establish a link between the deterministic and\\nstochastic policies. Then the policy among heterogeneous agents can be shared by transferring µφ(s).\\nHow to Transfer. As shown in Figure 1, policies are transferred following a hierarchical manner. The\\nprinciple is that policies are transferred from upper-level agents with higher sample efﬁciency to the\\nlower-level agents with lower sample efﬁciency. More speciﬁcally, policies are transferred (1) from\\noff-policy agents to both on-policy agents and EAs agents, and (2) from on-policy agents to EAs\\nagents. Note that EAs agents are population-based, and thus we allow them to accept the on-policy\\nagent’s policy to maximize the transfer capacity. To avoid collisions, we use different individuals of\\nEAs’ population to accept policies from different upper-level agents. As EAs agents accept policies\\nfrom both off-policy and on-policy agents, they naturally serve as a pool that stores all the transferred\\npolicies.\\nWhen to Transfer. Policy transfer happens only when upper-level agents ﬁnd a better policy than\\nthe current one of lower-level agents. Lower-level agents then re-initialize the exploration with the\\npolicy transferred from their upper-level agents as the new starting point. In order to compare the\\nperformance of policies, we use the average return as the evaluation metric. To be statistically stable,\\nwe use the average return over ﬁve episodes as the policy’s performance score. Moreover, to avoid\\nthat policy transfer happens too frequently to interrupt the learning stability of lower-level agents, we\\nenable policy transfer only when the performance gap is larger than a predeﬁned threshold.\\nLocal-Global Memory Relay (LGM): Off-policy agents can make more progress when considering\\non-policy data in their learning [12, 13]. Following this observation, we employ a local-global memory\\nreplay mechanism to enable global off-policy agents to beneﬁt from diverse local experiences from\\nboth on-policy agents and EAs agents. In particular, we propose two memory buffers – a global\\none and a local one – to store the generated exploration experiences. The global memory serves to\\nstore the entire exploration experiences of all the agents, while the local memory only stores the\\n4\\nAlgorithm 1 CSPC\\nRequire:\\nGs with policy πs .= µφs(s) + Σs and value ψs;\\nLp with πp .= µφp(s) + Σp and value ψp; local\\nmemory Ml,global memory Mg; Iteration steps T;\\nLc with policies as µφc0 (s), ..., µφcn (s); initial\\nsteps Tg; gap f, terminate step Tm, and initial test\\nscore Ss, Sp Sc. Initialize transfer label Ap, Ac to\\nFalse.\\n1: repeat\\n2:\\nTRAIN(Gs, Tg), t ←t + Tg\\n3:\\nfor Agent a in Gs, Lp, Lc do\\n4:\\nTRAIN(a, Ml, Mg, T)\\n5:\\nif a is not Gs then\\n6:\\nUPDATE(φs, Ml, Mg, T)\\n7:\\nend if\\n8:\\nt ←t + T\\n9:\\nend for\\n10:\\nUpdate test scores Ss, Sp and Sc\\n11:\\nif Ss −Sp > f then\\n12:\\nφp ←φs, ψp ←ψs, Ap ←True\\n13:\\nend if\\n14:\\nif Ss −Sc > f then\\n15:\\nφc0 ←φs, Ac ←True\\n16:\\nend if\\n17:\\nif Sp −Sc > f then\\n18:\\nφc1 ←φp\\n19:\\nend if\\n20: until t > Tm\\nAlgorithm 2 TRAIN\\nRequire:\\nInput agent a,\\ntraining steps Ta,\\nepisode reward R = 0,\\nRm ←min(Ss, Sp, Sc),\\nstep t = 0, te = 0,\\nglobal memory Mg,local memory Ml\\nepisode memory Me.\\n1: repeat\\n2:\\nObserve state s and select action a ∼µφs(s) +\\nΣs or a ∼µφp(s) + Σp or a .= µφci (s)\\n3:\\nExecute a in the environment\\n4:\\nObserve next state s′,reward r,and done signal\\nd\\n5:\\nStore (s, a, r, s′, d) in Me, R ←r + R\\n6:\\nt ←t + 1, te ←te + 1\\n7:\\nif s′ is terminal then\\n8:\\nφ′ ←UPDATE(φ, Mg, Ml, te) where φ ∈\\n{φs, φp, φc}\\n9:\\nif R > Rm and (a is Gs or Ap or Ac is True)\\nthen\\n10:\\nStore Me in Ml and Mg\\n11:\\nelse\\n12:\\nStore Me in Mg\\n13:\\nend if\\n14:\\nR ←0, Me ←[], te ←0\\n15:\\nend if\\n16: until t > Ta\\nrecently generated ones. Thus, we set an expandable global memory size increasing while learning,\\nbut a ﬁxed shared memory size with a ﬁrst-in-ﬁrst-out rule. Whenever new experiences arrive, the\\nearliest saved experiences in local memory are overridden. We aim to use the experience saved in the\\nlocal memory to simulate on-policy data. However, instead of exploiting a brute-force storage that\\nindiscriminately saves every new episode experience, we set an intuitive rule to determine whether to\\nstore an experience in local memory or not. Speciﬁcally, we only save a newly generated episode\\nfrom a local agent when (1) the local agent successfully accepts a policy from the global agent 2, and\\n(2) when its episode return is not worse than the minimum of all agents’ current performance. By\\ndoing so, we can avoid out-of-distribution data being saved in local memory to some extent, so as\\nto reduce variance and stabilize learning [12]. We then allow global agents to replay experiences\\nfrom the two memories drawn from a Bernoulli distribution, that is, sample experiences from the\\nlocal memory with a probability p, and from the global memory with a probability 1 −p. Such a\\nLocal-Global Memory Relay mechanism plays a very important role in guaranteeing global agents\\nto consistently beneﬁt from on-policy data as, if only a single global memory buffer is used, the\\nprobability of sampling a newly generated experience in it becomes lower and lower with more and\\nmore experiences saved alongside learning.\\nDistinctive Update (DU): Although global agents guide local agents for exploration, each agent still\\nmaintains its own policy updating schemes to preserve learning advantages. When an agent accepts a\\npolicy from its upper-level agent, it keeps updating using its update algorithms, e.g., policy gradient,\\nstarting from the accepted policy. This is naturally achieved by the hierarchical framework stated\\nabove as well as by the performance gap determining when to transfer.\\nTo understand CHDRL better, we provide a CHDRL instantiation, which employs a state-of-the-art\\noff-policy agent SAC, an on-policy agent PPO and EAs agent CEM, called Cooperative SAC-PPO-\\nCEM (CSPC). The pseudo code of the instantiated CSPC is presented in detail in algorithms 1 to 3.\\nGs, Lp, Lc represent global off-policy agent SAC, local on-policy agent PPO and EA agent CEM\\nrespectively. Algorithm 1 shows the general learning ﬂow of CSPC. Firstly, global agent Gs is\\n2It ensures the local experiences are close to the on-policy data of the global agent’s current policy.\\n5\\nAlgorithm 3 UPDATE\\nRequire:\\nAgent aφ, update steps tu, step t = 0, sample probability p; Global shared memory Mg, local memory Ml;\\n1: if a is G then\\n2:\\nwhile t < tu do\\n3:\\no ←Bernoulli(k, p) with k ∈{0, 1}\\n4:\\nif o = 1 then\\n5:\\nRandomly sample a batch B from Ml\\n6:\\nelse\\n7:\\nRandomly sample a batch B from Mg\\n8:\\nend if\\n9:\\nUpdate agent’s policy φs and value function ψs following [6]\\n10:\\nt ←t + 1\\n11:\\nend while\\n12: end if\\n13: if a is Lp then\\n14:\\nUpdate agent’s policy φp and value function ψp following [9].\\n15: end if\\n16: if a is Lc then\\n17:\\nUpdate agent’s new mean πµc and covariance matrix P\\nc following [19].\\n18:\\nDraw the current population Lc from N(πµc, Σc),\\n19: end if\\ntrained for speciﬁc steps Tg. This is to ensure the off-policy agent reaches a relatively good solution.\\nAfterwards, we orderly train Gs, Lp, and Lc to search the solution space for one iteration step T.\\nNote that Gs keeps learning from the experiences when other agents explore. After that, we evaluate\\nthe updated agent to get its new policy score Ss, Sp and Sc. We then transfer policies based on these\\nupdated scores following the above principle of policy transfer. Speciﬁcally, if the score of Ss is\\nbetter than those of Sp and Sc with at least f improvement, we re-initialize Lp and one individual of\\nLc with Gs’s policy. A similar transfer is done from Lp to Lc.\\nAlgorithm 1 shows what, how, and when to transfer policies, which are the three key factors in CE.\\nLines 9-13 in Algorithm 2 show how generated experiences are stored in global memory or local\\nmemory. Lines 3-8 in Algorithm 3 show how global agents replay experiences from the global and\\nlocal memories. These lines combined consist of the implementation of LGM. Lastly, lines 9, 14,\\nand 17 reﬂect DU, where each agent updates following its own update rules. The above procedure\\nproceeds iteratively until termination.\\nNote that CHDRL also accepts the same type of agents. In this case, cooperation only exists between\\nthe global agent and local agent, not across local ones. In the ablation study, we test a case where\\nthree off-policy agents are used in CHDRL. Moreover, our CHDRL is loosely coupled in the sense\\nthat it is ﬂexible enough to involve any other agents, e.g., DQN [1] and TRPO [5] etc., into it.\\n5\\nExperiments\\nWe conducted an empirical evaluation to verify the performance superiority of CSPC to other\\nbaselines, and ablation studies to show the effectiveness of each mechanism used in CHDRL.\\n5.1\\nExperiment Setup\\nAll the evaluations were done on a continuous control benchmark: Mujoco [34]. We used state-of-the-\\nart SAC, PPO and CEM to represent the off-policy agent, on-policy agent, and EA, respectively. Note\\nthat other off-policy (e.g., TD3), on-policy (e.g., TRPO) and gradient-free agents (e.g. CEM-ES), are\\napplicable to our framework. For SAC, PPO and CEM, we used the code from OpenAISpinningUp\\nfor the ﬁrst two, and code from CEM-RL for CEM 3. For hyper-parameters in these methods, we\\nfollowed the defaults speciﬁed by the authors. For CSPC, we set the gap f as 100, global agent initial\\nlearning steps Tg as 5e4, iteration time steps T as 1e4, global memory size Mg as 1e6, local memory\\nsize Ml as 2e4, and sample probability from local memory p as 0.3.\\n3OpenAISpinningUp: github.com/openai/spinningup; CEM-RL:github.com/apourchot/CEM-RL\\n6\\n(a) Hopper-v2\\n(b) Walker2d-v2\\n(c) Ant-v2\\n(d) Humanoid-v2\\n(e) Swimmer-v2\\nFigure 2: Training curves on Mujoco continuous control tasks.\\nTable 1: The max average return.\\nTask\\nCSPC\\nPPO\\nSAC\\nCEM\\nHumanoid-v2\\n5412±239\\n626±23\\n5142±133\\n616±88\\nAnt-v2\\n5337±220\\n1169±207\\n3766±2359\\n1019±33\\nWalker2d-v2\\n5317±256\\n1389±387\\n4222±290\\n1041±65\\nHopper-v2\\n3619±52\\n2923±88\\n3558 ±139\\n1057±53\\nSwimmer-v2\\n261±117\\n68±31\\n44±3\\n274±118\\nTable 2: The elite agent.\\nTask\\nHumanoid-v2\\nAnt-v2\\nWalker2d-v2\\nHopper-v2\\nSwimmer-v2\\nseed 0\\nSAC\\nSAC\\nPPO\\nCEM\\nCEM\\nseed 1\\nCEM\\nSAC\\nCEM\\nCEM\\nCEM\\nseed 2\\nPPO\\nCEM\\nCEM\\nCEM\\nCEM\\nseed 3\\nPPO\\nCEM\\nCEM\\nSAC\\nCEM\\nseed 4\\nSAC\\nPPO\\nCEM\\nCEM\\nCEM\\n5.2\\nComparative Evaluation\\nWe evaluated CSPC on ﬁve continuous control tasks from Mujoco in comparison to three baselines:\\nSAC, PPO, and CEM. We also used SAC, CEM, and PPO as our candidate agents in CSPC. We ran\\nthe training process for all the methods over one million time steps on four tasks with ﬁve different\\nseeds, and for the Swimmer-v2 task, we ran it for four million time steps. Time steps are accumulated\\ninteraction steps with the environment. For a fair comparison, we used the accumulated time steps\\nof three algorithms used in CSPC. Speciﬁcally, we summed up each agent’s time steps so that the\\ntotal time-steps stayed consistent with the other baselines. The ﬁnal performance was reported as the\\nmax average return of 5 independent trials for each seed. We reported the scores of all the methods\\ncompared against the number of time steps.\\nFigure 2 shows the comparison results for all methods on ﬁve Mujoco learning tasks. From the results,\\nwe ﬁrst observe that there is no clear winner among the existing state-of-the-art baselines SAC, PPO,\\nand CEM in terms of stability and sample efﬁciency. No one consistently outperforms the others on\\nthe ﬁve learning tasks. Speciﬁcally, it can be seen that, for four of ﬁve tasks (except for Swimmer\\ntask), SAC yields better results than PPO and CEM, which veriﬁes its sample efﬁciency for a long\\nrun. However, we can also observe a signiﬁcant variance of SAC, which indicates its high instability,\\nespecially in Ant task. In contrast, PPO and CEM have a lower variance but achieve unsatisfactory\\naverage returns. A special case is Swimmer task where both SAC and PPO fail to learn a good policy\\nbut CEM succeeds. Figure 2 also demonstrates that our proposed CSPC performs consistently better\\nor with comparable results to the best baseline methods on all tasks. This veriﬁes the capability\\nof CSPC to improve the performance of each individual agent by utilizing the cooperation among\\nthem. On Swimmer task where both gradient-based methods fail, CSPC still achieves a comparable\\nresult with CEM. This is because CSPC does not beneﬁt from SAC and PPO, and only maintains the\\ncapacity of CEM. Table 1 shows the maximum average return for each method.\\nOne may wonder about the possible computation cost of CPSC. In our experiments, it mainly comes\\nfrom the global agent, as it keeps learning for other agents’ experiences in the background. The local\\nagents run much faster than global agents, especially the CEM agent, as it is gradient-free. The total\\nrunning time of CSPC is only slightly longer than the SAC agent.\\n7\\n(a) Walker2d-v2\\n(b) Walker2d-v2\\n(c) Walker2d-v2\\n(d) Swimmer-v2\\n(e) Swimmer-v2\\n(f) Swimmer-v2\\nFigure 3: Ablation study on two tasks: Walker2d and Swimmer.\\n5.3\\nLocal Agent vs Global Agent\\nThe main motivation of this study is to ﬁgure out whether local agents really help in ﬁnding the best\\nﬁnal policy in different random settings. To do so, we show the elite agent, that is, the agent yielding\\nthe best performance among heterogenous agents after training has terminated, in different random\\nseeds. The results are shown in Table 2. It can be seen that CSPC could obtain different elite agents\\non the same task under different random seeds. Such an observation indicates that local search agents\\ndo help to ﬁnd a better policy around the global guided agent. Surprisingly, the EA-based CEM\\nagent performs better than other local agent (PPO) in most cases. However, on the complex task,\\nHumanoid-v2, the gradient-based agents perform much better than CEM.\\n5.4\\nAblation Studies\\nIn this section, we conducted ablation studies to understand the contributions of each key component\\nof CSPC. To do this, we built three variants of CSPC: CSPC without cooperative exploration (CE),\\ni.e., CSPC-CE, CSPC without local memory(LM), i.e., CSPC-LM, and CSPC without global memory\\n(GM), i.e., CSPC-GM. Speciﬁcally, in CSPC-CE, we stopped the policy transfer and let each agent\\nexplore and exploit by itself. In CSPC-LM, the off-policy agent SAC replays from all experiences\\nuniformly. In CSPC-GM, the off-policy agent SAC only learns from its own experiences. We further\\nanalyzed the inﬂuence of each individual agent to CSPC. To do so, we developed a CSPC without\\nPPO, called CSPC-PPO, a CSPC without CEM, called CSPC-CEM, and a CSPC without SAC, called\\nCSPC-SAC. As CHDRL also allows the same types of agents, to verify that heterogeneous agents\\nindeed matters, a variant of CHDRL consisting of only one type of agent was proposed. In this case,\\nwe introduced two variants: three SAC agents with CE and LGM, and three SAC agents without\\nthem. We called the former C3SAC and the latter 3SAC. For 3SAC, the three agents only shared\\nglobal memory and no policy transfer existed. We evaluated all the variants on Walker2d-v2 and\\nSwimmer-v2. The results are shown in Figure 3.\\nAs shown in Figure 3, for task Walker2d-v2, CSPC achieved the best among all the ablation variants\\nin terms of ﬁnal average performance. From Figure 3(a), it is easy to deduce that LGM and CE\\nindeed matter in CSPC, as without these two elements, the ﬁnal performance drops quickly. From\\nFigure 3(b), we can see that the results of CSPC-PPO and CSPC-CEM are satisfactory and only\\nslightly worse than that of CSPC, while the result of CSPC-SAC dramatically decreases. This implies\\nthat the global agent has a more signiﬁcant impact on the ﬁnal performance than local agents. This\\nis reasonable as the global agent determines the starting position of CSPC, and highly affects the\\nfollowing search efﬁciency. Note that CSPC-PPO and CSPC-CEM are CSPC without one speciﬁc\\nlocal agent, but still follow CHDRL’s core mechanism: CE and LGM. From the fact that their\\nperformances are much higher than CSPC-CE and CSPC-LM/GM, we again verify the signiﬁcance\\n8\\nof LGM and CE. From Figure 3(c), we can see that C3SAC performs better than 3SAC and SAC.\\nEven though the three agents are with the same type, local agents still provide a diverse local search\\nas they explore in different random settings. However, our CSPC performs much better than C3SAC,\\nwhile 3SAC performs only slightly better than SAC. With this, we deduce that CHDRL still improves\\nthe performance when using the same type agents, but using heterogeneous agents would further\\nboost the performance.\\nFor the Swimmer-v2 task, the results are different as SAC and PPO agents typically fail on this\\ntask. In other words, the global agent is incapable of ﬁnding a relatively good position, and only\\nthe CEM agent works. The most likely explanation is that in Swimmer-v2, existing DRL methods\\nprovide deceptive gradient information that is detrimental to convergence towards efﬁcient policy\\nparameters [16]. Hence, LM/GM/CL cannot enhance the ﬁnal performance, which is shown by\\nCSPC-LM,CSPC-GM and CSPC-CL in Figure 3 (d). In such a case, the learning curves of the\\nthree methods mostly overlap. On the other hand, CSPC-PPO and CSPC-SAC gain a better ﬁnal\\nperformance than CSPC, which is also reasonable as the CEM agent has more iterations leading to a\\nbetter ﬁnal performance, as shown in Figure 3(e). For the same reason, C3SAC and 3SAC both fail.\\n6\\nConclusion\\nIn this paper, we present CHDRL, a framework that incorporates the beneﬁts of off-policy agents,\\npolicy gradient on-policy agents and EAs agents. The proposed CHDRL is based on three key\\nmechanisms, i.e., cooperation exploration, local-global memory and distinctive update. We also\\nprovide a practical algorithm CSPC by using SAC, PPO, and CEM. Experiments in a range of\\ncontinuous control tasks show that CSPC achieves a better or comparable performance compared\\nwith baselines. We also note that CHDRL introduces some new hyper-parameters which may have\\na crucial impact on performance, however, we do not tune that too much. Moreover, we should\\ncarefully select the agents, as the ﬁnal performance highly depends on the agents used, particularly\\nthe global one.\\nBroader Impact\\nThe DRL agent that learns from an incompletely known environment runs the risk of making wrong\\ndecisions. This could lead to catastrophic consequences in practice, such as automated driving,\\nthe stock market, or medical robots. One approach to alleviate this risk is to combine with other\\ntechniques or involve human beings’ supervision. In terms of beneﬁts, DRL can be deployed in a\\nsafe environment where a wrong decision will not lead to a signiﬁcant loss, e.g., the recommendation\\nsystem. Moreover, in some environments that we can simulate well, it would be very promising to\\ndevelop an intelligent robot to work in such an environment.\\nAcknowledgments and Disclosure of Funding\\nThis research is partially funded by the Australian Government through the Australian Research\\nCouncil (ARC) under grant LP180100654.\\nReferences\\n[1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\\n[2] Richard S Sutton. Temporal credit assignment in reinforcement learning. 1985.\\n[3] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David\\nMeger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on\\nArtiﬁcial Intelligence, 2018.\\n[4] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2011.\\n[5] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\\npolicy optimization. In International conference on machine learning, pages 1889–1897, 2015.\\n9\\n[6] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n[7] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods:\\nGlobal optimality and rates of convergence. September 2019.\\n[8] Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy\\noptimization attains globally optimal policy. In Advances in Neural Information Processing\\nSystems, pages 10564–10575, 2019.\\n[9] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-\\npolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint\\narXiv:1801.01290, 2018.\\n[10] Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba\\nSzepesvári. Convergent temporal-difference learning with arbitrary smooth function approxi-\\nmation. In Advances in Neural Information Processing Systems, pages 1204–1212, 2009.\\n[11] Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In\\nMachine Learning Proceedings 1995, pages 30–37. Elsevier, 1995.\\n[12] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy\\nq-learning via bootstrapping error reduction. In Advances in Neural Information Processing\\nSystems, pages 11761–11771, 2019.\\n[13] S Fujimoto, D Meger, and D Precup. Off-policy deep reinforcement learning without exploration.\\narXiv preprint arXiv:1812.02900, 2018.\\n[14] David B Fogel and Evolutionary Computation. Toward a new philosophy of machine intelligence.\\nIEEE Evolutionary Computation, 1995.\\n[15] William M Spears, Kenneth A De Jong, Thomas Bäck, David B Fogel, and Hugo De Garis. An\\noverview of evolutionary computation. In European Conference on Machine Learning, pages\\n442–459. Springer, 1993.\\n[16] Aloïs Pourchot and Olivier Sigaud. Cem-rl: Combining evolutionary and gradient-based\\nmethods for policy search. In International Conference on Learning Representations, 2018.\\n[17] Shauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement\\nlearning. In Advances in Neural Information Processing Systems, pages 1188–1200, 2018.\\n[18] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies\\nas a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.\\n[19] Freek Stulp and Olivier Sigaud. Path integral policy improvement with covariance matrix\\nadaptation. arXiv preprint arXiv:1206.4621, 2012.\\n[20] Pedro Larrañaga and Jose A Lozano. Estimation of distribution algorithms: A new tool for\\nevolutionary computation, volume 2. Springer Science & Business Media, 2001.\\n[21] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and\\nteaching. Machine learning, 8(3-4):293–321, 1992.\\n[22] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.\\nIn International Conference on Learning Representations, 2016.\\n[23] Zhizheng Zhang, Jiale Chen, Zhibo Chen, and Weiping Li. Asynchronous episodic deep deter-\\nministic policy gradient: Towards continuous control in computationally complex environments.\\narXiv preprint arXiv:1903.00827, 2019.\\n[24] Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine. Divide-and-\\nconquer reinforcement learning. arXiv preprint arXiv:1711.09874, 2017.\\n[25] Whiyoung Jung, Giseung Park, and Youngchul Sung. Population-guided parallel policy search\\nfor reinforcement learning. International Conference on Learning Representations, 2020.\\n[26] Shauharda Khadka, Somdeb Majumdar, Tarek Nassar, Zach Dwiel, Evren Tumer, Santiago\\nMiret, Yinyin Liu, and Kagan Tumer. Collaborative evolutionary reinforcement learning. In\\nInternational Conference on Machine Learning, 2019.\\n[27] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-\\ncrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep\\nreinforcement learning. In International conference on machine learning, pages 1928–1937,\\n2016.\\n10\\n[28] Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, and Jan Kautz. Reinforce-\\nment learning through asynchronous advantage actor-critic on a gpu. International Conference\\non Learning Representations, 2017.\\n[29] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam\\nDoron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA:\\nScalable distributed deep-RL with importance weighted actor-learner architectures. In Jennifer\\nDy and Andreas Krause, editors, Proceedings of the 35th International Conference on Ma-\\nchine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1407–1416,\\nStockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR.\\n[30] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Has-\\nselt, and David Silver. Distributed prioritized experience replay. In International Conference on\\nLearning Representations, 2018.\\n[31] Han Zheng, Jing Jiang, Pengfei Wei, Guodong Long, and Chengqi Zhang. Competitive and\\ncooperative heterogeneous deep reinforcement learning. In Proceedings of the 19th International\\nConference on Autonomous Agents and MultiAgent Systems, AAMAS ’20, page 1656–1664,\\nRichland, SC, 2020.\\n[32] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on\\nknowledge and data engineering, 22(10):1345–1359, 2009.\\n[33] Pengfei Wei, Yiping Ke, and Chi Keong Goh. Deep nonlinear feature coding for unsupervised\\ndomain adaptation. In IJCAI, pages 2189–2195, 2016.\\n[34] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\\ncontrol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages\\n5026–5033. IEEE, 2012.\\n11\\n'),\n",
              " Document(metadata={'Published': '2023-08-21', 'Title': \"Improving of Robotic Virtual Agent's errors that are accepted by reaction and human's preference\", 'Authors': 'Takahiro Tsumura, Seiji Yamada', 'Summary': \"One way to improve the relationship between humans and anthropomorphic agents\\nis to have humans empathize with the agents. In this study, we focused on a\\ntask between an agent and a human in which the agent makes a mistake. To\\ninvestigate significant factors for designing a robotic agent that can promote\\nhumans empathy, we experimentally examined the hypothesis that agent reaction\\nand human's preference affect human empathy and acceptance of the agent's\\nmistakes. The experiment consisted of a four-condition, three-factor mixed\\ndesign with agent reaction, selected agent's body color for human's preference,\\nand pre- and post-task as factors. The results showed that agent reaction and\\nhuman's preference did not affect empathy toward the agent but did allow the\\nagent to make mistakes. It was also shown that empathy for the agent decreased\\nwhen the agent made a mistake on the task. The results of this study provide a\\nway to control impressions of the robotic virtual agent's behaviors, which are\\nincreasingly used in society.\", 'entry_id': 'http://arxiv.org/abs/2304.00247v2', 'published_first_time': '2023-04-01', 'comment': '13 pages, 4 figures, 5 tables, submitted ICSR2023. arXiv admin note:\\n  text overlap with arXiv:2206.06128', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.HC', 'categories': ['cs.HC'], 'links': ['http://arxiv.org/abs/2304.00247v2', 'http://arxiv.org/pdf/2304.00247v2']}, page_content='Improving of Robotic Virtual Agent’s errors that\\nare accepted by reaction and human’s preference\\nTakahiro Tsumura1,2[0000−0002−3145−3120] and Seiji\\nYamada2,1[0000−0002−5907−7382]\\n1 Department of Informatics, The Graduate University for Advanced Studies,\\nSOKENDAI, Tokyo, Japan\\n2 Digital Content and Media Sciences Research Division, National Institute of\\nInformatics, Tokyo, Japan\\nAbstract. One way to improve the relationship between humans and\\nanthropomorphic agents is to have humans empathize with the agents.\\nIn this study, we focused on a task between an agent and a human\\nin which the agent makes a mistake. To investigate significant factors\\nfor designing a robotic agent that can promote humans empathy, we\\nexperimentally examined the hypothesis that agent reaction and human’s\\npreference affect human empathy and acceptance of the agent’s mistakes.\\nThe experiment consisted of a four-condition, three-factor mixed design\\nwith agent reaction, selected agent’s body color for human’s preference,\\nand pre- and post-task as factors. The results showed that agent reaction\\nand human’s preference did not affect empathy toward the agent but did\\nallow the agent to make mistakes. It was also shown that empathy for\\nthe agent decreased when the agent made a mistake on the task. The\\nresults of this study provide a way to control impressions of the robotic\\nvirtual agent’s behaviors, which are increasingly used in society.\\nKeywords: human-agent interaction · empathy agent · human’s prefer-\\nence\\n1\\nIntroduction\\nHumans use a variety of tools in their daily lives. They become attached to\\nthese tools and sometimes treat them like humans. The Media Equation claims\\nthat humans treat artifacts like humans [19]. It has been shown that humans\\nhave the same feelings toward artifacts as they do toward other humans. In\\nfact, there are examples of people empathizing with artifacts in the same way\\nthat humans empathize with humans. Typical examples include cleaning robots,\\npet-type robots, characters in competitive video games, and anthropomorphic\\nagents that provide services such as online shopping and help desks. However,\\nwhen humans and agents work together, such as at a help desk or when cleaning,\\nthere can be problems when humans view these agents as tools. There are also\\ncertain types of humans who cannot accept agents [10,11,9]. Currently, such\\nagents are already being used in human society and coexist with humans.\\narXiv:2304.00247v2  [cs.HC]  21 Aug 2023\\n2\\nT. Tsumura et al.\\nAgents used in society often perform tasks with humans. At times, an agent\\nmay get the task wrong. When an agent makes a mistake on a task, many humans\\nlower their expectations and trust in the agent. However, we often develop agents\\nso that they do not make mistakes, but rarely do we take an approach that\\npreserves the human’s impression of the agent when it actually makes a mistake.\\nOne way to do so is to have the human empathize with the agent. When agents\\nare used as tools, they may not need empathy, but when they are used in place of\\nhumans, being empathized with by humans can help build a smooth relationship.\\nHumans and anthropomorphic agents already interact in a variety of tasks.\\nFor a human to develop a good relationship with an agent, empathy toward the\\nagent is necessary. Empathy makes it easier for humans to take positive action\\ntoward an agent and to accept it [21,22,23]. This can also be effective when an\\nagent makes a mistake on a task.\\nAlthough various factors have been studied that cause empathy, including\\nverbal and nonverbal information, situations, and relationships, this study fo-\\ncuses on situations in which the robotic virtual agent (RVA) gets the task wrong\\nand experimentally examines how the agent’s reaction and the agent’s human’s\\npreference affect empathy. The empathy investigated in this study is human em-\\npathy toward the agent, and we investigated changes in impressions of the agent\\nused in the experiment.\\n2\\nRelated work\\nIn the field of psychology, empathy has been the focus of much attention and\\nresearch. Omdahl [14] classified empathy into three main categories: (1) affective\\nempathy, which is an emotional response to another person’s emotional state,\\n(2) cognitive empathy, which is a cognitive understanding of another person’s\\nemotional state, and (3) empathy that includes both of the above. Preston and\\nDe Waal [18] proposed that at the heart of empathic responses is a mechanism\\nthat allows the observer access to the subjective emotional state of the subject.\\nThe Perception-Action Model (PAM) was defined by them to unify the differ-\\nences in empathy. They defined empathy as a total of three types: (a) sharing or\\nbeing affected by the emotional states of others, (b) evaluating the reasons for\\nemotional states, and (c) the ability to identify and incorporate the perspectives\\nof others.\\nVarious questionnaires are used as measures of empathy, but we used the\\nInterpersonal Reactivity Index (IRI). IRI, also used in the field of psychology,\\nis used to investigate the characteristics of empathy [4]. There is another ques-\\ntionnaire, the Empathy Quotient (EQ) [2], but we did not use it in our study\\nbecause we wanted to investigate which categories of empathy were affected after\\nexperiencing the task.\\nIn the fields of human-agent interaction (HAI) and human-robot interaction\\n(HRI), empathy between humans and agents or robots is studied. The following\\nstudies have been conducted in various areas of HRI. Leite et al. [7] conducted\\na long-term study in elementary schools to present and evaluate an empathy\\nRVA errors are accepted by reaction and human’s preference\\n3\\nmodel for a social robot that interacts with children over a long period of time.\\nThey measured children’s perceptions of social presence, engagement, and social\\nsupport. Mathur et al. [8] present a first approach to modeling user empathy\\nelicited during interaction with a robot agent. They collected a new dataset from\\na novel interaction context in which participants listen to a robotic storyteller.\\nJohanson et al. [6] examined whether the use of verbal empathic statements and\\nhead nodding by a robot during video-recorded interactions between a healthcare\\nrobot and a patient could improve participants’ trust and satisfaction.\\nIn addition, the following studies have been conducted in the field of HAI.\\nOkanda et al. [13] focused on appearance and investigated Japanese adults’ be-\\nliefs about friendship and morality toward robots. They examined whether the\\nappearances of robots (i.e., humanoid, dog-like, oval-shaped) differed in rela-\\ntion to their animistic tendencies and empathy. Samrose et al. [20] designed a\\nprotocol to elicit user boredom to investigate whether empathic conversational\\nagents can help reduce boredom. With the help of two conversational agents,\\nan empathic agent and a non-empathic agent, in a Wizard-of-Oz setting, they\\nattempted to reduce the user’s boredom. Al Farisi et al. [1] believe that in or-\\nder for chatbots to have human-like cues, it is necessary to apply the concepts\\nof human-computer interaction (HCI) to chatbots and compare the empathy of\\ntwo chatbots, one with anthropomorphic design cues (ADC), and one without.\\nTsumura and Yamada [21] focused on tasks between agents and humans, experi-\\nmentally examining the hypothesis that task difficulty and task content promote\\nhuman empathy. We also considered the design of empathy factors from previous\\nstudies of anthropomorphic agents using empathy. Tsumura and Yamada [22]\\nfocused on self-disclosure from agents to humans in order to enhance human\\nempathy toward anthropomorphic agents, and they experimentally investigated\\nthe potential for self-disclosure by agents to promote human empathy. Tsumura\\nand Yamada [23] also focused on tasks in which humans and agents engage in a\\nvariety of interactions, and they investigated the properties of agents that have\\na significant impact on human empathy toward them.\\nPaiva defined the relationship between human beings and empathic agents,\\nreferred to as empathy agents, as designed in previous HAI and HRI research.\\nAs a definition of empathy between an anthropomorphic agent or robot and a\\nhuman, Paiva represented empathy agents in two different ways and illustrated\\nthem [16,15,17]: A) targets to be empathized with by humans and B) observers\\nwho empathize with humans. In this study, we use the empathic target agent to\\npromote human empathy.\\n3\\nExperimental methods\\n3.1\\nExperimental goals and design\\nThe purpose of this study is to investigate whether human empathy toward an\\nagent is affected by the agent’s reaction and human’s preference during interac-\\ntion with an robotic virtual agent (RVA). It will then investigate whether agents\\ncan be forgiven when they make mistakes. We believe that this research will\\n4\\nT. Tsumura et al.\\nfacilitate the use of agents in human society by influencing human empathy. In\\naddition, knowing the factors that allow agents to make mistakes will be use-\\nful for future use of agents in society. For these purposes, we formulated two\\nhypotheses.\\nH1: Agent’s reaction and human’s preference affect human empathy toward agents.\\nH2: Agent’s reaction and human’s preference influence when humans accept\\nagents’ mistakes.\\nWe arrived at this hypothesis because previous studies have shown that agent\\nreactions (facial expressions and gestures) affect human empathy. In addition,\\nthe intention of the agent’s human’s preference was to investigate whether the\\nresults of human selections affect empathy toward an agent by adding an element\\nthat allows humans to interact with the agent.\\nSimilarly, this study focuses on whether agents’ mistakes are acceptable.\\nThis study investigates an agent’s relationship with a human in situations where\\nthe agent is wrong. If the agent’s reaction and human’s preference affect how\\na human accepts the agent’s mistakes, then there is no need to incorporate\\nempathy toward the agent in order to maintain the human’s impression of the\\nagent.\\nTo test these hypotheses, an experiment was conducted with a three-factor\\nmixed design with three factors: agent reaction, human’s preference, and pre-\\nand post-task. The levels between participants were 2 (available, not available)\\nfor agent reaction and 2 (available, not available) for human’s preference. The\\nwithin-participants level was 2 pre- and post-task. Participants participated in\\nonly one of the four different content conditions. The dependent variable was\\nthe questionnaire that participants responded to (empathy, tolerance for error,\\nother).\\n3.2\\nExperimental details\\nThe experiment was conducted in an online environment. The environment used\\nis already a common method of experimentation[5,3,12]. As mentioned earlier,\\nthe goal of this study is to promote human empathy toward RVA. A scheduling\\nagent was also used in this study to measure the acceptance of the agent’s\\nmistakes. For this reason, we believed that the same effect as being face to face\\ncould be achieved even in an online environment.\\nBefore performing the task, a questionnaire was administered to measure\\nempathy toward RVA. At the same time, another questionnaire was administered\\nto determine whether participants could accept the agent’s mistakes. At this\\ntime, participants were not allowed to see the agent’s reactions or to select the\\ncolor of the agent. This questionnaire was administered before the task in order\\nto see the effect of participants’ empathy toward the agent and the change in\\nthe acceptance of the agent’s mistakes.\\nParticipants selected an agent by color from among multiple differently col-\\nored agents before beginning the scheduling task. In the no human’s preference\\nRVA errors are accepted by reaction and human’s preference\\n5\\nFig. 1. Flowchart of the schedule\\ncondition, participants were told that the agent displayed would manage a sched-\\nule. The schedule consisted of 10 items: the participant’s weekly schedule, waking\\ntime, sleeping time, and number of outings per week. Fig. 1 is a flowchart of the\\nschedule entry order and up to the confirmation screen. During the scheduling\\ntask, the agent exhibited several reactions to the input information. The schedul-\\ning agent was designed to remember the participant’s schedule but to make sure\\nthat the agent made a mistake when participant checked the schedule for the\\nlast time. There were three areas where mistakes were made: (1) the waking and\\nsleeping times were reversed, the (2) Monday and Wednesday schedules were re-\\nversed, and (3) the schedules for Thursday and Saturday were reversed. Waking\\nand sleeping times and the number of outings per week were selected from a list\\nof options, and schedules from Monday to Sunday were answered in the form of\\nfree-text responses.\\nThis was done to investigate how an agent’s mistakes affect human empathy\\nand acceptance. To screen out unfair participants, participants reported whether\\nthe schedule was correct or incorrect at the last confirmation of the schedule.\\nOnly those participants who reported that the schedule was wrong were sub-\\nsequently administered the same questionnaire about RVA as before the task.\\nTwo additional questions were also asked. Finally, they were asked to write their\\nimpressions of the experiment in free text.\\n3.3\\nExperimental environment and Participants\\nParticipants were recruited for the experiment using a Yahoo! crowdsourcing\\ncompany. They were paid 55 yen after completing all tasks as a reward for\\nparticipating. A website was created for the experiment, which was limited to\\nusing a PC.\\n6\\nT. Tsumura et al.\\nTable 1. Number of participants in each condition\\nReaction\\nYes No\\nYes 50\\n50\\nhuman’s preference No 49\\n48\\nFig. 2. Types of agent reactions\\nThere were a total of 197 participants. The average age was 48.82 years\\n(standard deviation: 11.08), with a minimum of 19 years and a maximum of\\n77 years. The gender breakdown was 144 males and 53 females. The number of\\nparticipants in each condition is shown in Table 1. We then applied Cronbach’s\\nα coefficient to determine the reliability of the questionnaire responses, which\\nwas found to be between 0.4040 and 0.8190 in all conditions. Some participant\\ngroups had lower Cronbach’s α values, but we also found several conditions with\\nvalues between 0.7 and 0.8. Therefore, we used the questionnaire without any\\nmodifications.\\n3.4\\nAgent’s reactions\\nIn this study, two levels of agent reactions were prepared. In the one with the\\nagent’s reaction, RVA responded with gestures and comments when the partic-\\nipant’s schedule was sent. For the one without the agent’s reaction, it did not\\nrespond to the participant’s schedule and stayed upright. The three types of\\nreactions actually seen by the participants are shown in Fig. 2. The three types\\nwere displayed three times equally and in the order in which they were displayed\\nto the participants. Because the reactions were based on the content of the par-\\nticipant’s schedule, RVA did not react to anything when the schedule was first\\nentered.\\nThere is a study by Tsumura and Yamada [23] as an example of how agent\\nrepresentations affected human empathy toward agents, but unlike reactions,\\nwhich are very brief representations, they did not focus on detailed representa-\\ntions for a single action, as in this experiment.\\nRVA errors are accepted by reaction and human’s preference\\n7\\nFig. 3. Types of agent’s color\\n3.5\\nHuman’s preference\\nThere were two levels for a human’s preference factor. In this study, participants\\nwere asked to choose the color of the agent’s appearance to investigate people’s\\npreferences. If a human’s preference was had, the participants were required to\\nchoose one of three types of RVA: red, blue, or green. There was no difference in\\nthe agent’s personality or behavior based on human’s preference. If no human’s\\npreference was had, the gray agent would manage the schedule. The color of\\neach agent is shown in Fig. 3.\\nIn this study, we did not consider bias in human’s preference. The human’s\\npreference factor was chosen as a factor that allowed participants to interact with\\nRVA. The purpose was to investigate whether the participants’ impressions of\\nRVA changed depending on the selection. Therefore, differences in participants’\\nfeelings towards particular colors and gender bias were ignored in this study.\\n3.6\\nQuestionnaire\\nParticipants completed a questionnaire before and after the task. In this study,\\nhuman empathy for the agent was evaluated based on changes in human em-\\npathy characteristics. The questionnaire was a 12-item questionnaire modified\\nfrom the Interpersonal Reactivity Index (IRI), which is used to investigate the\\ncharacteristics of empathy, to suit the present experiment [4]. The modified ques-\\ntionnaire has already been used in several previous studies by Tsumura and Ya-\\nmada [21,22,23]. The two questionnaires before and after were the same. Both\\nwere based on the IRI and were surveyed on a 5-point Likert scale (1: not appli-\\ncable, 5: applicable). The questionnaire used is shown in Table 2. Q4, Q9, and\\nQ10 are inverted items, so the scores were reversed when analyzing them.\\nThree questions other than those related to empathy were prepared: QA,\\nQB, and QC. QA was surveyed before and after the task, while QB and QC\\nwere surveyed only after the task. QA was investigated before and after the\\n8\\nT. Tsumura et al.\\nTable 2. Summary of questionnaire used in this experiment\\nAffective empathy\\nPersonal distress\\nQ1: If an emergency happens to the character, you would be anxious and restless.\\nQ2: If the character is emotionally disturbed, you would not know what to do.\\nQ3: If you see the character in need of immediate help, you would be confused and would not know what to do.\\nEmpathic concern\\nQ4: If you see the character in trouble, you would not feel sorry for that character.\\nQ5: If you see the character being taken advantage of by others, you would feel like you want to protect that character.\\nQ6: The character’s story and the events that have taken place move you strongly.\\nCognitive empathy\\nPerspective taking\\nQ7: You look at both the character’s position and the human position.\\nQ8: If you were trying to get to know the character better, you would imagine how that character sees things.\\nQ9: When you think you’re right, you don’t listen to what the character has to say.\\nFantasy scale\\nQ10: You are objective without being drawn into the character’s story or the events taken place.\\nQ11: You imagine how you would feel if the events that happened to the character happened to you.\\nQ12: You get deep into the feelings of the character.\\nOther questions than empathy\\nQA: If the scheduling agent makes a mistake, can you forgive the mistake?\\nQB: Did the scheduling agent express emotions?\\nQC: Would you like to use a scheduling agent in the future?\\ntask to compare the difference between the participants’ assumed tolerance of\\nagent error and the actual agent error after it occurred. QB was prepared to\\ninvestigate whether the agent’s very brief reactions appeared emotional. QC was\\nan item to investigate the impact of differences in factors on the participants’\\nevaluations of the agent’s future use. These three questions were also surveyed\\non a 5-point Likert scale (1: not applicable, 5: applicable). The questionnaire is\\nshown in Table 2.\\n3.7\\nAnalysis method\\nThe analysis was a three-factor analysis of variance (ANOVA). The between-\\nparticipant factors were the two levels of agent’s reaction and two levels of\\nagent’s human’s preference. The within-participants factor consisted of two lev-\\nels of empathy values before and after the task. On the basis of the results of\\nthe participants’ questionnaires, we investigated how the agent’s reaction and\\nhuman’s preference influenced the promotion of empathy as factors that elicit\\nhuman empathy. The numerical values of empathy aggregated before and after\\nthe task were used as the dependent variable. Three of our own questions were\\nalso used as dependent variables. R (R ver. 4.1.0) was used for the ANOVA.\\n4\\nResults\\nAll 12 questionnaire items were analyzed together. Table 3 shows the statistical\\nresults for each. Table 4 shows the results of each ANOVA.\\nTo begin, as can be seen from Table 4, when the participants’ empathy for the\\nagent was examined, there were no significant differences other than the main\\nRVA errors are accepted by reaction and human’s preference\\n9\\nTable 3. Results of participants’ statistical information\\nCategory\\nConditions\\nMean S.D.\\nCategory\\nConditions\\nMean S.D.\\nreaction-preference\\n36.94 5.705\\nreaction-preference\\n2.660 0.8947\\nreaction-no preference\\n38.78 5.363\\nreaction-no preference\\n2.837 0.7457\\nEmpathy pre\\nno reaction-preference\\n36.90 6.370\\nAgent’s\\npre\\nno reaction-preference\\n2.880 0.8722\\nno reaction-no preference 38.67 6.096 acceptance\\nno reaction-no preference 2.563 0.9655\\nreaction-preference\\n33.98 7.347\\nreaction-preference\\n2.560 0.9510\\n(Q1-Q12)\\nreaction-no preference\\n34.94 6.710\\n(QA)\\nreaction-no preference\\n2.755 0.7781\\npost\\nno reaction-preference\\n34.72 6.716\\npost\\nno reaction-preference\\n2.600 1.107\\nno reaction-no preference 35.71 6.694\\nno reaction-no preference 2.271 0.9618\\nreaction-preference\\n2.840 0.9116\\nreaction-preference\\n2.140 0.9260\\nExpressed\\nreaction-no preference\\n2.674 1.068\\nContinued\\nreaction-no preference\\n2.286 0.9129\\nemotions post\\nno reaction-preference\\n2.000 0.8571\\nuse\\npost\\nno reaction-preference\\n2.080 0.9223\\n(QB)\\nno reaction-no preference 1.896 0.7217\\n(QC)\\nno reaction-no preference 1.979 0.8627\\nTable 4. Analysis results of ANOVA\\nFactor\\nF\\np\\nη2\\np\\nReaction\\n0.1567 0.6926 ns\\n0.0008\\nhuman’s preference\\n2.606\\n0.1081 ns\\n0.0133\\nEmpathy Before/after task\\n94.11\\n0.0000 *** 0.3278\\n(Q1-12)\\nReaction × human’s preference\\n0.0001 0.9909 ns\\n0.0000\\nReaction × Before/after task\\n1.817\\n0.1793 ns\\n0.0093\\nhuman’s preference × Before/after task\\n1.810\\n0.1801 ns\\n0.0093\\nReaction × human’s preference × Before/after task 0.0064 0.9363 ns\\n0.0000\\nReaction\\n1.132\\n0.2887 ns\\n0.0058\\nhuman’s preference\\n0.3440 0.5582 ns\\n0.0018\\nAgent’s\\nBefore/after task\\n10.68\\n0.0013 **\\n0.0525\\nacceptance Reaction × human’s preference\\n4.725\\n0.0309 *\\n0.0239\\n(QA)\\nReaction × Before/after task\\n2.864\\n0.0922 ns\\n0.0146\\nhuman’s preference × Before/after task\\n0.0008 0.9768 ns\\n0.0000\\nReaction × human’s preference × Before/after task 0.0170 0.8964 ns\\n0.0001\\nExpressed Reaction\\n39.86\\n0.0000 *** 0.1712\\nemotions human’s preference\\n1.116\\n0.2921 ns\\n0.0057\\n(QB)\\nReaction × human’s preference\\n0.0592 0.8080 ns\\n0.0003\\nContinued Reaction\\n2.012\\n0.1577 ns\\n0.0103\\nuse\\nhuman’s preference\\n0.0302 0.8623 ns\\n0.0002\\n(QC)\\nReaction × human’s preference\\n0.9100 0.3413 ns\\n0.0047\\np: *p<0.05 **p<0.01 ***p<0.001\\nTable 5. Analysis results of simple main effect\\nFactor\\nF\\np\\nη2\\np\\nReaction with preference\\n0.5613 0.4555 ns 0.0057\\nAgent’s\\nReaction with no preference\\n5.851\\n0.0175 * 0.0580\\nacceptance human’s preference with reaction\\n1.577\\n0.2122 ns 0.0160\\n(QA)\\nhuman’s preference with no reaction 3.160\\n0.0786 + 0.0319\\np: +p<0.1 *p<0.05 **p<0.01\\n10\\nT. Tsumura et al.\\nFig. 4. Result of each main effect or simple main effect\\npre- and post-task effects. Comparing the pre- and post-task empathy values in\\nTable 3 for the ability to empathize with the agent, empathy decreased after the\\ntask (all pre-task: mean = 37.81, S.D. = 5.921; all post-task: mean = 34.83, S.D.\\n= 6.850).\\nThe results for agent tolerance showed an interaction between the agent’s\\nreaction factor and human’s preference factor. Therefore, we analyzed the sim-\\nple main effect in Table 5 and found that the agent’s reaction more likely lead\\nto tolerance to the agent’s error when the participant did not make a human’s\\npreference. We also found that, although a significant trend, the human’s prefer-\\nence made its mistake more acceptable when there was no agent reaction. These\\nresults are shown in Fig. 4(a) and (b).\\nThe results for the agent’s emotional expression showed a main effect for\\nthe agent’s reaction factor. However, no main effect was found for the agent’s\\nhuman’s preference factor. This indicated that the agent’s reactions appeared to\\nbe emotionally charged. These results are shown in Fig. 4(c). Finally, there were\\nno significant differences in the continued use of the agents in all conditions.\\n5\\nDiscussion\\n5.1\\nSupporting hypotheses\\nThis experiment was designed to investigate the conditions necessary for humans\\nto empathize with anthropomorphic agents. In particular, by investigating when\\nan agent makes a mistake on a task, the goal was to identify factors that influence\\nempathy between an agent who makes a mistake on a task and a human. To this\\nend, two hypotheses were formulated, and the data obtained from the experiment\\nwere analyzed.\\nThe results supported one hypothesis, but not the other. In H1, we thought\\nthat the agent’s reaction and human’s preference would affect the participants’\\nempathy toward the agent, but this one was not supported. In the present ex-\\nperiment, there was a decrease in empathy after the task in all conditions. This\\nRVA errors are accepted by reaction and human’s preference\\n11\\nwas also the case in Tsumura and Yamada [21]. The reason for the decrease in\\nempathy may be that the all agents made mistakes, as there was no significant\\ndifference in each factor. Therefore, as a future study, we will compare the results\\nwith those obtained when the agents did not make mistakes.\\nIn H2, we thought that the agent’s reaction and human’s preference would\\naffect the tolerance toward the mistakes the agent made, which was supported\\nhere. In each case, when the agent’s reaction was absent, the agent’s mistake\\nwas accepted when human’s preference was present, and when it was absent,\\nthe agent’s mistake was accepted when the agent’s reaction was present. On\\nthe other hand, when both agent reaction and human’s preference were present,\\nthere was no effect. A possible reason for the lack of acceptance of agent error\\nwhen both conditions were included is discouragement toward the agent. There-\\nfore, as a future study, we will compare the results with those obtained when\\nthe agent does not make mistakes.\\nWe also investigated the agent’s emotional expressions and found that the\\nagent’s reactions appeared to be emotional expressions. However, regardless of\\nthe agent’s reaction, empathy toward the agent was reduced, indicating that even\\nwhen the agent acts emotionally, it is unlikely to affect empathy in situations\\nwhere the agent makes a mistake on the task.\\n5.2\\nLimitations\\nOne limitation of this experiment is that by eliminating factors other than the\\nagent’s reaction and the human’s preference, the task itself was perceived as te-\\ndious, and the simplicity of the task may have reduced empathy. By not allowing\\nRVA to engage in conversation or introduce themselves beyond the scheduling\\ntask, participants may have decreased their impression of RVA. Also, RVA were\\nsilent in this experiment, which was also done to eliminate the effect of voice on\\nempathy and thus simplified the agents’ reactions.\\nAlthough there was no need for an in-person experiment in this experiment,\\nan in-person experiment using actual equipment could have made a difference\\nin the impact on participants’ impressions. A scheduling task was used in this\\nstudy to investigate whether humans empathize with RVA and accept their mis-\\ntakes even when they make mistakes on a task. However, even when agents’\\nmistakes are acceptable, it is necessary to investigate the extent to which they\\nare acceptable.\\n6\\nConclusions\\nIn this study, we investigated agent reaction and human’s preference, focusing\\non human-agent task error as a factor that causes humans to empathize with\\nRVA. RVA was designed to be in charge of managing the human’s schedule\\nand to make some mistakes in the input information. Two hypotheses were\\nformulated and tested. The results showed that empathy toward RVA decreased\\nwhen RVA made a mistake on the task. In addition, agent reaction and human’s\\n12\\nT. Tsumura et al.\\npreference were shown to be effective in helping humans accept agent mistakes.\\nHowever, it was shown that the use of either factor was not effective. Future\\nresearch should investigate empathy and acceptance toward agents when they\\ndo not make mistakes on a task since it was confirmed that empathy toward\\nRVA decreases when they make mistakes on a task.\\nAcknowledgments\\nThis work was partially supported by JST, CREST (JPMJCR21D4), Japan.\\nThis work was also supported by JST, the establishment of university fellow-\\nships towards the creation of science technology innovation, Grant Number JP-\\nMJFS2136.\\nReferences\\n1. Al Farisi, R., Ferdiana, R., Adji, T.B.: The effect of anthropomorphic design\\ncues on increasing chatbot empathy. In: 2022 1st International Conference on\\nInformation System & Information Technology (ICISIT). pp. 370–375 (2022).\\nhttps://doi.org/10.1109/ICISIT54091.2022.9873008\\n2. Baron-Cohen, S., Wheelwright, S.: The empathy quotient: an investigation of\\nadults with asperger syndrome or high functioning autism, and normal sex dif-\\nferences. J Autism Dev Disord 34(2), 163–175 (Apr 2004)\\n3. Crump, M.J.C., McDonnell, J.V., Gureckis, T.M.: Evaluating amazon’s mechanical\\nturk as a tool for experimental behavioral research. PLOS ONE 8(3), 1–18 (03\\n2013). https://doi.org/10.1371/journal.pone.0057410, https://doi.org/10.1371/\\njournal.pone.0057410\\n4. Davis, M.H.: A multidimensional approach to individual difference in empathy. In:\\nJSAS Catalog of Selected Documents in Psychology. p. 85 (1980)\\n5. Davis, R.: Web-based administration of a personality questionnaire: Comparison\\nwith traditional methods. Behavior Research Methods, Instruments, & Computers\\n31, 572–577 (1999), https://doi.org/10.3758/BF03200737\\n6. Johanson, D., Ahn, H.S., Goswami, R., Saegusa, K., Broadbent, E.: The ef-\\nfects of healthcare robot empathy statements and head nodding on trust\\nand satisfaction: A video study. J. Hum.-Robot Interact. 12(1) (feb 2023).\\nhttps://doi.org/10.1145/3549534, https://doi.org/10.1145/3549534\\n7. Leite, I., Castellano, G., Pereira, A., Martinho, C., Paiva, A.: Empathic\\nrobots\\nfor\\nlong-term\\ninteraction.\\nInternational\\nJournal\\nof\\nSocial\\nRobotics\\n(2014). https://doi.org/10.1007/s12369-014-0227-1, https://doi.org/10.1007/\\ns12369-014-0227-1\\n8. Mathur, L., Spitale, M., Xi, H., Li, J., Matari´c, M.J.: Modeling user em-\\npathy elicited by a robot storyteller. In: 2021 9th International Conference\\non Affective Computing and Intelligent Interaction (ACII). pp. 1–8 (2021).\\nhttps://doi.org/10.1109/ACII52823.2021.9597416\\n9. Nomura,\\nT.,\\nKanda,\\nT.,\\nKidokoro,\\nH.,\\nSuehiro,\\nY.,\\nYamada,\\nS.:\\nWhy\\ndo\\nchildren\\nabuse\\nrobots?\\nInteraction\\nStudies\\n17(3),\\n347–369\\n(2016).\\nhttps://doi.org/https://doi.org/10.1075/is.17.3.02nom,\\nhttps:\\n//www.jbe-platform.com/content/journals/10.1075/is.17.3.02nom\\nRVA errors are accepted by reaction and human’s preference\\n13\\n10. Nomura, T., Kanda, T., Suzuki, T.: Experimental investigation into influence of\\nnegative attitudes toward robots on human–robot interaction. AI & SOCIETY\\n20(2), 138–150 (Mar 2006). https://doi.org/10.1007/s00146-005-0012-7, https://\\ndoi.org/10.1007/s00146-005-0012-7\\n11. Nomura, T., Kanda, T., Suzuki, T., Kato, K.: Prediction of human behavior in\\nhuman–robot interaction using psychological scales for anxiety and negative at-\\ntitudes toward robots. IEEE Transactions on Robotics 24(2), 442–451 (2008).\\nhttps://doi.org/10.1109/TRO.2007.914004\\n12. Okamura, K., Yamada, S.: Adaptive trust calibration for human-ai collaboration.\\nPLOS ONE 15(2), 1–20 (02 2020). https://doi.org/10.1371/journal.pone.0229132,\\nhttps://doi.org/10.1371/journal.pone.0229132\\n13. Okanda, M., Taniguchi, K., Itakura, S.: The role of animism tendencies and empa-\\nthy in adult evaluations of robot. In: Proceedings of the 7th International Confer-\\nence on Human-Agent Interaction. p. 51–58. HAI ’19, Association for Computing\\nMachinery, New York, NY, USA (2019). https://doi.org/10.1145/3349537.3351891,\\nhttps://doi.org/10.1145/3349537.3351891\\n14. Omdahl, B.L.: Cognitive appraisal, emotion, and empathy. Lecture Notes in Com-\\nputer Science, Psychology Press, New York, 1 edn. (1995), https://doi.org/10.\\n4324/9781315806556\\n15. Paiva, A.: Empathy in social agents. International Journal of Virtual Reality\\n10(1),\\n1–4 (Jan 2011). https://doi.org/10.20870/IJVR.2011.10.1.2794, https:\\n//ijvr.eu/article/view/2794\\n16. Paiva, A., Dias, J., Sobral, D., Aylett, R., Sobreperez, P., Woods, S., Zoll, C.,\\nHall, L.: Caring for agents and agents that care: Building empathic relations with\\nsynthetic agents. Autonomous Agents and Multiagent Systems, International Joint\\nConference on 1, 194–201 (01 2004). https://doi.org/10.1109/AAMAS.2004.82\\n17. Paiva, A., Leite, I., Boukricha, H., Wachsmuth, I.: Empathy in virtual agents\\nand robots: A survey. ACM Trans. Interact. Intell. Syst. 7(3) (Sep 2017).\\nhttps://doi.org/10.1145/2912150, https://doi.org/10.1145/2912150\\n18. Preston,\\nS.D.,\\nde\\nWaal,\\nF.B.M.:\\nEmpathy:\\nIts\\nultimate\\nand\\nprox-\\nimate\\nbases.\\nBehavioral\\nand\\nBrain\\nSciences\\n25(1),\\n1–20\\n(2002).\\nhttps://doi.org/10.1017/S0140525X02000018\\n19. Reeves, B., Nass, C.: The Media Equation: How People Treat Computers, Tele-\\nvision, and New Media like Real People and Places. Cambridge University Press,\\nUSA (1996)\\n20. Samrose, S., Anbarasu, K., Joshi, A., Mishra, T.: Mitigating boredom using an\\nempathetic conversational agent. In: Proceedings of the 20th ACM International\\nConference on Intelligent Virtual Agents. IVA ’20, Association for Computing Ma-\\nchinery, New York, NY, USA (2020). https://doi.org/10.1145/3383652.3423905,\\nhttps://doi.org/10.1145/3383652.3423905\\n21. Tsumura, T., Yamada, S.: Agents facilitate one category of human empa-\\nthy through task difficulty. In: 2022 31st IEEE International Conference on\\nRobot and Human Interactive Communication (RO-MAN). pp. 22–28 (2022).\\nhttps://doi.org/10.1109/RO-MAN53752.2022.9900686\\n22. Tsumura, T., Yamada, S.: Influence of agent’s self-disclosure on human empathy.\\nPLOS ONE 18(5), 1–24 (05 2023). https://doi.org/10.1371/journal.pone.0283955,\\nhttps://doi.org/10.1371/journal.pone.0283955\\n23. Tsumura,\\nT.,\\nYamada,\\nS.:\\nInfluence\\nof\\nanthropomorphic\\nagent\\non\\nhu-\\nman\\nempathy\\nthrough\\ngames.\\nIEEE\\nAccess\\n11,\\n40412–40429\\n(2023).\\nhttps://doi.org/10.1109/ACCESS.2023.3269301\\n'),\n",
              " Document(metadata={'Published': '2014-05-07', 'Title': 'On Networks with Active and Passive Agents', 'Authors': 'Tansel Yucelen', 'Summary': 'We introduce an active-passive networked multiagent system framework, which\\nconsists of agents subject to exogenous inputs (active agents) and agents\\nwithout any inputs (passive agents), and analyze its convergence using Lyapunov\\nstability.', 'entry_id': 'http://arxiv.org/abs/1405.1480v1', 'published_first_time': '2014-05-07', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'math.OC', 'categories': ['math.OC', 'math.DS'], 'links': ['http://arxiv.org/abs/1405.1480v1', 'http://arxiv.org/pdf/1405.1480v1']}, page_content='arXiv:1405.1480v1  [math.OC]  7 May 2014\\nOn Networks with Active and Passive Agents\\nTansel Yucelen\\nAbstract— We introduce an active–passive networked multi-\\nagent system framework, which consists of agents subject to\\nexogenous inputs (active agents) and agents without any inputs\\n(passive agents), and analyze its convergence using Lyapunov\\nstability.\\nA. Preliminaries\\nIn the multiagent literature, graphs are broadly adopted\\nto encode interactions in networked systems [1], [2]. An\\nundirected graph G is deﬁned by a set VG = {1, . . . , n} of\\nnodes and a set EG ⊂VG × VG of edges. If (i, j) ∈EG, then\\nthe nodes i and j are neighbors and the neighboring relation\\nis indicated with i ∼j. The degree of a node is given by the\\nnumber of its neighbors. Letting di be the degree of node i,\\nthen the degree matrix of a graph G, D(G) ∈Rn×n, is given\\nby D(G) ≜diag(d), d = [d1, . . . , dn]T. A path i0i1 . . . iL is\\na ﬁnite sequence of nodes such that ik−1 ∼ik, k = 1, . . . , L,\\nand a graph G is connected if there is a path between any\\npair of distinct nodes. The adjacency matrix of a graph G,\\nA(G) ∈Rn×n, is given by\\n[A(G)]ij\\n≜\\n(\\n1,\\nif (i, j) ∈EG,\\n0,\\notherwise.\\n(1)\\nThe Laplacian matrix of a graph, L(G) ∈S\\nn×n\\n+\\n, playing a\\ncentral role in many graph theoretic treatments of multiagent\\nsystems, is given by\\nL(G)\\n≜\\nD(G) −A(G).\\n(2)\\nThroughout this note, we model a given multiagent system\\nby a connected, undirected graph G, where nodes and edges\\nrepresent agents and inter-agent communication links, re-\\nspectively.\\nB. Problem Formulation\\nConsider a system of n agents exchanging information\\namong each other using their local measurements according\\nto a connected, undirected graph G. In addition, consider that\\nT. Yucelen is an Assistant Professor of the Mechanical and Aerospace\\nEngineering Department and the Director of the Advanced Systems Re-\\nsearch Laboratory at the Missouri University of Science and Technology,\\nRolla, MO 65409, USA (e-mail: yucelen@mst.edu).\\nthere exists m ≥1 exogenous inputs that interact with this\\nsystem. We make the following deﬁnitions.\\nDeﬁnition 1. If agent i, i = 1, . . . , n, is subject to one or\\nmore exogenous inputs (resp., no exogenous inputs), then it\\nis an active agent (resp., passive agent).\\nDeﬁnition 2. If an exogenous input interacts with only\\none agent (resp., multiple agents), then it is an isolated input\\n(resp., non-isolated input).\\nIn this note, we are interested in the problem of driving\\nthe states of all (active and passive) agents to the average\\nof the applied exogenous inputs. Motivating from this stand-\\npoint, we propose an integral action-based distributed control\\napproach given by\\n˙xi(t)\\n=\\n−\\nX\\ni∼j\\n\\x10\\nxi(t) −xj(t)\\n\\x11\\n+\\nX\\ni∼j\\n\\x10\\nξi(t) −ξj(t)\\n\\x11\\n−\\nX\\ni∼h\\n\\x10\\nxi(t) −ch\\n\\x11\\n,\\nxi(0) = xi0,\\n(3)\\n˙ξi(t)\\n=\\n−\\nX\\ni∼j\\n\\x10\\nxi(t) −xj(t)\\n\\x11\\n,\\nξi(0) = ξi0,\\n(4)\\nwhere xi(t) ∈R and ξi(t) ∈R denote the state and the\\nintegral action of agent i, i = 1, . . . , n, respectively, and\\nch ∈R, h = 1, . . . , m, denotes an exogenous input applied\\nto this agent. Similar to the i ∼j notation indicating the\\nneighboring relation between agents, we use i ∼h to indicate\\nthe exogenous inputs that an agent is subject to.\\nNext, let\\nx(t)\\n=\\n\\x02\\nx1(t), x2(t), . . . , xn(t)\\n\\x03T∈\\nRn,\\nξ(t) =\\n\\x02\\nξ1(t), ξ2(t), . . . , ξn(t)\\n\\x03T∈Rn, and c =\\n\\x02\\nc1, c2,\\n. . . , cm, 0, . . . , 0\\n\\x03\\n∈Rn, where we assume m ≤n for the\\nease of the following notation and without loss of generality.\\nWe can now write (3) and (4) in a compact form as\\n˙x(t)\\n=\\n−L(G)x(t) + L(G)ξ(t) −K1x(t) + K2c,\\nx(0) = x0,\\n(5)\\n˙ξ(t)\\n=\\n−L(G)x(t),\\nξ(0) = ξ0,\\n(6)\\nwhere L(G) ∈S\\nn×n\\n+\\n,\\nK1\\n≜\\ndiag([k1,1, k1,2, . . . , k1,n]T) ∈S\\nn×n\\n+\\n,\\n(7)\\nwith k1,i ∈Z+ denoting the number of the exogenous inputs\\napplied to agent i, i = 1, . . . , n, and\\nK2\\n≜\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nk2,11\\nk2,12\\n· · ·\\nk2,1n\\nk2,21\\nk2,22\\n· · ·\\nk2,2n\\n...\\n...\\n...\\n...\\nk2,n1\\nk2,n2\\n· · ·\\nk2,nn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈Rn×n,\\n(8)\\nwith k2,ih = 1 if the exogenous input ch(t), h = 1, . . . , m,\\nis applied to agent i, i = 1, . . . , n, and k2,ih = 0 otherwise.\\nNote that k1,i = Pn\\nj=1 k2,ij.\\nSince we are interested in driving the states of all (active\\nand passive) agents to the average of the applied exogenous\\ninputs, let\\nδ(t)\\n≜\\nx(t) −ǫ1n ∈Rn,\\n(9)\\nǫ\\n≜\\n1T\\nnK2c\\n1T\\nnK21n\\n∈R,\\n(10)\\nbe the error between xi(t), i = 1, . . . , n, and the average\\nof the applied exogenous inputs ǫ. Based on (10), ǫ can be\\nequivalently written as\\nǫ\\n=\\n\\x10\\nk2,11c1 + k2,12c2 + · · · + k2,21c1\\n+k2,22c2 + · · ·\\n\\x11\\n/\\n\\x10\\nk2,11 + k2,12\\n+ · · · + k2,21 + k2,21 + · · ·\\n\\x11\\n,\\n(11)\\nwhich is the average of the applied exogenous inputs.\\nC. Convergence Analysis\\nIt follows from (9) and L(G)1n = 0n of Lemma 1 that\\n˙δ(t)\\n=\\n−L(G)\\n\\x02\\nδ(t) + ǫ1n\\n\\x03\\n+L(G)ξ(t) −K1\\n\\x02\\nδ(t)\\n+ǫ1n\\n\\x03\\n+K2c(t)\\n=\\n−F(G)δ(t) + L(G)ξ(t) −\\n\\x02\\nK11nǫ −K2c\\n\\x03\\n=\\n−F(G)δ(t) + L(G)ξ(t) −\\n\\x14K11n1T\\nnK2c\\n1T\\nnK21n\\n−K2c\\n\\x15\\n=\\n−F(G)δ(t) + L(G)ξ(t) −LcK2c,\\n(12)\\nwhere F(G) ≜L(G) + K1 and\\nLc\\n≜\\nK11n1T\\nn\\n1T\\nnK21n\\n−In.\\n(13)\\nNote that F(G) ∈Sn×n\\n+\\nand\\n1T\\nnLc = 1T\\nn\\n\\x14K11n1T\\nn\\n1T\\nnK21n\\n−In\\n\\x15\\n= 1T\\nnK11n\\n1T\\nnK21n\\n1T\\nn −1T\\nn = 0, (14)\\nsince (1T\\nnK11n)/(1T\\nnK21n) = 1 from k1,i = Pn\\nj=1 k2,ij.\\nNext, letting\\ne(t)\\n≜\\nξ(t) −L†(G)LcK2c,\\n(15)\\nand using (15) in (12) yields\\n˙δ(t) = −F(G)δ(t) + L(G)\\n\\x02\\ne(t) + L†(G)LcK2c\\n\\x03\\n−LcK2c\\n= −F(G)δ(t) + L(G)e(t) +\\nh\\nIn −1\\nn1n1T\\nn\\ni\\nLcK2c\\n−LcK2c\\n= −F(G)δ(t) + L(G)e(t),\\n(16)\\nsince 1\\nn1n1T\\nnLcK2c = 0 as a direct consequence of (14). In\\naddition, differentiating (15) with respect to time yields\\n˙e(t)\\n=\\n−L(G)\\n\\x02\\nδ(t) + ǫ1n\\n\\x03\\n=\\n−L(G)δ(t),\\n(17)\\nwhere L(G)1n = 0n. The following theorem shows that\\nthe state of all agents xi(t), i = 1, . . . , n asymptotically\\nconverge to ǫ.\\nTheorem 1. Consider the networked multiagent system\\ngiven by (3) and (4), where agents exchange information\\nusing local measurements and with G deﬁning a connected,\\nundirected graph topology. Then, the closed-loop error dy-\\nnamics deﬁned by (16) and (17) are Lyapunov stable for all\\ninitial conditions and δ(t) asymptotically vanishes.\\nProof. Proof follows by considering Lyapunov function\\ncandidate given by V (δ, e) = 1\\n2δTδ + 1\\n2eTe and differentiat-\\ning it along the trajectories of (16) and (17).\\nNote that a generalized version of the proposed integral\\naction-based distributed control approach can be given by\\n˙xi(t)\\n=\\n−α\\nX\\ni∼j\\n\\x10\\nxi(t) −xj(t)\\n\\x11\\n+\\nX\\ni∼j\\n\\x10\\nξi(t) −ξj(t)\\n\\x11\\n−α\\nX\\ni∼h\\n\\x10\\nxi(t) −ch\\n\\x11\\n,\\nxi(0) = xi0,\\n(18)\\n˙ξi(t)\\n=\\n−γ\\nX\\ni∼j\\n\\x10\\nxi(t) −xj(t)\\n\\x11\\n,\\nξi(0) = ξi0,\\n(19)\\nwhere α ∈R+ and γ ∈R+.\\nD. Concluding Remarks\\nWe investigated a system consisting of agents subject to\\nexogenous constant inputs and agents without any inputs.\\nFuture research will consider extensions to time-varying\\nexogenous inputs and more general graph topologies.\\nREFERENCES\\n[1] M. Mesbahi and M. Egerstedt, “Graph Theoretic Methods in Multi-\\nagent Networks,” Princeton University Press, 2010.\\n[2] R. Olfati-Saber, J. Fax, and R. M. Murray, “Consensus and Cooper-\\nation in Networked Multiagent Systems,” Proc. of the IEEE, vol. 95,\\npp. 215–233, 2007.\\n'),\n",
              " Document(metadata={'Published': '2021-01-18', 'Title': 'Cooperative and Competitive Biases for Multi-Agent Reinforcement Learning', 'Authors': 'Heechang Ryu, Hayong Shin, Jinkyoo Park', 'Summary': \"Training a multi-agent reinforcement learning (MARL) algorithm is more\\nchallenging than training a single-agent reinforcement learning algorithm,\\nbecause the result of a multi-agent task strongly depends on the complex\\ninteractions among agents and their interactions with a stochastic and dynamic\\nenvironment. We propose an algorithm that boosts MARL training using the biased\\naction information of other agents based on a friend-or-foe concept. For a\\ncooperative and competitive environment, there are generally two groups of\\nagents: cooperative-agents and competitive-agents. In the proposed algorithm,\\neach agent updates its value function using its own action and the biased\\naction information of other agents in the two groups. The biased joint action\\nof cooperative agents is computed as the sum of their actual joint action and\\nthe imaginary cooperative joint action, by assuming all the cooperative agents\\njointly maximize the target agent's value function. The biased joint action of\\ncompetitive agents can be computed similarly. Each agent then updates its own\\nvalue function using the biased action information, resulting in a biased value\\nfunction and corresponding biased policy. Subsequently, the biased policy of\\neach agent is inevitably subjected to recommend an action to cooperate and\\ncompete with other agents, thereby introducing more active interactions among\\nagents and enhancing the MARL policy learning. We empirically demonstrate that\\nour algorithm outperforms existing algorithms in various mixed\\ncooperative-competitive environments. Furthermore, the introduced biases\\ngradually decrease as the training proceeds and the correction based on the\\nimaginary assumption vanishes.\", 'entry_id': 'http://arxiv.org/abs/2101.06890v1', 'published_first_time': '2021-01-18', 'comment': 'Accepted as a full paper at the Twentieth International Conference on\\n  Autonomous Agents and Multiagent Systems (AAMAS-21), Virtual Conference', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.AI', 'cs.MA'], 'links': ['http://arxiv.org/abs/2101.06890v1', 'http://arxiv.org/pdf/2101.06890v1']}, page_content='Cooperative and Competitive Biases\\nfor Multi-Agent Reinforcement Learning\\nHeechang Ryu\\nKAIST\\nDaejeon, Republic of Korea\\nrhc93@kaist.ac.kr\\nHayong Shin\\nKAIST\\nDaejeon, Republic of Korea\\nhyshin@kaist.ac.kr\\nJinkyoo Park∗\\nKAIST\\nDaejeon, Republic of Korea\\njinkyoo.park@kaist.ac.kr\\nABSTRACT\\nTraining a multi-agent reinforcement learning (MARL) algorithm is\\nmore challenging than training a single-agent reinforcement learn-\\ning algorithm, because the result of a multi-agent task strongly\\ndepends on the complex interactions among agents and their inter-\\nactions with a stochastic and dynamic environment. We propose\\nan algorithm that boosts MARL training using the biased action\\ninformation of other agents based on a friend-or-foe concept. For a\\ncooperative and competitive environment, there are generally two\\ngroups of agents: cooperative-agents and competitive-agents. In\\nthe proposed algorithm, each agent updates its value function using\\nits own action and the biased action information of other agents\\nin the two groups. The biased joint action of cooperative agents is\\ncomputed as the sum of their actual joint action and the imaginary\\ncooperative joint action, by assuming all the cooperative agents\\njointly maximize the target agent’s value function. The biased joint\\naction of competitive agents can be computed similarly. Each agent\\nthen updates its own value function using the biased action informa-\\ntion, resulting in a biased value function and corresponding biased\\npolicy. Subsequently, the biased policy of each agent is inevitably\\nsubjected to recommend an action to cooperate and compete with\\nother agents, thereby introducing more active interactions among\\nagents and enhancing the MARL policy learning. We empirically\\ndemonstrate that our algorithm outperforms existing algorithms in\\nvarious mixed cooperative-competitive environments. Furthermore,\\nthe introduced biases gradually decrease as the training proceeds\\nand the correction based on the imaginary assumption vanishes.\\nKEYWORDS\\nMulti-Agent Reinforcement Learning; Cooperation; Competition;\\nBias\\nACM Reference Format:\\nHeechang Ryu, Hayong Shin, and Jinkyoo Park∗. 2021. Cooperative and\\nCompetitive Biases for Multi-Agent Reinforcement Learning. In Proc. of the\\n20th International Conference on Autonomous Agents and Multiagent Systems\\n(AAMAS 2021), Online, May 3–7, 2021, IFAAMAS, 10 pages.\\n1\\nINTRODUCTION\\nReinforcement learning (RL) algorithms solve sequential decision-\\nmaking problems using experiences obtained by a single agent (de-\\ncision maker) dynamically interacting with an environment. The RL\\nalgorithms typically estimate an action-value function (𝑄-function)\\n∗Corresponding author\\nProc. of the 20th International Conference on Autonomous Agents and Multiagent Systems\\n(AAMAS 2021), U. Endriss, A. Nowé, F. Dignum, A. Lomuscio (eds.), May 3–7, 2021, Online.\\n© 2021 International Foundation for Autonomous Agents and Multiagent Systems\\n(www.ifaamas.org). All rights reserved.\\nor a decision-making policy, using various function approximators\\n(e.g., deep neural networks), to model how a particular action (deci-\\nsion) affects future outcomes. From this model, the optimal action\\nin the current state for completing a target task can be deduced [17].\\nThe success of RL algorithms for solving various tasks depends on\\nhow effectively they learn such temporal interactions between an\\naction and a future outcome.\\nIn particular, when the RL algorithm is trained with a sparse or\\ndelayed reward, i.e., a reward signal is infrequently realized after\\nan agent executes action, it becomes difficult to estimate the 𝑄-\\nfunction or policy. This is because it is challenging for the RL agent\\nto learn the dynamic causal effect of its action on future outcomes,\\nowing to the sparse and delayed reward signal. A representative\\nexample of such a task with a sparse reward is a goal-oriented task,\\nin which a binary reward is given only when an agent reaches\\na goal. To solve this task, HER [1] has been proposed to learn\\nthe 𝑄-function or policy using the reward signals obtained from\\nfailed tasks (episodes), while considering these reward signals as\\nbeing obtained from the successful tasks that are different from the\\noriginal task. This strategy of HER transforms the sparse-reward\\nenvironment into a dense-reward environment, enabling the RL\\nagent to easily learn the 𝑄-function or policy. In addition, HPG [13]\\nextends the concept of HER to efficiently generalize learning about\\ndifferent goals using information obtained by the current policy for\\na specific goal.\\nRecently, to enhance the performance of multi-agent reinforce-\\nment learning (MARL) algorithms, which are extensions of the RL\\nalgorithms to multi-agent settings, various strategies have been\\nproposed. Some researchers have proposed an intrinsic reward\\nto induce certain collective behaviors of multiple agents that are\\nbelieved to help achieve the objective of a multi-agent task. For\\nexample, intrinsic reward is designed to promote the agents to\\nexecute actions influencing other agents’ state transitions [2, 18]\\nor visit unexplored state spaces more frequently [6]. However, in\\ngeneral, designing a good intrinsic reward is difficult because it\\nrequires prior knowledge of the types of interactions that help solve\\nthe multi-agent task. Moreover, designing an effective intrinsic re-\\nward often requires an iterative reward-shaping procedure until\\nsatisfactory performance is reached. Consequently, for multi-agent\\ntasks where a sparse reward is given and prior knowledge is not\\navailable, another effective method for MARL should be developed.\\nTo address these difficulties, we herein propose an algorithm,\\ncalled Friend-or-Foe multi-agent Deep Deterministic Policy Gradient\\n(F2DDPG), which boosts MARL training using the biased action\\ninformation of other agents based on a friend-or-foe concept [11].\\nF2DDPG adopts an actor-critic algorithm in a centralized training\\narXiv:2101.06890v1  [cs.LG]  18 Jan 2021\\nand decentralized execution (CTDE) framework [12]. For a cooper-\\native and competitive environment, there are generally two groups\\nof agents: an ally group composed of cooperative agents to a tar-\\nget agent and an enemy group composed of competitive agents\\nto a target agent. In the proposed algorithm, each agent updates\\nits 𝑄-function (critic) using its own action and the biased action\\ninformation of other agents in the two groups. The biased joint\\naction of cooperative agents is computed as the sum of their actual\\njoint action and the imaginary cooperative joint action obtained\\nby assuming that all the cooperative agents jointly maximize the\\ntarget agent’s critic. The biased joint action of competitive agents\\ncan be computed similarly. Each agent then updates its own critic\\nusing biased action information, resulting in a biased critic and\\ncorresponding biased policy. Thereafter, the biased policy of each\\nagent is inevitably subjected to recommend an action to cooper-\\nate and compete with other agents, which introduces more active\\ninteractions among agents, and thus, enhances the MARL policy\\nlearning.\\nUsing biased actions in estimating a critic can be viewed as a\\ndifferent version of using the biased reward (intrinsic reward) in\\nthe estimation to induce the intended interactions among agents.\\nHowever, we do not compare our method with MARL approaches\\nusing the intrinsic reward because they require a certain type of\\nprior knowledge about the environment (game). Instead, we com-\\npare F2DDPG with M3DDPG [8], the latter of which uses mod-\\nified (biased) action information of other agents when training\\nthe critic; it modifies the other agents’ actions in an adversarial\\nmanner to induce a robust policy. Empirically, we demonstrate\\nthat our algorithm outperforms existing algorithms in four mixed\\ncooperative-competitive game scenarios, in which the agents have\\ncooperative and competitive interactions [12]. Furthermore, we em-\\npirically show that the introduced biases gradually decrease as the\\ntraining proceeds and that the correction based on the imaginary\\nassumption vanishes.\\n2\\nRELATED WORK\\n2.1\\nFriend-or-Foe Q-Learning\\nIn general-sum games, such as mixed cooperative-competitive\\ngames, friend-or-foe Q-learning (FFQ) [11] has been proposed to\\nprovide strong convergence guarantees compared to the existing\\nNash-equilibrium-based learning rule [4]. It requires that other\\nagents be identified as either ‘friend’ (ally) or ‘foe’ (enemy). FFQ\\nthen assumes that agent 𝑖’s friends are working together to maxi-\\nmize agent 𝑖’s value, while agent 𝑖’s foes are working together to\\nminimize agent 𝑖’s value. Thus, 𝑛-player FFQ considers any game\\nas a two-player zero-sum game with an extended action set, and is\\neasy to implement for multiple agents.\\n2.2\\nBiases in RL and MARL\\nIn RL and MARL, various forms of inductive bias have been used\\nto improve learning. The most straightforward inductive biases\\nentail designing network structures for the critic or policy, such as\\nattention networks [5], graph neural networks [15], and implicit\\ncommunication structures [14]. However, biases in game informa-\\ntion, such as state, reward, and action have also been used in an\\nattempt to boost training. We review the biases in the information\\nin this subsection.\\n2.2.1\\nBiases in States. Bias has been reported to help train RL by\\ninjecting a biased belief regarding the state at the initialization\\nstage of the 𝑄-table [3]. For example, in a goal-oriented task, if the\\ngoal is known in advance, biased information about the state, near\\nand far from the goal, is injected into the 𝑄-table before training. In\\naddition, a distributed 𝑄-learning algorithm for a cooperative multi-\\nagent setting has been proposed based on the optimistic assumption\\n[7]. Under this assumption, the algorithm biasedly updates the 𝑄-\\ntable only when the new value for 𝑄is greater than the current\\nvalue in the current state.\\n2.2.2\\nBiases in Rewards. Intrinsic rewards have been proposed\\nas a bias for multi-agent exploration to induce certain collective\\nbehaviors of agents, based on prior knowledge of the types of inter-\\nactions that help solve the multi-agent task. The intrinsic rewards\\nare provided when one agent’s action affects the state transitions of\\nother agents [2, 18] and when all the agents explore only different\\nor the same areas for the task of collecting scattered treasures [6].\\n2.2.3\\nBiases in Actions. M3DDPG [8] has been proposed to learn\\na robust policy using other agents’ action information corrupted\\nwith adversarial noise. In this approach, each agent assumes that\\nother agents provide the adversarial actions to the target agent,\\nand updates its critic using such adversarial action information. To\\ncompute the adversarial action (biased action), each agent modifies\\nthe actions of other agents in the direction that minimizes the\\ntarget agent’s critic. M3DDPG asserts that the trained policy using\\nthis biased action information outperforms its baseline algorithm,\\nMADDPG [12]. However, the limitation of this approach is that it\\ndoes not consider the relationships among agents. It assumes that all\\nthe agents are adversarial to the target agent, regardless of whether\\nthey are allies or enemies in the cooperative and competitive game;\\nthis assumption is inconsistent with the actual situation.\\nNotably, M3DDPG is similar to our method as it uses the biased\\ninformation of other agents’ actions. However, F2DDPG explic-\\nitly considers the roles of agents in a cooperative and competitive\\nenvironment, in which the cooperative and competitive agents\\nare known a priori. In addition, our method can be justified by\\nthe well-known FFQ [11]. We mainly compare the performance\\nof our method with that of M3DDPG, while not addressing other\\napproaches using different types of information bias. In this study,\\nwe assume that it is possible to identify other agents as being either\\ncooperative (allies) or competitive (enemies) in mixed cooperative-\\ncompetitive environments, to fully implement and compare the\\nproposed method with the baseline methods.\\n3\\nBACKGROUND\\n3.1\\nPartially Observable Markov Game\\nWe consider a partially observable Markov game [10], which is an\\nextension of the partially observable Markov decision process to a\\ngame with multiple agents. A partially observable Markov game\\nfor 𝑁agents is defined as follows: 𝑠∈S denotes the global state\\nof the game; 𝑜𝑖∈S ↦→O𝑖denotes a local observation that agent 𝑖\\ncan acquire correlated with the state; and 𝑎𝑖∈A𝑖is an action of\\n𝑖𝑖\\nreward!\\n𝑖𝑖\\nreward!\\n𝑖𝑖\\nreward!\\n𝐚𝐚−𝑖𝑖\\n∗𝐴𝐴: Imaginary cooperative joint action \\n𝐚𝐚−𝑖𝑖\\n∗𝐸𝐸: Imaginary competitive joint action\\n𝐚𝐚−𝑖𝑖\\n𝐴𝐴: Joint action of ally agents\\n𝐚𝐚−𝑖𝑖\\n𝐸𝐸: Joint action of enemy agents\\nBiased cooperative joint action of ally agents:\\nത𝐚𝐚−𝑖𝑖\\n𝐴𝐴= 𝐚𝐚−𝑖𝑖\\n𝐴𝐴+ 𝛿𝛿𝐴𝐴𝛻𝛻𝐚𝐚−𝑖𝑖\\n𝐴𝐴𝑄𝑄𝑖𝑖\\n𝜇𝜇𝐨𝐨, 𝑎𝑎𝑖𝑖, 𝐚𝐚−𝑖𝑖\\n𝐴𝐴, 𝐚𝐚−𝑖𝑖\\n𝐸𝐸\\nBiased competitive joint action of enemy agents:\\nത𝐚𝐚−𝑖𝑖\\n𝐸𝐸= 𝐚𝐚−𝑖𝑖\\n𝐸𝐸−𝛿𝛿𝐸𝐸𝛻𝛻𝐚𝐚−𝑖𝑖\\n𝐸𝐸𝑄𝑄𝑖𝑖\\n𝜇𝜇𝐨𝐨, 𝑎𝑎𝑖𝑖, 𝐚𝐚−𝑖𝑖\\n𝐴𝐴, 𝐚𝐚−𝑖𝑖\\n𝐸𝐸\\nReal Environment\\nImaginary  Environment\\n𝑄𝑄𝑖𝑖\\n𝜇𝜇𝐨𝐨, 𝑎𝑎𝑖𝑖, 𝐚𝐚−𝑖𝑖\\n𝐴𝐴, 𝐚𝐚−𝑖𝑖\\n𝐸𝐸\\n𝑄𝑄𝑖𝑖\\n𝜇𝜇𝐨𝐨, 𝑎𝑎𝑖𝑖, 𝐚𝐚−𝑖𝑖\\n∗𝐴𝐴,𝐚𝐚−𝑖𝑖\\n∗𝐸𝐸\\n𝑄𝑄𝑖𝑖\\n𝜇𝜇𝐨𝐨, 𝑎𝑎𝑖𝑖, 𝐚𝐚−𝑖𝑖\\n∗𝐴𝐴,𝐚𝐚−𝑖𝑖\\n∗𝐸𝐸≈𝑄𝑄𝑖𝑖\\n𝜇𝜇𝐨𝐨, 𝑎𝑎𝑖𝑖, ത𝐚𝐚−𝑖𝑖\\n𝐴𝐴,ത𝐚𝐚−𝑖𝑖\\n𝐸𝐸\\nBiased Environment\\nFigure 1: Overview of F2DDPG.\\nagent 𝑖. The reward for agent 𝑖is obtained as a function of state\\n𝑠and joint action a as 𝑟𝑖: S × A1 × · · · × A𝑁↦→R. The state\\nevolves to the next state according to the state transition function\\nT : S × A1 × · · · × A𝑁↦→S. The initial state is determined by the\\ninitial state distribution 𝜌: S ↦→[0, 1]. Agent 𝑖aims to maximize\\nits discounted return 𝑅𝑖= Í𝑇\\n𝑡=0 𝛾𝑡𝑟𝑖,𝑡, where𝛾∈[0, 1] is a discount\\nfactor.\\n3.2\\nMulti-Agent Deep Deterministic Policy\\nGradient (MADDPG)\\nWhile the policy can be deterministic, 𝑎= 𝜇(𝑠), or stochastic,\\n𝑎∼𝜋(·|𝑠), deterministic policy gradient (DPG) [16] for RL adopts\\na deterministic policy. DPG aims to directly derive the determin-\\nistic policy, 𝑎= 𝜇(𝑠;𝜃), that maximizes the expected return or\\nobjective J (𝜃) = E𝑠∼𝜌𝜇,𝑎∼𝜇𝜃[𝑅] ≈E𝑠∼𝜌𝜇,𝑎∼𝜇𝜃[𝑄𝜇(𝑠,𝑎;𝜙)], where\\n𝑄𝜇(𝑠,𝑎;𝜙) = E𝑠′ [𝑟+𝛾E𝑎′∼𝜇𝜃[𝑄𝜇(𝑠′,𝑎′;𝜙)]]. Parameter 𝜃of 𝜇(𝑠;𝜃)\\nis subsequently optimized by the gradient of J (𝜃) as ∇𝜃J (𝜃) =\\nE𝑠∼D [∇𝜃𝜇(𝑠;𝜃)∇𝑎𝑄𝜇(𝑠,𝑎;𝜙)|𝑎=𝜇(𝑠;𝜃)]. D is an experience replay\\nbuffer that stores (𝑠,𝑎,𝑟,𝑠′) samples obtained from the training\\nepisodes. Deep deterministic policy gradient (DDPG) [9], an actor-\\ncritic algorithm based on DPG, uses deep neural networks to ap-\\nproximate critic 𝑄𝜇(𝑠,𝑎;𝜙) and actor 𝜇(𝑠;𝜃) of the agent.\\nMADDPG is a multi-agent extension of DDPG for deriving de-\\ncentralized policies in the CTDE framework. In MADDPG, each\\nagent learns an individual policy that maps the observation to its\\naction to maximize its expected return, which is approximated by\\nthe 𝑄-network. MADDPG comprises individual 𝑄-networks and\\npolicy networks for each agent. MADDPG lets the 𝑄-network (cen-\\ntralized critic) of agent 𝑖be trained by minimizing the loss with the\\ntarget 𝑄-value, 𝑦𝑖, as follows:\\nL(𝜙𝑖) = Eo,a,𝑟,o′∼D [(𝑄𝜇\\n𝑖(o, a;𝜙𝑖) −𝑦𝑖)2],\\n𝑦𝑖= 𝑟𝑖+ 𝛾𝑄𝜇′\\n𝑖(o′, a′;𝜙′\\n𝑖)|𝑎′\\n𝑗=𝜇′\\n𝑗(𝑜′\\n𝑗;𝜃′\\n𝑗),\\n(1)\\nwhere o = (𝑜1, . . . ,𝑜𝑁) and a = (𝑎1, . . . ,𝑎𝑁) represent the joint\\nobservation and joint action of all agents, respectively. D is an\\nexperience replay buffer that stores (o, a,𝑟, o′) samples obtained\\nfrom the training episodes. 𝑄𝜇′ and 𝜇′ are target networks for the\\nstable learning of𝑄and policy networks. The policy network (actor),\\n𝜇𝑖(𝑜𝑖;𝜃𝑖), of agent 𝑖is optimized using the gradient computed as\\n∇𝜃𝑖J (𝜃𝑖) = Eo,a∼D [∇𝜃𝑖𝜇𝑖(𝑜𝑖;𝜃𝑖)∇𝑎𝑖𝑄𝜇\\n𝑖(o, a;𝜙𝑖)|𝑎𝑖=𝜇𝑖(𝑜𝑖;𝜃𝑖)].\\n(2)\\n4\\nMETHODS\\nF2DDPG learns the critic and actor of each agent using the biased\\naction information of other agents based on a friend-or-foe con-\\ncept [11]. In F2DDPG, as shown in Figure 1, each agent has two\\nperceptions on the environment:\\n• The real environment, where agent 𝑖’s true critic can be esti-\\nmated using its own action and the realized (actual) actions,\\n(𝑎𝑖, a𝐴\\n−𝑖, a𝐸\\n−𝑖);\\n• An imaginary environment, where agent 𝑖’s imaginary critic\\ncan be estimated using its own action and the imaginary co-\\noperative and competitive joint actions, (𝑎𝑖, a∗𝐴\\n−𝑖, a∗𝐸\\n−𝑖). These\\nare computed based on the assumption that all the ally agents\\nexecute the cooperative joint action to agent 𝑖, while all the\\nenemy agents execute the competitive joint action to agent\\n𝑖.\\nIn F2DDPG, each agent learns the decentralized actor (policy) by\\napplying the following three iterative procedures: (1) computing the\\nbiased actions, a𝐴\\n−𝑖and a𝐸\\n−𝑖, by combining the real joint action and\\nimaginary cooperative and competitive joint actions; (2) updating\\nthe biased critic using the biased actions; and (3) updating the actor\\nusing the biased critic.\\nThus, the updated policy with biased cooperative-competitive\\njoint actions is more likely to recommend such cooperative and\\ncompetitive actions to other agents, which introduces meaningful\\ninteractions among agents and enhances policy learning. Therefore,\\nusing the biased actions, F2DDPG can learn the level of cooperation\\nand competition among agents in a sample-efficient manner. The\\nbiased actions become increasingly closer to the actually executed\\nactions as the training proceeds, implying that the biases in the\\nactors vanish.\\nFigure 1 illustrates how agent 𝑖(predator) updates its critic in\\nF2DDPG, while playing the 3 vs. 3 predator-prey game. In this\\ngame, the three predators (red circles) attempt to capture the three\\nprey (green squares) together. Because the prey are faster than the\\npredators, the predators (ally group) must cooperate strategically\\nto capture the prey (enemy group). In the early stages of learn-\\ning, however, it is difficult for the predators to achieve a reward.\\nWhenever the reward is realized, whether by chance or strategic\\nmoves, predator 𝑖computes imaginary actions by assuming that\\nthe reward is realized when the other two predators choose the\\noptimal cooperative joint action for predator 𝑖, while the other\\nthree prey execute the competitive joint action to predator 𝑖. Agent\\n𝑖updates its critic using its own action and the one-step biased\\njoint cooperative-competitive actions of other agents.\\n4.1\\nComputing Biased\\nCooperative-Competitive Actions\\nConsidering relationships to agent 𝑖, in the mixed cooperative-\\ncompetitive environment, we categorize all agents, except 𝑖, as\\ncooperative and competitive to agent 𝑖as follows:\\n• 𝐴(𝑖): set of agents cooperative to agent 𝑖(ally group);\\n• 𝐸(𝑖): set of agents competitive to agent 𝑖(enemy group);\\n• a𝐴\\n−𝑖: joint action of agents in 𝐴(𝑖);\\n• a𝐸\\n−𝑖: joint action of agents in 𝐸(𝑖).\\nIf one assumes that the agents in 𝐴(𝑖) and 𝐸(𝑖) jointly maximize\\nand minimize the critic of agent 𝑖, respectively, then agent 𝑖can\\nestimate its critic as follows:\\n𝑄𝜇\\n𝑖(o,𝑎𝑖;𝜙𝑖) = max\\na𝐴\\n−𝑖\\nmin\\na𝐸\\n−𝑖\\n𝑄𝜇\\n𝑖(o,𝑎𝑖, a𝐴\\n−𝑖, a𝐸\\n−𝑖;𝜙𝑖).\\n(3)\\nThis estimated critic is biased because each agent in 𝐴(𝑖) or 𝐸(𝑖)\\nexecutes its action to only maximize its own individual critic similar\\nto the process in MADDPG.\\nThe two optimal joint actions (a∗𝐴\\n−𝑖, a∗𝐸\\n−𝑖) that achieve the maxmin\\nvalue in Equation 4 are called a saddle-point equilibrium strategy,\\nwhich is equivalent to the Nash equilibrium strategy, which satisfies\\na∗𝐴\\n−𝑖= argmax\\na𝐴\\n−𝑖\\n𝑄𝜇\\n𝑖(o,𝑎𝑖, a𝐴\\n−𝑖, a∗𝐸\\n−𝑖;𝜙𝑖),\\na∗𝐸\\n−𝑖= argmin\\na𝐸\\n−𝑖\\n𝑄𝜇\\n𝑖(o,𝑎𝑖, a∗𝐴\\n−𝑖, a𝐸\\n−𝑖;𝜙𝑖).\\n(4)\\nWe refer to these two actions in Equation 4 as the imaginary co-\\noperative and competitive joint actions because these two joint\\nactions rarely occur in the real environment.\\nThe estimated critic of agent 𝑖using Equation 4 can be used\\nin a decentralized training setting, where each agent cannot ob-\\nserve other agents’ actions during training, and thus, has to infer\\nthem. However, the current study focuses on developing an efficient\\nMARL algorithm in the CTDE framework that allows each agent\\nto observe other agents’ actions during training. To help stabilize\\ntraining without introducing any bias, other agents’ actions are\\nexplicitly used when learning the critic and associated actor.\\nIn this study, we propose to combine these two different learning\\nparadigms to: (1) achieve reliable and stable learning using the true\\naction information in the CTDE framework and (2) infuse the de-\\nsirable behaviors (information biases) into agents using imaginary\\njoint cooperative-competitive actions (biased action information)\\ncomputed in the decentralized learning framework. To achieve both\\nobjectives, the proposed method computes the biased actions by\\ncombining the actual and imaginary actions as follows:\\na𝐴\\n−𝑖= a𝐴\\n−𝑖+ 𝛿𝐴∇a𝐴\\n−𝑖𝑄𝜇\\n𝑖(o,𝑎𝑖, a𝐴\\n−𝑖, a𝐸\\n−𝑖;𝜙𝑖),\\na𝐸\\n−𝑖= a𝐸\\n−𝑖−𝛿𝐸∇a𝐸\\n−𝑖𝑄𝜇\\n𝑖(o,𝑎𝑖, a𝐴\\n−𝑖, a𝐸\\n−𝑖;𝜙𝑖).\\n(5)\\nIn Equation 5, we compute the one-step-biased cooperative-competitive\\njoint actions, which approximate the imaginary cooperative-competitive\\njoint actions in Equation 4, using the partial gradient of critic 𝑄𝜇\\n𝑖.\\nIn addition, 𝛿𝐴and 𝛿𝐸are the step sizes for the biased cooperative\\nand competitive joint actions, respectively.\\nNote that computing a𝐴\\n−𝑖and a𝐸\\n−𝑖is computationally tractable\\nbecause it only requires computing the partial gradient of the 𝑄-\\nnetwork with respect to the joint action variables. In addition, 𝛿𝐴\\nand 𝛿𝐸adjust the level of biases (injected cooperative and competi-\\ntive biases). The optimal 𝛿𝐴and 𝛿𝐸can be empirically determined\\nduring training; however, these values do not significantly affect\\nthe learning performance because the partial gradient eventually\\nbecomes small, making the amount of action modification negligi-\\nble.\\n4.2\\nLearning the Biased Critic Using Biased\\nActions\\nThe biased actions in Equation 5 are then used to update the 𝑄-\\nnetwork as:\\nL(𝜙𝑖) = Eo,a,𝑟,o′∼D [(𝑄𝜇\\n𝑖(o, a;𝜙𝑖) −𝑦𝑖)2,\\n𝑦𝑖= 𝑟𝑖+ 𝛾𝑄𝜇′\\n𝑖(o′,𝑎′\\n𝑖, a′𝐴\\n−𝑖, a′𝐸\\n−𝑖;𝜙′\\n𝑖)|𝑎′\\n𝑖=𝜇′\\n𝑖(𝑜′\\n𝑖;𝜃′\\n𝑖)],\\n(6)\\nwhere a′𝐴\\n−𝑖and a′𝐸\\n−𝑖are, respectively, the biased actions computed us-\\ning Equation 5 with the target 𝑄-network, 𝑄𝜇′\\n𝑖. The target network\\nis designed to stabilize the learning of the 𝑄-network by slowly\\nchanging the parameters 𝜙′ of the target 𝑄-network [12].\\n4.3\\nLearning the Actor from the Biased Critic\\nThe biased critic 𝑄𝜇\\n𝑖is used to update the decentralized actor as a\\ndeterministic policy. The deterministic policy network 𝜇𝑖is updated\\nusing the gradient computed as\\n∇𝜃𝑖J (𝜃𝑖) =\\nEo,a∼D [∇𝜃𝑖𝜇𝑖(𝑜𝑖;𝜃𝑖)∇𝑎𝑖𝑄𝜇\\n𝑖(o,𝑎𝑖, a𝐴\\n−𝑖, a𝐸\\n−𝑖;𝜙𝑖)|𝑎𝑖=𝜇𝑖(𝑜𝑖;𝜃𝑖)],\\n(7)\\nwhere a𝐴\\n−𝑖and a𝐸\\n−𝑖are the biased joint actions of the ally and energy\\nagents, respectively, each of which is computed using Equation 5\\nwith the 𝑄-network, 𝑄𝜇\\n𝑖. We estimate the gradient reliably with the\\nbiased actions computed using the sampled joint actions (executed\\ntrue actions) from the experience reply buffer.\\nEach agent then updates the parameters of its own policy net-\\nwork using the computed policy gradient in Equation 7. Owing\\nto the biased action information, the trained policy is also biased\\nsuch that it is inevitably subjected to recommend an action to co-\\noperate and compete with other agents. Consequently, the biased\\npolicy introduces more active interactions among agents, and thus,\\nAlgorithm 1 Friend-or-Foe Multi-Agent Deep Deterministic Policy Gradient Algorithm for 𝑁agents\\n1: Initialize actor networks 𝜇, critic networks 𝑄, target networks 𝜇′ and 𝑄′, experience replay buffer D\\n2: for episode = 1 to 𝑀do\\n3:\\nInitialize a random process N for action exploration\\n4:\\nReceive initial observation o\\n5:\\nfor 𝑡= 1 to 𝑇do\\n6:\\nFor each agent 𝑖, select action 𝑎𝑖= 𝜇𝑖(𝑜𝑖;𝜃𝑖) + N𝑡\\n7:\\nExecute actions a and receive reward r = (𝑟1, . . . ,𝑟𝑁) and new observation o′\\n8:\\nStore (o, a, r, o′) in experience replay buffer D\\n9:\\no ←o′\\n10:\\nfor agent 𝑖= 1 to 𝑁do\\n11:\\nSample a random minibatch of B samples (o𝑏, a𝑏, r𝑏, o′𝑏) from D\\n12:\\nSet 𝑦𝑏\\n𝑖= 𝑟𝑏\\n𝑖+ 𝛾𝑄𝜇′\\n𝑖(o′𝑏,𝑎′\\n𝑖, a′𝐴\\n−𝑖, a′𝐸\\n−𝑖;𝜙′\\n𝑖)|𝑎′\\n𝑖=𝜇′\\n𝑖(𝑜′𝑏\\n𝑖;𝜃′\\n𝑖),\\n13:\\na′𝐴\\n−𝑖= a′𝐴\\n−𝑖+ 𝛿𝐴∇a′𝐴\\n−𝑖𝑄𝜇′\\n𝑖(o′𝑏,𝑎′\\n𝑖, a′𝐴\\n−𝑖, a′𝐸\\n−𝑖;𝜙′\\n𝑖)|𝑎′\\n𝑘=𝜇′\\n𝑘(𝑜′𝑏\\n𝑘;𝜃′\\n𝑘),\\n14:\\na′𝐸\\n−𝑖= a′𝐸\\n−𝑖−𝛿𝐸∇a′𝐸\\n−𝑖𝑄𝜇′\\n𝑖(o′𝑏,𝑎′\\n𝑖, a′𝐴\\n−𝑖, a′𝐸\\n−𝑖;𝜙′\\n𝑖)|𝑎′\\n𝑘=𝜇′\\n𝑘(𝑜′𝑏\\n𝑘;𝜃′\\n𝑘)\\n15:\\nUpdate critic by minimizing the loss: L(𝜙𝑖) = 1\\nB\\nÍ\\n𝑏(𝑄𝜇\\n𝑖(o𝑏, a𝑏;𝜙𝑖) −𝑦𝑏\\n𝑖)2\\n16:\\nUpdate actor using the sampled policy gradient: ∇𝜃𝑖J (𝜃𝑖) ≈1\\nB\\nÍ\\n𝑏∇𝜃𝑖𝜇𝑖(𝑜𝑏\\n𝑖;𝜃𝑖)∇𝑎𝑖𝑄𝜇\\n𝑖(o𝑏,𝑎𝑖, a𝐴\\n−𝑖, a𝐸\\n−𝑖;𝜙𝑖)|𝑎𝑖=𝜇𝑖(𝑜𝑏\\n𝑖;𝜃𝑖),\\n17:\\na𝐴\\n−𝑖= a𝑏𝐴\\n−𝑖+ 𝛿𝐴∇a𝑏𝐴\\n−𝑖𝑄𝜇\\n𝑖(o𝑏,𝑎𝑏\\n𝑖, a𝑏𝐴\\n−𝑖, a𝑏𝐸\\n−𝑖;𝜙𝑖),\\n18:\\na𝐸\\n−𝑖= a𝑏𝐸\\n−𝑖−𝛿𝐸∇a𝑏𝐸\\n−𝑖𝑄𝜇\\n𝑖(o𝑏,𝑎𝑏\\n𝑖, a𝑏𝐴\\n−𝑖, a𝑏𝐸\\n−𝑖;𝜙𝑖)\\n19:\\nend for\\n20:\\nUpdate target network parameters for each agent 𝑖: 𝜙′\\n𝑖←𝜏𝜙𝑖+ (1 −𝜏)𝜙′\\n𝑖and 𝜃′\\n𝑖←𝜏𝜃𝑖+ (1 −𝜏)𝜃′\\n𝑖\\n21:\\nend for\\n22: end for\\nenhances the MARL policy learning. In addition, if the critic’s gra-\\ndient (bias) in the biased action and actual action from the learned\\npolicy gradually become similar, the introduced bias then naturally\\ndecreases, and the training eventually becomes unbiased, as the\\nbiased and actual actions become the same. For completeness, we\\nprovide the F2DDPG algorithm in Algorithm 1.\\nIn some cases of training F2DDPG, the critic’s gradient (bias)\\ndominates the actual action in the biased action in learning the 𝑄-\\nnetwork and policy network. Thus, the magnitude of the gradient\\nis made equal to the magnitude of the actual action to prevent the\\nbiased action from being extremely biased and destabilizing the\\ntraining, as follows:\\n𝑔= ∇𝑎−𝑖𝑄𝜇\\n𝑖(o, a;𝜙𝑖),\\n𝑎−𝑖= 𝑎−𝑖± 𝛿∥𝑎−𝑖∥2\\n𝑔\\n∥𝑔∥2\\n.\\n(8)\\nThe trick of Equation 8 is utilized in Equation 5.\\n5\\nEXPERIMENTS\\nFigure 2 shows the environments (games) used to evaluate the per-\\nformances of the proposed and baseline algorithms. The environ-\\nments are those used in previous studies [8, 12] and those designed\\nto make MARL training more difficult. We assume that the agents\\nin the environments observe the relative positions and velocities of\\nall agents. In Figure 2, games (a) and (b) correspond to cooperative\\nenvironments with only agents in a cooperative relationship, and\\ngames (c) and (d) correspond to mixed cooperative-competitive\\nenvironments with both cooperative and competitive agents. In\\ngames (b) and (d), the agents need to communicate with other\\nagents depending on the purpose of the games.\\nFor the experiments in these games, we compare the performance\\nof F2DDPG to the following baseline algorithms:\\n• MADDPG [12] is the algorithm that learns the critics and\\nactors using only the actual action information in the CTDE\\nframework.\\n• M3DDPG [8] is the algorithm based on MADDPG. Instead\\nof using the actual action information, this algorithm uses\\nthe noisy actions of other agents to update the critics and\\nactors. In particular, this algorithm computes the adversarial\\nnoise for other agents’ actions such that the noisy actions\\ncollectively minimize the target agent’s critic. Owing to the\\nuse of adversarial noise, this algorithm is referred to as ro-\\nbust MARL. In the view of F2DDPG, this algorithm can be\\ninterpreted as one in which the competitive roles are infused,\\nas an information bias, to all the agents, regardless of their\\nroles (cooperative or competitive) with respect to the target\\nagent.\\n• All Plus is the algorithm with only cooperative biases em-\\nployed in the actions of other agents; other agents are as-\\nsumed to maximize the target agent’s critic, regardless of\\ntheir roles.\\n• Random Sign is the algorithm with random biases em-\\nployed in the actions of other agents. Other randomly se-\\nlected agents are assumed to maximize the target agent’s\\ncritic, while the remaining agents are assumed to minimize\\nthe target agent’s critic, regardless of their roles.\\nagent 1\\nlandmark \\nagent 2\\nagent 3\\na) Cooperative Navigation\\nlistener\\nspeaker\\n“blue”\\nlandmark \\nb) Cooperative Communication\\npredator\\nprey\\nc) Predator-Prey\\nlistener\\nspeaker\\nadversary\\no-o-o-o\\nred!\\ngreen?\\nd) Covert Communication\\nFigure 2: Illustrations of the experimental environments.\\n0\\n20\\n40\\n60\\n80\\n100 120\\n#Episodes(x1000)\\n-17\\n-16\\n-15\\n-14\\n-13\\n-12\\nReward(x10)\\nMADDPG\\nM3DDPG\\nRandom Sign\\nF2DDPG\\na) Cooperative Navigation\\n0\\n20\\n40\\n60\\n80\\n100 120\\n#Episodes(x1000)\\n-35\\n-30\\n-25\\n-20\\nReward\\nMADDPG\\nM3DDPG\\nRandom Sign\\nF2DDPG\\nb) Cooperative Communication\\n0\\n20\\n40\\n60\\n80\\n100 120\\n#Episodes(x1000)\\n20\\n60\\n100\\n140\\nReward\\nMADDPG\\nM3DDPG\\nAll Plus\\nRandom Sign\\nF2DDPG\\nc) Predator-Prey\\n0\\n20\\n40\\n60\\n80\\n100 120\\n#Episodes(x1000)\\n0\\n5\\n10\\n15\\n20\\nReward\\nMADDPG\\nM3DDPG\\nAll Plus\\nRandom Sign\\nF2DDPG\\nd) Covert Communication\\nFigure 3: Rewards of the red agents in the experimental environments.\\nWe can differentiate the proposed F2DDPG and other baseline al-\\ngorithms depending on the type of information bias infused into\\nother agents’ actions. While the baseline algorithms either do not\\nuse any information bias (MADDPG) or use a certain information\\nbias, regardless of the relationships among agents, F2DDPG is the\\nonly algorithm that aligns the information bias with the actual roles\\nof agents. Note that we exclude the All Plus algorithm from the\\nbaseline algorithms in cooperative environments, such as games (a)\\nand (b), because it has the same cooperative biases as F2DDPG for\\nall the cooperative agents. In this study, all the performances are\\nobtained by the trained policies with four different random seeds.\\n5.1\\nCooperative Navigation\\nCooperative navigation is a cooperative environment, as shown in\\nFigure 2 (a), in which three cooperative agents (red circles) must\\nreach three landmarks (blue crosses) without colliding with each\\nother, while covering all of the landmarks. Every episode starts\\nwith randomly initialized positions for the agents and landmarks.\\nThe agents are collectively rewarded based on the distance of the\\nnearest agent to each landmark and penalized for collisions with\\nother agents during navigation. Thus, each agent must cooperate\\nto occupy a distinct landmark without colliding with other agents.\\nAs shown in Figure 3 (a), F2DDPG outperforms other baseline al-\\ngorithms with faster training speed and higher converged rewards.\\nWe consider that the performance improvement of F2DDPG is a\\nresult of the optimal use of information bias corresponding to the\\nagents’ roles. F2DDPG constantly updates the critic and actor of\\neach agent using biased joint actions, which induces agents’ poli-\\ncies to recommend more coherently exploratory actions, especially\\ntoward cooperation. We believe that such coherent exploration\\nhelps the MARL policy learning more than random exploration.\\nIn contrast, M3DDPG exhibits slow training in inducing coop-\\neration among the three agents by learning only the adversarial\\naction information of other agents, although the agents are in a\\ncooperative relationship. Training with adversarial action informa-\\ntion may enable robust policy learning; however, it is believed to be\\nunhelpful in inducing cooperation among the agents. The Random\\nSign algorithm randomly injects cooperative or competitive biases\\nfor biased actions at every step of training, which leads to ran-\\ndom noise in training. This random noise allows the algorithm to\\ntrain faster than MADDPG and M3DDPG; however, the algorithm\\nexhibits slower training than F2DDPG.\\n5.2\\nCooperative Communication\\nCooperative communication is a cooperative environment, as shown\\nin Figure 2 (b), with two cooperative agents, a speaker and listener\\n(red circles), and three landmarks of differing colors. The listener\\nmust navigate to a landmark of a particular color. However, the\\nlistener does not know the landmark to which it must navigate,\\nwhile observing the relative position and color of the landmarks. In\\ntraining\\nFigure 5: Actual actions (red arrows) and biases (black arrows) for the centered predator as training proceeds.\\ncontrast, the speaker observes the correct color of the landmark to\\nwhich the listener must navigate, and broadcasts a message (commu-\\nnication vector) at each time step, which is observed by the listener.\\nFor each episode, the positions of the listener and landmarks are\\nrandomly initialized. The listener and speaker are rewarded based\\non listener’s distance to the correct landmark. Thus, the speaker\\nmust learn to generate a message that optimally guides the listener\\nto reach the correct landmark, and simultaneously, the listener must\\nlearn to decipher the message that is transmitted from the speaker\\nand navigate to the correct landmark.\\nAs shown in Figure 3 (b), F2DDPG outperforms other baseline\\nalgorithms with faster training speed and higher rewards. Addi-\\ntionally, M3DDPG exhibits better performance than MADDPG.\\nMoreover, the Random Sign algorithm learns faster than MADDPG,\\npossibly because of the enhanced exploration with random noises\\nadded to the observed actions.\\n5.3\\nPredator-Prey\\nPredator-prey is a mixed cooperative-competitive environment,\\nas shown in Figure 2 (c), in which the five predator agents (red\\ncircles) seek to capture the three prey agents (blue squares and\\ngreen diamond), which is called 5 vs. 3 predator-prey. If there are 𝑚\\npredators and 𝑛prey, it is denoted as𝑚vs. 𝑛predator-prey. Because\\nprey can move at a higher speed and have greater acceleration\\nthan predators, predators must cooperate to capture the prey. In\\nparticular, the green prey (green diamond) can move faster with\\ngreater acceleration than the blue prey (blue squares); the green\\nand blue prey are factors of 3 and 1.3 faster than the predators,\\nrespectively. The positions of the predators and prey are randomly\\ninitialized for every episode.\\nEach time the predators collide with (i.e., capture) the prey, the\\npredators are collectively rewarded, while the prey are penalized.\\nWhen the predators capture green prey, they are rewarded with a\\nfactor of 10 more than when they capture blue prey. The predators\\ncan capture prey multiple times during an episode. In the predator-\\nprey game, the prey are trained with MADDPG, while the predators\\nare trained with F2DDPG and other baseline algorithms.\\nAs shown in Figure 3 (c), F2DDPG outperforms other baseline\\nalgorithms with higher rewards. The reason why F2DDPG achieves\\nhigher rewards is hypothesized by investigating the reward lost\\nby the green prey shown in Figure 4 (a). This figure compares the\\nreward achieved by the green prey (trained with MADDPG) when\\nplaying against the predators trained with F2DDPG and baseline\\nalgorithms; when the reward of the prey is lower, it is more likely to\\nbe captured by the predators. As shown in the figure, the reward of\\n0\\n20\\n40\\n60\\n80\\n100 120\\n#Episodes(x1000)\\n-15\\n-10\\n-5\\nReward\\nMADDPG\\nM3DDPG\\nAll Plus\\nRandom Sign\\nF2DDPG\\na) Rewards of green prey\\n0\\n20\\n40\\n60\\n80\\n100 120\\n#Episodes(x1000)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nCosine Similarity\\nF2DDPG\\nb) Cosine similarity between\\nactual actions and biases\\nFigure 4: Results in 5 vs. 3 predator-prey.\\nthe green prey captured by the predators trained with F2DDPG is\\nlower than that of the prey captured by the predators trained with\\nother baseline algorithms. This indicates that the high rewards of\\nthe predators trained with F2DDPG (shown in Figure 3 (c)) are the\\nresults of capturing the green prey more frequently. Thus, it can be\\nconcluded that the predators trained with F2DDPG are more capable\\nof cooperating strategically to capture the most rewardable, but\\nfast, prey than the predators trained with other baseline algorithms.\\nWe also verify that the infused bias induces desirable behaviors\\nfrom the agents. Figure 4 (b) shows how the cosine similarity be-\\ntween the actual action a𝐴\\n−𝑖and bias ∇a𝐴\\n−𝑖𝑄𝜇\\n𝑖in the biased action in\\nEquation 5 varies as the F2DDPG training proceeds. The cosine sim-\\nilarity between a𝐴\\n−𝑖and ∇a𝐴\\n−𝑖𝑄𝜇\\n𝑖is calculated as\\n⟨𝑎𝐴\\n−𝑖,∇𝑎𝐴\\n−𝑖𝑄𝜇\\n𝑖⟩\\n∥𝑎𝐴\\n−𝑖∥2 ∥∇𝑎𝐴\\n−𝑖𝑄𝜇\\n𝑖∥2 for\\nall agents in 𝐴(𝑖), where ⟨·, ·⟩represents the inner product operator.\\nThe similarity is used to judge the similarity of directions of the\\nactual and biased actions. The similarity ranges from -1, indicating\\nexactly opposite, to 1, indicating exactly the same, and 0 indicating\\northogonality or decorrelation. As shown in Figure 4 (b), the similar-\\nity increases from 0 to close to 1 as the training proceeds, indicating\\nthat the actual and biased actions (desirable actions designed by the\\nbiases) become more similar as the training proceeds. Therefore,\\nthe agents eventually execute the actions designed by the biases,\\nand accordingly, the biases between the actual and biased actions\\nvanish.\\nTo consider a particular fixed state where a centered predator cap-\\ntures the prey, we investigate how the centered predator’s trained\\npolicy induces other agents’ (predators’) actions. In Figure 5, the\\nTable 1: Fraction between the number of successful episodes and the total number of testing episodes in predator-prey games.\\n3 vs. 1\\n5 vs. 3\\n7 vs. 3\\n𝑁𝑐≥1\\n𝑁𝑐≥3\\n𝑁𝑐≥1\\n𝑁𝑐≥3\\n𝑁𝑐≥1\\n𝑁𝑐≥3\\nMADDPG\\n2.75± 0.83\\n0.25± 0.43\\n14.50± 1.50\\n1.25± 1.09\\n56.25± 6.46\\n17.00± 6.09\\nM3DDPG\\n3.75± 0.43\\n0.25± 0.43\\n14.25± 0.43\\n1.25± 0.83\\n55.75± 6.17\\n13.25± 3.63\\nAll Plus\\n3.75± 1.09\\n0.25± 0.43\\n15.75± 2.28\\n1.00± 1.00\\n62.75± 7.39\\n14.24± 4.14\\nRandom Sign\\n1.50± 0.50\\n0.00± 0.00\\n18.75± 6.98\\n1.75± 0.83\\n62.50± 11.71\\n19.25± 6.37\\nF2DDPG\\n3.25± 0.43\\n0.25± 0.43\\n32.50± 11.39\\n4.50± 2.29\\n71.75± 7.52\\n28.75± 6.17\\nFigure 6: Actual actions (red arrows) and biases (black ar-\\nrows) for the predator capturing the green prey after the\\npolicies are learned with F2DDPG.\\nfigures on the left show the other agents’ biased joint action (black)\\ninduced at an early training stage, and those on the right show\\nhow the other agents’ actual joint action (red) concurrently and\\ncoherently alter as the training proceeds. Noticeable observations\\nare that (1) the other agents are jointly heading toward the prey,\\nwhich is a strategic movement to capture the prey, and (2) the actual\\n(red) and biased (black) actions become extremely similar, which\\ndemonstrates that the biases vanish as the training proceeds, and\\nthe agents actually behave as intended by the biases.\\nIn addition, we execute the policies trained with F2DDPG in\\nthe predator-prey game with a random state. Figure 6 shows four\\nsnapshots of the predator-prey game with a random state after the\\npolicies are trained with F2DDPG. In the figure, the predators tend\\nto gather toward the predator capturing the green prey and attempt\\nto capture it together. The differences between the actual (red) and\\nbiased (black) actions are negligible, meaning that the converged\\npolicies no longer carry the biases.\\nTable 1 compares the fraction between the number of successful\\nepisodes and the total number of testing episodes (100 episodes).\\nWe define two types of success: 𝑁𝑐≥1 is the case in which the\\npredators capture the green prey at least once and 𝑁𝑐≥3 is the\\ncase in which the predators capture the green prey at least three\\ntimes. To validate the scalability of the proposed algorithm, we\\ncompare these performance measures for F2DDPG and other base-\\nline algorithms for different sizes of predator-prey games. In 3 vs.\\n1, the performances are not significantly differentiated because the\\nnumber of predators is insufficient to capture the faster green prey,\\neven if the three predators cooperate. However, as the number of\\npredators increases, the predators capture the green prey through\\ncooperation, and F2DDPG outperforms other baseline algorithms\\nin 5 vs. 3, as shown in the table. In addition, F2DDPG outperforms\\nother baseline algorithms in 7 vs. 3. Thus, it can be concluded that\\nthe proposed method improves the MARL training, even in cases\\nwith many agents, by utilizing biased action information.\\n5.4\\nCovert Communication\\nCovert communication is a mixed cooperative-competitive envi-\\nronment, as shown in Figure 2 (d), with two cooperative agents, a\\nspeaker and listener (red circles), and an adversary (green circle).\\nThe speaker must encode a message as a communication vector us-\\ning a randomly generated key to output the communication vector.\\nThe listener must reconstruct the communication vector into the\\nmessage using the key. However, the adversary also observes the\\ncommunication vector and attempts to reconstruct the communica-\\ntion vector without the key. The speaker and listener are rewarded\\nbased on the listener’s reconstruction and penalized based on the\\nadversary’s reconstruction. The adversary is rewarded based on its\\nreconstruction. Therefore, the speaker must encrypt the message as\\nthe communication vector such that the adversary cannot decrypt\\nthe communication vector, and the listener must decrypt the com-\\nmunication vector as the message. In the covert communication,\\nthe adversary is trained with MADDPG for comparison, while the\\nspeaker and listener are trained with F2DDPG and other baseline\\nalgorithms.\\nAs shown in Figure 3 (d), F2DDPG outperforms other baseline\\nalgorithms with higher rewards. At the early stage of training,\\nM3DDPG, Random Sign, and F2DDPG outperform MADDPG and\\nAll Plus. However, as the training proceeds and the adversary be-\\ncomes intelligent, the rewards of M3DDPG and Random Sign de-\\ncrease, while F2DDPG still maintains high rewards.\\n6\\nCONCLUSIONS\\nWe proposed F2DDPG, an algorithm that boosts MARL training\\nusing biased action information of other agents based on a friend-\\nor-foe concept. Empirically, we demonstrated that F2DDPG outper-\\nforms existing algorithms in several mixed cooperative-competitive\\nenvironments. We also demonstrated that F2DDPG learns the agents’\\npolicies such that their actions become similar to the biased actions\\nand that the biases decrease as the learning proceeds.\\nREFERENCES\\n[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,\\nPeter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech\\nZaremba. 2017. Hindsight Experience Replay. In Advances in Neural Information\\nProcessing Systems. 5048–5058.\\n[2] Wendelin Böhmer, Tabish Rashid, and Shimon Whiteson. 2019. Exploration with\\nunreliable intrinsic reward in multi-agent reinforcement learning. arXiv preprint\\narXiv:1906.02138 (2019).\\n[3] G Hailu and G Sommer. 1999. On amount and quality of bias in reinforcement\\nlearning. In IEEE SMC’99 Conference Proceedings. 1999 IEEE International Confer-\\nence on Systems, Man, and Cybernetics, Vol. 2. IEEE, 728–733.\\n[4] Junling Hu, Michael P Wellman, et al. 1998. Multiagent reinforcement learn-\\ning: theoretical framework and an algorithm.. In Proceedings of the Fifteenth\\nInternational Conference on Machine Learning, Vol. 98. Citeseer, 242–250.\\n[5] Shariq Iqbal and Fei Sha. 2019. Actor-attention-critic for multi-agent reinforce-\\nment learning. In International Conference on Machine Learning, Vol. 97. PMLR,\\n2961–2970.\\n[6] Shariq Iqbal and Fei Sha. 2019. Coordinated Exploration via Intrinsic Rewards\\nfor Multi-Agent Reinforcement Learning. arXiv preprint arXiv:1905.12127 (2019).\\n[7] Martin Lauer and Martin Riedmiller. 2000. An algorithm for distributed rein-\\nforcement learning in cooperative multi-agent systems. In In Proceedings of the\\nSeventeenth International Conference on Machine Learning. Citeseer.\\n[8] Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. 2019.\\nRobust multi-agent reinforcement learning via minimax deep deterministic policy\\ngradient. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33.\\n4213–4220.\\n[9] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,\\nYuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control with deep\\nreinforcement learning. International Conference on Learning Representations\\n(2016).\\n[10] Michael L Littman. 1994. Markov games as a framework for multi-agent rein-\\nforcement learning. In Machine learning proceedings 1994. Elsevier, 157–163.\\n[11] Michael L. Littman. 2001. Friend-or-Foe Q-Learning in General-Sum Games. In\\nProceedings of the Eighteenth International Conference on Machine Learning, Vol. 1.\\nMorgan Kaufmann Publishers Inc., 322–328.\\n[12] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor\\nMordatch. 2017. Multi-agent actor-critic for mixed cooperative-competitive\\nenvironments. In Advances in Neural Information Processing Systems. 6379–6390.\\n[13] Paulo Rauber, Avinash Ummadisingu, Filipe Mutz, and Juergen Schmidhuber. 2019.\\nHindsight policy gradients. International Conference on Learning Representations\\n(2019).\\n[14] Julien Roy, Paul Barde, Félix G Harvey, Derek Nowrouzezahrai, and Christopher\\nPal. 2019. Promoting Coordination through Policy Regularization in Multi-Agent\\nDeep Reinforcement Learning. arXiv preprint arXiv:1908.02269 (2019).\\n[15] Heechang Ryu, Hayong Shin, and Jinkyoo Park. 2020. Multi-Agent Actor-Critic\\nwith Hierarchical Graph Attention Network. In Proceedings of the AAAI Confer-\\nence on Artificial Intelligence, Vol. 34. 7236–7243.\\n[16] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Mar-\\ntin Riedmiller. 2014. Deterministic Policy Gradient Algorithms. In International\\nConference on Machine Learning, Vol. 32. PMLR, 387–395.\\n[17] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-\\nduction. MIT press.\\n[18] Tonghan Wang, Jianhao Wang, Yi Wu, and Chongjie Zhang. 2020. Influence-Based\\nMulti-Agent Exploration. International Conference on Learning Representations\\n(2020).\\nSUPPLEMENTARY MATERIAL\\nDETAILS ABOUT ENVIRONMENTS\\nTable 2: Classification of the experimental environments.\\nEnvironment\\nCooperative?\\nMixed?\\nCommunication?\\nCooperative Navi.\\n✓\\nCooperative Comm.\\n✓\\n✓\\nPredator-Prey\\n✓\\nCovert Comm.\\n✓\\n✓\\nTable 2 categorizes the experimental environments into a coop-\\nerative environment and a mixed cooperative-competitive environ-\\nment and indicates whether the environments need communication\\nbetween agents. We assume that the agents in the environments\\nobserve the relative positions and velocities of all agents. The po-\\nsitions of the agents and landmarks in all the environments are\\nrandomly initialized within [−1, 1]2 for every episode.\\nCooperative Navigation\\nCooperative navigation (𝑁= 3) has three agents and three land-\\nmarks.\\nCooperative Communication\\nCooperative communication (𝑁= 2) has two agents, a speaker and\\nlistener, and three landmarks. The speaker outputs a communica-\\ntion vector, which is a three-sized tensor, at each timestep, and the\\nlistener observes the communication vector.\\nPredator-Prey\\nPredator-prey (𝑁= 4, 8, 10) has predator agents and prey agents.\\nThe environment imposes a penalty on prey when the prey go\\nbeyond the boundary of the environment.\\nCovert Communication\\nCovert communication (𝑁= 3) has three agents, a speaker, lis-\\ntener, and adversary. The speaker observes a message vector and\\na key vector, which are randomly generated, and outputs a com-\\nmunication vector. The listener observes the key vector and the\\ncommunication vector. The adversary observes only the communi-\\ncation vector. The message, key, and communication vectors are\\nfour-sized tensors.\\nHYPER-PARAMETERS FOR EXPERIMENTS\\nWe use 120,000 training episodes with 25 timesteps (total 3 million\\ntimesteps) for training the proposed and other baseline algorithms\\nin all the environments. All codes used in the experiments will be\\nreleased.\\nHyper-Parameters of F2DDPG\\nThe hyper-parameters of F2DDPG used in the experiments are\\nsummarized in Table 3. The output layer of the policy network of\\nF2DDPG for the environments provides the action as a five-sized\\ntensor for hold, right, left, up, and down.\\nThe hyper-parameters in the table are also used for the All Plus\\nand Random Sign algorithms in the experiments. For MADDPG\\nand M3DDPG, the hyper-parameters reported to have the highest\\nperformance in previous studies are used in the experiments and\\nsimilar to the hyper-parameters of F2DDPG.\\nTable 3: Hyper-parameters of F2DDPG.\\nF2DDPG Hyper-Parameter\\n# Policy network MLP units\\n(64, 64)\\n# 𝑄-network MLP units\\n(64, 64)\\nNetwork parameter initialization\\nXavier uniform\\nNonlinear activation\\nReLU\\nPolicy network learning rate\\n10−2\\n𝑄-network learning rate\\n10−2\\n𝜏for updating target networks\\n10−2\\n𝛾\\n0.95\\nReplay buffer size\\n106\\nMini-batch size\\n1024\\nOptimizer\\nAdam\\n𝛿𝐴\\n10−5\\n𝛿𝐸\\n10−3\\n'),\n",
              " Document(metadata={'Published': '2023-10-18', 'Title': 'Fact-based Agent modeling for Multi-Agent Reinforcement Learning', 'Authors': 'Baofu Fang, Caiming Zheng, Hao Wang', 'Summary': 'In multi-agent systems, agents need to interact and collaborate with other\\nagents in environments. Agent modeling is crucial to facilitate agent\\ninteractions and make adaptive cooperation strategies. However, it is\\nchallenging for agents to model the beliefs, behaviors, and intentions of other\\nagents in non-stationary environment where all agent policies are learned\\nsimultaneously. In addition, the existing methods realize agent modeling\\nthrough behavior cloning which assume that the local information of other\\nagents can be accessed during execution or training. However, this assumption\\nis infeasible in unknown scenarios characterized by unknown agents, such as\\ncompetition teams, unreliable communication and federated learning due to\\nprivacy concerns. To eliminate this assumption and achieve agent modeling in\\nunknown scenarios, Fact-based Agent modeling (FAM) method is proposed in which\\nfact-based belief inference (FBI) network models other agents in partially\\nobservable environment only based on its local information. The reward and\\nobservation obtained by agents after taking actions are called facts, and FAM\\nuses facts as reconstruction target to learn the policy representation of other\\nagents through a variational autoencoder. We evaluate FAM on various Multiagent\\nParticle Environment (MPE) and compare the results with several\\nstate-of-the-art MARL algorithms. Experimental results show that compared with\\nbaseline methods, FAM can effectively improve the efficiency of agent policy\\nlearning by making adaptive cooperation strategies in multi-agent reinforcement\\nlearning tasks, while achieving higher returns in complex\\ncompetitive-cooperative mixed scenarios.', 'entry_id': 'http://arxiv.org/abs/2310.12290v1', 'published_first_time': '2023-10-18', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.AI', 'categories': ['cs.AI'], 'links': ['http://arxiv.org/abs/2310.12290v1', 'http://arxiv.org/pdf/2310.12290v1']}, page_content='JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n1\\nFact-based Agent modeling for Multi-Agent\\nReinforcement Learning\\nBaofu Fang, Caiming Zheng and Hao Wang\\nAbstract—In multi-agent systems, agents need to interact and\\ncollaborate with other agents in environments. Agent modeling\\nis crucial to facilitate agent interactions and make adaptive\\ncooperation strategies. However, it is challenging for agents to\\nmodel the beliefs, behaviors, and intentions of other agents\\nin non-stationary environment where all agent policies are\\nlearned simultaneously. In addition, the existing methods realize\\nagent modeling through behavior cloning which assume that\\nthe local information of other agents can be accessed during\\nexecution or training. However, this assumption is infeasible\\nin unknown scenarios characterized by unknown agents, such\\nas competition teams, unreliable communication and federated\\nlearning due to privacy concerns. To eliminate this assumption\\nand achieve agent modeling in unknown scenarios, Fact-based\\nAgent modeling (FAM) method is proposed in which fact-based\\nbelief inference (FBI) network models other agents in partially\\nobservable environment only based on its local information. The\\nreward and observation obtained by agents after taking actions\\nare called facts, and FAM uses facts as reconstruction target\\nto learn the policy representation of other agents through a\\nvariational autoencoder. We evaluate FAM on various Multiagent\\nParticle Environment (MPE) and compare the results with\\nseveral state-of-the-art MARL algorithms. Experimental results\\nshow that compared with baseline methods, FAM can effectively\\nimprove the efficiency of agent policy learning by making adap-\\ntive cooperation strategies in multi-agent reinforcement learning\\ntasks, while achieving higher returns in complex competitive-\\ncooperative mixed scenarios.\\nIndex Terms—Multi-agent Reinforcement Learning, Multi-\\nagent Systems, Agent Modeling.\\nI. INTRODUCTION\\nR\\nEINFORCEMENT Learning (RL) has achieved rapid\\nprogress in cooperative and competitive multi-agent\\ngames, such as OpenAI Five[1] and AlphaStar[2]. In multi-\\nagent environments, agents must interact with each other,\\nwhere the interaction relationship includes competition and\\ncooperation. Due to the policy of all agents are simulta-\\nneously learning, it affects the state transitions and reward\\nfunctions experienced by an individual agent[3]. From the\\nperspective of a single agent, interacting with other agents\\nwhose policies change makes the environment non-stationary.\\nTherefore, other agents cannot be simply treated as part of the\\nenvironment. Agent modeling promotes the agent to adjust its\\nown policy to adapt to the policy changes of other agents by\\nManuscript received 4, June 2023. This work was supported by the Univer-\\nsity Synergy Innovation Program of Anhui Province (Grant No.GXXT-2022-\\n055), Open Fund of Key Laboratory of Flight Techniques and Flight Safety,\\nCAAC (Grant No.FZ2022KF09), and the R&D Program of Key Laboratory\\nof Flight Techniques and Flight Safety, CAAC(Grant No.FZ2022ZZ02).\\nThe authors are with the School of Computer Science and Information\\nEngineering, Hefei University of Technology, Hefei, 230601, China (e-mail:\\nfangbf@hfut.edu.cn; 2502282770@@qq.com; jsjwangh@hfut.edu.cn).\\nexplicitly modeling the beliefs, behaviors and intentions of\\nother agents[4]. Since the agent learns in the same partially\\nobservable environment while other agents whose strategies\\nare complex, diverse, and time-varying. Therefore, modeling\\nother agents in non-stationary environments is a major chal-\\nlenge for multi-agent reinforcement learning.\\nTraditional agent modeling assumes that agents can access\\nthe local information of other agents during execution and\\ntraining[4], [5], [6], including the local observations and\\nactions taken by other agents. However, this assumption often\\ndoes not hold in many multi-agent scenarios. In practical,\\nagents may have limited visibility of their surrounding en-\\nvironment and communication with competing agents may be\\nprohibited, while communication between cooperating agents\\nis often unreliable[7], for example in federated learning tasks.\\nIn such situations, agents must inference and make decisions\\nbased on their local information. To weaken this assumption,\\nLIAM[8] and SMA2C[9] utilize the local information of\\nagents, including their own observations, actions, and rewards,\\nto infer the representations of other agents in a recurrent man-\\nner. These methods relax the assumption of traditional agent\\nmodeling by allowing access to the local information of other\\nagents only during the training stage. However, in unknown\\nscenarios, agents may also be prohibited from accessing the\\ninformation of other agents during both execution and training\\nstages. It is infeasible for behavior cloning-based approaches\\nto explicitly minimize the difference between an agent’s policy\\nmodel and the true policy. Therefore, the agent modeling in\\nthis case requires the agent to rely on its own local information,\\nthat is, it does not access the local information of other agents\\nduring the training and execution phases.\\nConsider a simple real-world scenario as shown in Figure\\n1, where the fruits collection task requires three people (Alice,\\nBob and Carol) to collect three fruits include apples, oranges,\\nand pears with the shortest time. In order to achieve this\\ntask without communication, each person should go through\\nfour stages: 1).Preliminary decision making, 2).Observing and\\ninferring, 3).Interacting, 4).Repeating steps 2) and 3) to avoid\\nconflits and achieve collaborative consensus until the fruits\\ncollection task is complete. In this process, each person needs\\nto start from the recent observation to infer other person policy\\nrepresentations to help itself make adaptive decisions. At the\\nsame time, the facts that happened after decision making are\\nused to verify the inference result. In multi-agent systems, the\\nrewards and observation received by the agent after performing\\nthe action imply rich information about the actions of other\\nagents at the same moment.\\nBased on this viewpoint, We propose Fact-based Agent\\narXiv:2310.12290v1  [cs.AI]  18 Oct 2023\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n2\\nA\\nB\\nC\\nA\\nB\\nC\\nAlice wants to collect \\noranges, if so, then she \\nwill be closer to \\noranges!\\nBob wants to collect \\noranges, but I am \\nfaster than him!\\nA\\nB\\nC\\nOrange is closest to \\nme and I will collect \\nit.\\nApple is closest to me \\nand I will collect it.\\nOrange is closest \\nto me and I will \\ncollect it.\\nA\\nB\\nC\\nAlice is closer to the \\norange, I am sure she \\nwants to collect them, but \\nshe is faster than me. \\nAlso, Carol wants to \\ncollect apple, so I am \\ngoing to collect pear.\\nStep 1. Preliminary decision making\\nStep 2. Observing and inferring\\nStep 3. Interacting\\nStep 4. Repeating Steps 2). and 3)\\nFig. 1. Fruit collection example. Everyone needs to cooperate to collect three konds of fruits which are apple, orange, and pear in shortest possible time. The\\nwhole process of fruit collection can be divided into 4 stages: 1).Preliminary decision making: Observing the surrounding environment, independently select\\nfruits, and making effective decisions. 2).Observing and inferring: Observing information related others and inferring their behavioral intentions. 3).Interacting:\\nMaking adaptive decisions based on the inferred results to interact with environment and other agents and verifying the previous inferences through the fact\\nof consequences after decisions. 4).Repeating steps 2). and 3). to avoid cooperative goal conflicts and achieve collaborative consensus until the fruit collection\\ntask is completed.\\nModeling (FAM) for multi-agent learning, which eliminates\\nthe assumption of accessing local information of other agents\\nduring execution and training phases. We build fact-based\\nbelief inference (FBI) network to model other agents based\\non own local information which is a variational autoEncoder\\n(VAE) that has the advantage of being able to compensate\\nfor the information difference between the execution and\\ntraining phases. The difference from the existing work is that\\nSMA2C[9] adopts the method of behavior cloning during\\ntraining phase. Howerver, in this paper, the reward signal\\ncontaining global information and the observation of local\\ninformation received by the agent after performing the action\\nare used as the reconstruction target. The proposed FAM\\nis suitable for non-stationary and partially observable envi-\\nronments. In addition, the complexity of agent modeling of\\nSMA2C[9] and LIAM[8] is O(N) while FAM is O(1) that\\nis independent of the number of agents. FAM is also more\\nsuitable for unknown scearios. The main contributions of this\\narticle as follows.\\n1) In order to elimate the assumption of accessing the local\\ninformation of other agents for agent modeling, fact-\\nbased belief inference (FBI) network is proposed, which\\nmodels other agents based on own local information\\nusing variational autoencoder.\\n2) Combining FBI with Actor-Critc, Fact-based Agent\\nModeling (FAM) is proposed for multi-agent learning,\\nwhich learns adaptive collaboration strategies by con-\\nsidering the policies of other agents. It can effectively\\napplicable to partially observable environments.\\n3) Extensive experimental was conducted to verify the\\neffectiveness and feasibility of the proposed FAM, and\\nanalyze the information encoded by FBI.\\nThe remainder of this article is organised as follows. Section\\nII describes the background of deep reinforcement learning\\nand variational autoencoder. Section III reviews the related\\nwork about multi-agent reinforcement learning and agent\\nmodeling. Section IV presents the proposed FBI and FAM.\\nSection V describes the detailed results and analysis of the\\nexperiments. Ultimately, conclusions are provided in Section\\nVI.\\nII. NOTATION AND BACKGROUND\\nThe fully cooperative multi-agent task can be modeled\\nas a decentralized partially observable Markov decision\\nprocess(Dec-POMDP)[10]. It is represented by the tuple\\nG =< I, S, U, P, r, Z, O, γ >, where I = {1, 2, ..., n} is a\\nfinite set of agents, and n represents the number of agents.\\ns ∈S describes the global state of the environment. At each\\ntimestep t, each agent a ∈I receives an observation oa\\nt ∈O\\nthrough the observation function Z(s, a) : S × I →O and\\nselects an action ua\\nt , forming a joint action ut ∈U. After\\nexecuting the actions, the agents receive rewards signal rt,\\nwhere all agents share the same reward function r(s, u) :\\nS × U →R, and transition to the next state according to\\ntransition probability function P(s′|s, u) : S ×U ×S →[0, 1].\\nThe action-observation history of each agent is denoted as\\nτ a\\n∈T\\n≡(O × U)∗, and the policy π(ua\\nt |τ a\\n1:t; θa) :\\nT × U\\n→[0, 1] is based on its own action-observation\\nhistory, with policy parameters θa. The goal of Dec-POMDP\\nis to learn a joint policy π = (π1, ..., πn) that maximizes\\nthe team cumulative discounted return Rt = P∞\\nl=0 γlrt+l,\\nwhere γ ∈[0, 1) is the discount factor. The joint action-\\nvalue function of the joint policy π is denoted as Qπ(st, ut) =\\nEst+1:∞,ut+1:∞[Rt|st, ut], the joint state-value function is de-\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n3\\nnoted as V π(st) = Est+1:∞, ut:∞[Rt|st], and the advantage\\nfunction is denoted as Aπ(st, ut) = Qπ(st, ut) −V π(st).\\nPolicy Gradient: Vanilla Policy Gradient (REINFORCE) is\\nan on-policy algorithm that directly uses a parameterized\\nmodel[11] to approximate the policy π(ut|st; θ). REINFORCE\\ndoes not require a separate behavior policy because πθ(ut|st)\\nnaturally explores and exploits the environment. The policy\\nparameters θ are updated at each step by increasing the log-\\nlikelihood of the chosen actions with respect to the sampled\\ntrajectory return Rt. The gradient update direction is given by:\\ng = Et [∇θ log π(ut|st; θ)Rt]\\n(1)\\nThe baseline bt(st)[12] is subtracted from the return to\\nreduce the variance of the estimated return while remaining\\nunbiased, and the gradient update direction becomes:\\ng = Et [∇θ log π(ut|st; θ)(Rt −bt(st))]\\n(2)\\nThis allows for more stable learning and potentially faster\\nconvergence. The baseline can be a value function estimate\\nor a learned function that approximates the expected return at\\nstate st.\\nAdvantage Actor-Critic (A2C): A2C is an on-policy Actor-\\nCritic method that utilizes parallel environments to break the\\ncorrelation between consecutive samples. It introduces a state\\nvalue function estimator Vw(st) to approximate the state value\\nE [Rt|st], and used as a baseline to reduce the variance of\\nsampling returns to improve policy gradient updates. Since\\nQ(st, ut) is an approximate estimate of Rt, A(st, ut) = Rt −\\nV (st) is expressed as the advantage of action at under state st,\\nthen the direction of A2C policy gradient update is as follows,\\ng = Et [∇θ log π(ut|st; θ)(A(st, ut))]\\n(3)\\nBy using the advantage function A(st, ut), A2C facilitates\\nmore stable and efficient learning. And the loss function for\\nthe state value function is given by:\\nLa2c(ω) = E(st,ut,rt+1,st+1)∼B(Rt −Vω(st))2\\n(4)\\nwhere B denotes the sampled batch trajectory.\\nProximal Policy Optimization (PPO): PPO is an Actor-Critic\\nalgorithm whose core idea is to achieve stable training by\\nlimiting the distance between old and new policies. The PPO\\nalgorithm uses a loss function called \"clipped surrogate objec-\\ntive\" to control the step size of policy update, thus achieving\\nstability in training without slowing down the training speed.\\nUnlike A2C, PPO employs a technique called importance\\nsampling, which allows multiple gradient descent updates to\\nbe performed using the same batch of trajectories. The loss\\nfunction of actor for PPO as follows:\\nLppo(θ) = Eτ∼B [min(rt(θ), clip(ratiot(θ), 1 −ϵ, 1 + ϵ))At]\\n(5)\\nwhere ratiot(θ) =\\nπθ(ut|st)\\nπθold(ut|st) represents the ratio between the\\nnew and old policies, and ϵ is a hyperparameter used to control\\nthe difference between the new and old policies. Compared to\\nA2C, PPO has a higher sample efficiency.\\nVariational Autoencoder (VAE): VAE is a generative model\\nused to approximate the true posterior distribution p(z|x),\\nwhere the dataset samples X = {xi}N\\ni=1 are generated from an\\nunknown parameterized generative distribution p(x|z; θ) with\\nthe latent variable z being unobserved. The prior distribution\\nof the latent variable z is assumed to be a Gaussian distribution\\np(z) = N(z; 0, I) with mean 0 and variance 1. The goal of\\nVAE is to learn a variational distribution q(z|x; ϕ) parameter-\\nized by ϕ to approximate the true posterior distribution p(z|x),\\nwhere q(z|x; ϕ) = N(z; µ, σ, ϕ) is a Gaussian distribution\\nwith mean µ and variance σ.\\nVariational inference uses the KL divergence as a distance\\nmeasure function to minimize the distance between the ap-\\nproximate posterior distribution q(z|x; ϕ) and the true poste-\\nrior distribution p(z|x) , the Evidence Lower Bound(ELBO)\\nas follows:\\nlog p(x) ≥ELBO(ψ, ϕ|x)\\n= Ez∼q(z|x;ϕ) [log p(x|z; ψ) −DKL(q(z|x; ϕ)||p(z))]\\n(6)\\nwhere DKL represents the Kullback-Leibler (KL) divergence.\\nThe first term on the right-hand side of the equation is the\\nreconstruction loss, which measures the quality of the gener-\\nated samples. The second term is a regularization term, which\\nis used to constrain the distribution of the latent variables.\\nHiggins et al.[13] proposes β-VAE, where the parameter β ≥0\\nis used to balance the reconstruction loss and the regularization\\nterm. The overall optimization objective of the β-VAE is as\\nfollows:\\nLvae(ϕ, ψ) = Ez∼qϕ [log p(x|z; ψ) −βDKL(q(z|x; ϕ)||p(z))]\\n(7)\\nIII. RELATED WORK\\nMulti-agent system (MAS) consists of multiple agents in-\\nteracting in the shared environment to accomplish tasks. For\\ncomplex tasks, Multi-Agent Reinforcement Learning (MARL)\\nenables agents to learn effective policies through interaction\\nwith the environment. One of the main challenges in MARL\\nis the inherent non-stationarity of the environment where all\\nagents learn simultaneousl. Since the policies of other agents\\nare unknown, it is unstable for agents to learn policies if\\nthey are considered part of the environment. To address this\\nchallenge, one approach is to adopt the Centralized Training\\nwith Decentralized Execution (CTDE) framework, where a\\ncentralized Critic is used to approximate joint action value or\\nstate-action value to guide the policy learning of individual\\nagents. The value-based methods include QMIX[14], OW-\\nQIX[15], and TransfQMIX[16] and the Actor-Critic-based\\nmethods such as MADDPG[17] and MAAC[18].\\nAnother\\napproach\\nto\\naddress\\nthe\\nchallenge\\nof\\nnon-\\nstationarity in MARL is agent modeling, which mitigates\\nthe effect of non-stationarity by incorporating information\\nabout other agents’ beliefs, behaviors, and intentions. Many\\nstudies on agent modeling rely on predicting the actions or\\ngoals of other agents during training. He et al.[5] proposed a\\nbehavior cloning-based agent modeling approach that uses a\\nneural network to predict the actions executed by other agents\\nbased on their observations. Hernandez-Leal et al.[19] treated\\nlearning other non-learning agents’ policy as an auxiliary\\ntask and simplified it to a standard single-agent reinforcement\\nlearning problem. Similarly, Georgios et al.[8] used an encoder\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n4\\nto construct representations of other agents based on their\\nlocal information in a recurrent manner, while the decoder\\nreconstructed the observations and actions of other agents.\\nAnother method[9] applied a variational autoencoder for agent\\nmodeling, where the encoder generates a high-dimensional\\ncontinuous distribution as a representation of the other agent’s\\npolicy, and the decoder is trained by reconstructing the agent’s\\nactions. Both of these methods allow access to other agents’\\nlocal information during the training or execution.\\nIn terms of considering simultaneously learning opponents,\\nFoerster et al.[20] proposed LOLA, which incorporates the\\ninfluence of an agent’s policy on the parameter updates of\\nother agents’ policies. Al-Shedivat et al.[21] introduced Mate-\\nPG, a meta-policy gradient-based method that leverages the\\ntrajectories of other agents in multiple meta-gradient steps to\\nconstruct a policy that benefits from updating other agents.\\nKim et al.[22] proposed an extension to the existing method\\ncalled Meta-MAPG. They introduced an additional term that\\ncaptures the influence of an agent’s current policy on the\\nfuture policies of other agents, similar to LOLA. These meta-\\nlearning-based methods require the trajectory distributions to\\nmatch between training and testing, implicitly assuming that\\nall other agents use the same learning algorithm.\\nUnlike existing work, we consider a more complex and\\ngeneral setting where the policies of other agents are learned\\nsimultaneously with the agent’s own policy. Furthermore, there\\nis partial observability in multi-agent environment, and the\\nagents are not allowed to access the local information of other\\nagents to achieve agent modeling during the execution and\\ntraining.\\nIV. METHODS\\nIn this section, we introduce a Fact-based Agent Modeling\\n(FAM) Algorithm as shown in Figure 2, which completely\\neliminates the assumption that traditional agent modeling\\nallow access to other agents local information during the\\ntraining or execution phases. Firstly, we provide the structure\\nand details of fact-based belief inference module (FBI). Fur-\\nthermore, we present the optimization objective and training\\nprocedure for FAM.\\nA. Fact-based Belief Inference\\nTo enable an agent to interact with other agents and learn\\nadaptive policy, it needs to infer the current policies of the\\ninteracting agents. Fact-based Belief Inference (FBI) elim-\\ninates the assumption that agents can access other agents’\\nlocal information during training or execution. This module\\nextracts policy representation denoted as zi of other agent\\nfrom the interaction trajectories of agent i, including triplets\\nof observations, actions, and rewards triplets. Policy repre-\\nsentations are learned from facts acquired by the agent after\\nperforming actions. The extracted representation zi denotes\\nagent i’s beliefs about other agents, i.e., estimates of their\\npolicy. This introduces uncertainty of other agents’ policies\\ninto agent i’s policy πi(ai\\nt|τ i\\n1:t, zi).\\nAssuming the joint policy of other agents are unobservable\\nvariables zi in the latent space Zi for agent i, and the latent\\nFig. 2. The architecture of Fact-based Agent Modeling (FAM). During the\\nexecution phase, the encoder module of FBI utilizes the local information of\\nthe agent to extract representations of other agents’ policies, which are then\\nused for the agent’s decision-making process. On the other hand, during the\\ntraining phase, the decoder module reconstructs the facts and simultaneously\\ntrains both the encoder and decoder. The agent makes decisions based on its\\nown action-observation trajectories and the representations of other agents’\\npolicies.\\nvariable zi\\nt at time step t contains the policy representation of\\nother agents except agent i itself. To learn the latent informa-\\ntion, FAM employs FBI which is a variational encoder-decoder\\narchitecture[23] as shown in Fig.2. Agent i uses an encoder\\nq consists of a recurrent neural network and a fully con-\\nnected neural network to infer representations of other agents’\\npolicies by local information including the observation-action-\\nreward triplet. It outputs µi and log(σi) which is the param-\\neters of variational distribution. And the policy representation\\nof other agents are sampled from the variational distribution.\\nSpecifically, the goal of the encoder is to approximate the\\ntrue posterior p(zi) using a variational distribution obtained\\nsolely from local information. FBI constructs a decoder p to\\nlearn the representation of other agents by reconstructing facts\\nconditioned on policy representations zi ∼N(µi, σi) and the\\nagent’s observation-action. The encoder is parameterized by\\nψi, and the state prediction and reward prediction function\\nin the decoder are parameterized by ϕi and φi, respectively.\\nFBI jointly optimizes ψi, ϕi, and φi to maximize the evidence\\nlower bound(ELBO) of the sampled trajectory τ i\\n1:t, as follows,\\nELBO(ψi, ϕi, φi|τ i\\n1:t) = Ezi\\nt∼qψi)\\n\\x02\\nJrecon\\n\\x03\\n−βDKL(q(µi\\nt, σi\\nt|τ i\\n1:t; ψi)||p(zi))\\n(8)\\nwhere Jrecon = log p(ˆoi\\nt+1, ˆri\\nt+1|oi\\nt, ai\\nt, zi\\nt; ϕi, φi) is recon-\\nstruction loss. The ELBO is related to the state tran-\\nsition and reward functions of each agent i. τ i\\n1:t\\n=\\n(oi\\n1, ai\\n1, ri\\n2, oi\\n2, ..., ri\\nt, oi\\nt) represents the local trajectory infor-\\nmation of agent i up to time step t. p(zi) is the prior\\ndistribution of the latent variable zi, we assume the latent vari-\\nable follows a standard Gaussian distribution zi ∼N(0, I).\\nDKL is the Kullback-Leibler(KL) divergence which measures\\nthe distance between the approximate posterior distribution q\\nand the true posterior distribution p. The hyperparameter β\\nis used for controling the importance of the regularization\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n5\\nAlgorithm 1 Training Procedure for FAM Algorithm\\nInitialize: θi, ωi, Φi = {ψi, ϕi, φi}, β, α1, α2, B, E;\\nOutput: θ∗\\ni , ω∗\\ni , Φ∗\\ni = {ψ∗\\ni , ϕ∗\\ni , φ∗\\ni };\\n1: for each episode j do\\n2:\\nInitial observation o0 ←{oi\\n0}n\\n1\\n3:\\nfor each timestep t do\\n4:\\nGet observations ot = {oi\\nt}n\\ni=1;\\n5:\\nCompute opponent embeddings zt\\n=\\n{zi\\nt\\n∼\\nqψi}n\\ni=1;\\n6:\\nSample action ui\\nt ∼πi(·|oi\\nt, zi\\nt; θi);\\n7:\\nPerform joint actions ut = (u1\\nt, ..., un\\nt ) and reveive\\njoint reward rt+1 and next observations\\n8:\\not+1 = {oi\\nt+1}n\\ni=1;\\n9:\\nAdd transition {ot, zt, ut, rt+1, ot+1} →Bj,t\\n10:\\nend for\\n11:\\nif |B| = batch size then\\n12:\\nfor each epoch e < E do\\n13:\\nfor each agent i do\\n14:\\nωi ←ωi −α1∇ωiLcritic (Eq.10)\\n15:\\nθi ←θi −α1∇θiLactor (Eq.11)\\n16:\\nΦi ←Φi −α2∇ΦiLfbi (Eq. 9)\\n17:\\nB ←∅;\\n18:\\nend for\\n19:\\nend for\\n20:\\nSoft update parameters θ\\n′\\ni, ω\\n′\\ni, Φ\\n′\\ni with θi, ωi, Φi\\n21:\\nend if\\n22: end for\\nterm KL divergence[13]. Minimizing the loss is equivalent\\nto maximizing the ELBO, and thus the loss function of the\\nFBI as follows:\\nLfbi(ψi, ϕi, φi) = Ezi\\nt∼qψi\\n\\x02\\nJrecon_obs + Jrecon_rew\\n\\x03\\n−β 1\\n2\\nd\\nX\\nj=1\\n(1 + log(σ2\\nt,i,j) −µ2\\nt,i,j −σ2\\nt,i,j)\\n(9)\\nwhere Jrecon_obs = (pobs(ˆoi\\nt+1|oi\\nt, ai\\nt, zi\\nt; ϕi) −oi\\nt+1)2 and\\nJrecon_rew = (pr(ˆri\\nt+1|oi\\nt, ai\\nt, zi\\nt; φi)−ri\\nt+1)2 are the observa-\\ntion prediction and reward prediction loss functions, respec-\\ntively. d represents the dimensionality of the latent variable\\nzi. The intuitive interpretation of this loss function is that\\nthe decoder p is optimized to reconstruct the facts that occur\\nafter taking an action, specifically received in next time step’s\\nobservation and reward.\\nB. Training Algorithm of FAM\\nIn this section, we describe the training process of FAM.\\nThe sampled trajectory of the agent, along with the latent\\nvariable zi, is used to optimize the RL policy. We consider\\nan augmented policy space T augi = Oi × Ui × Zi for agent\\ni, where Oi and Ui are the original observation and action\\nspaces of the Dec-POMDP, and Zi represents the belief space\\nof agent i on other agents. Specifically, the belief refers to the\\npolicy representations of other agnets. Compared to the policy\\nspace without considering other agents’ policy representations\\nT augi = Oi × Ui, the augmented policy space T i allows\\nfor different responses to different zi ∈Zi. This enables\\nadaptive behavior based on the policy representations of other\\nagents. We assume that all agents learn simultaneously in\\nthe same environment. Due to the delayed nature of other\\nagents’ policy changes, which affect the agent’s belief about\\ntheir policy representations, we train the FAM using on-policy\\nalgorithm. In our experiments, we use the PPO algorithm to\\noptimize the agent’s policy. The inputs to the Actor and Critic\\nare the local action-observation trajectories and the inferenced\\npolicy representation zi. Additionally, the RL loss does not\\nbackpropagate into FBI. To encourage exploration, we also\\nuse policy entropy[24]. Given a batch of sampled trajectories\\nB, the loss for the Critic network is defined as follows:\\nLcritic(ωi) = EB\\n\\x02\\n(ri\\nt+1 + γV (oi\\nt+1, zi\\nt+1; ω−\\ni )\\n−V (oi\\nt, zi\\nt; ωi))\\n\\x03\\n(10)\\nwhere V is the target network, z indicates that the loss of\\nthe Critic network does not backpropagate through z, and\\nω−represents the parameters of the Critic target network,\\nwhich are also not updated through gradient backpropagation.\\nAdditionally, the loss for the Actor network is defined as\\nfollows:\\nLactor(θi) = E(oi\\nt,ai\\nt,zi\\nt,ri\\nt+1,oi\\nt+1)∼B\\n\\x02\\nmin(ratiot(θi), clip(ratiot(θi), 1 −ϵ, 1 + ϵ))Ai\\nt\\n\\x03\\n(11)\\nwhere ratiot =\\nπ(at|ot,zt;θi)\\nπ(at|ot,zt;θi\\nold) is the action probability ratio\\nof the old and new policies, and clip modifies the surrogate\\nobjective by restricting the probability ratio.\\nNow we can define a training objective to learn the ap-\\nproximate posterior distribution q as well as reward prediction\\nfunction pr and observation prediction function pobs, as fol-\\nlows:\\nLfam(θ, ω, ψ, ϕ, φ)=\\nLactor + Lcritic + Lfbi\\n(12)\\nV. EXPERIMENTAL RESULTS AND ANALYSIS\\nIn this section, we aim to investigate the following aspects\\nof FAM: 1). whether FBI improve learning efficiency, promote\\ncooperative among multi agents, and learn adaptive collabo-\\nration strategies? 2). how FAM enables collaboration through\\nadaptive strategies, 3). does FBI encode the policies of other\\nagents, and beyong that, what other important information is\\nencoded. To answer these three questions, we conduct exper-\\nments in two multi-agent particle environments: Cooperative\\nNavigation (CN) and Predator-Prey (PP), as introduced by\\nLowe et al.[17]. The implementation is based on the epymarl\\nframework proposed by Papoudakis et al.[25].\\nA. Experimental Setup\\n1) Environments: We introduce the two environments used\\nfor our proposed FAM and baselines.\\nCooperative Navigation: In this task, N agents need to\\ncooperatively occupy L landmarks in an environment with\\npartial observability. The team reward function rdistance =\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n6\\n−PN\\ni=1 minj dis(Landmarki, Agentj) is based on the neg-\\native sum of distances between the landmarks and their closest\\nagents. Additionally, we discourage collisions between agents,\\nand a collision penalty rcollision = −1 is applied. Each agent\\nneeds to observe the movements of other agents to infer their\\nbehavior or goals, select a suitable landmark to occupy while\\navoiding conflicts and collisions with other agents. We conduct\\nexperiments with N = 5 agents and L = 5 landmarks, where\\neach agent can only observe the relative positions of the two\\nclosest agents and three closest landmarks. The agents have 5\\navailable actions, and the maximum episode length is set to\\n25.\\nPredator-Prey (PP): In this task, N predators try to capture\\nM preys in an environment with partial observability. The\\npreys follow predefined policies to move away from the closest\\npredators with a speed of 7, while the predators are only 5.\\nSince preys have a faster movement speed than predators,\\nindividual predators cannot capture preys on their own. The\\nteam reward for predators is the negative sum of distances\\nbetween the preys and their closest predators. A collision\\npenalty rcollision = −1 is also applied to the predators. In PP\\ntask, each predator can observe the relative positions of the two\\nclosest predators and three closest preys. The predators have 5\\navailable actions. Unlike in Cooperative Navigation, the preys\\nexhibit highly dynamic movement. Therefore, predators need\\nto infer the behavior of other predators and target preys to\\ncooperate with other predators and capture the desired preys.\\nWe conduct experiments with N = 7 predators and M = 3\\npreys.\\nIn IA2C, each agent treats other agents as a part of the\\nenvironment and utilizes the A2C algorithm to learn and\\noptimize its Actor network with 3 for approximating the policy\\nπθi and Critic network with 4 for approximating the value\\nfunction Vωi in distributed multi-agent systems[24].\\n2) Implementation Detials: Next, we will introduce the im-\\nplementation details of the proposed FBI and FAM. The FAM\\nconsists of actor network, critic network and FBI network\\nparameterized by θi, ωi and Φi = {ψi, ϕi, φi}, respectively.\\nThe FBI includes an RNN-based encoder and two MLP-based\\ndecoders. During execution, RNN-based encoder takes the\\nlocal observation-action-reward triplet (oi\\nt, ui\\nt−1, ri\\nt) as input\\nthrough a 1-layer fully connected neural network(FC) followed\\nby a ReLU activation function to extract features, which are\\nthen fed into a GRU recurrent network to capture temporal\\ndependencies. Finally, a 1-layer FC outputs the variational\\ndistribution parameters µi\\nt and log σi\\nt that approximate the true\\nposterior distribution. The sampled with dimension d = 5 is\\nthe policy representation of other agents for agent decision-\\nmaking. During training, MLP-based decoder takes the local\\nobservation-action-policy representation (oi\\nt, ui\\nt, zi\\nt) as input\\nthrough 3-layer FC and followed by ReLU activation functions\\nto output the predictions of rewards and observations obtained\\nafter executing action. It is important to note that the last fully\\nconnected layer does not require a ReLU activation function.\\nThe RNN-based encoder and MLP-based decoder are trained\\nby computing the prediction loss and regularization term.\\nTABLE I\\nPERFORMANCE EVALUATION COMPARISON OF FAM AGAINST BASELINES\\nIN CN.\\nMethods\\nAvg. Ret.\\nAvg. Rew.\\nAvg. Occ.\\nAvg. Dist.\\nIPPO\\n−26.2 ± 8.2\\n−0.52 ± 0.3\\n4.56 ± 0.4\\n0.45 ± 0.2\\nIA2C\\n−34.6 ± 10.1\\n−1.08 ± 0.5\\n3.48 ± 1.0\\n1.02 ± 0.4\\nMAA2C\\n−34.8 ± 9.4\\n−1.09 ± 0.4\\n3.06 ± 1.1\\n1.06 ± 0.4\\nMAPPO\\n−26.1 ± 7.8\\n−0.52 ± 0.3\\n4.57 ± 0.5\\n0.48 ± 0.3\\nFAM\\n−25.5 ± 8.8\\n−0.50 ± 0.3\\n4.45 ± 0.5\\n0.45 ± 0.3\\nLIAM\\n−35.2 ± 9.6\\n−1.06 ± 0.4\\n3.31 ± 1.0\\n1.05 ± 0.4\\nSMA2C\\n−35.3 ± 9.6\\n−1.12 ± 0.5\\n3.26 ± 1.1\\n1.05 ± 0.4\\nB. Main Results\\nWe compare FAM with several baselines to verify the\\neffectiveness and feasibility of the proposed method. Figure\\n3a) and Figure 3b) show the average episode return curves\\nand average landmarks occupied curve of FAM compared with\\nother baseline algorithms in CN during training. Figure 3c)\\nshows the average episode return curves of various methods\\ntrained in PP with training duration of 1e7 steps. And Table\\nI presents the performance metrics of various algorithms\\nevaluating 100 episodes in CN, including average episode\\nreturn(Avg. Ret.), average reward at the final timestep(Avg.\\nRew.), average occupied landmarks(Avg. Occ.), and the sum\\nof the average distances of all landmarks from the nearest\\nagent(Avg. Dist.).\\nFrom Fig. 3 a). and 3 b)., we can see that the proposed FAM\\nachieves higher learning efficiency than all other baselines\\nfrom 1e6 training steps, as well as faster convergence, and\\nslightly outperforms IPPO and MAPPO from 4e6 training\\nsteps. The main reason is that after all agents learn a certain\\nstrategy, considering the strategies of other agents will help\\nthe agents learn adaptive cooperation strategies. And they\\nstruggle with partial observability of the environment after 4e6\\nsteps. Meanwhile, the shaded areas of IPPO and MAPPO are\\nlarger compared with FAM, which indicates that the cooper-\\native strategy without considering other agents is less robust.\\nAmong the four evaluation performances as shown in Table I,\\nFAM has reached the best compared with all other baselines.\\nThe average episode return curves of IA2C, MAA2C, LIAM,\\nand SMA2C overlap and show slow learning. And in Figure\\n3b), IA2C occupies more landmarks. Similar results are shown\\nin Table I which indicates that IA2C performs slightly better\\nthan SMA2C, LIAM, and MAA2C. The possible reason is\\nthat the low sample efficiency of the A2C methods and agents\\nonly need to consider other agents at certain critical moments\\nin CN, which makes the performance of independent similar to\\nCTDE methods. Additionally, LIAM and SMA2C do not show\\nsuperiority. This could be due to the low sample efficiency\\nof the A2C methods and the high randomness in directly\\nmodeling the actions of other agents in a non-stationary\\nenvironment.\\nIt can be seen from Figure 3c) that IA2C performs the\\nworst and the proposed FAM outperforms all other baseline\\nby considering the strategies of other agents. The possible\\nreason is that preys are highly dynamic and move faster\\nthan predators, which requires closer cooperation between\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n7\\na)\\nb)\\nc)\\nFig. 3. The training results of the proposed FAM and the baseline method in CN and PP environments. a)The episode return curves during training in CN.\\nb)The occupied landmarks curves during training in CN. c)The episode return curves in PP. The solid line is the mean of the training results of 5 random\\nseeds, and the shaded area is the 25%-75% quartile.\\nFig. 4. Comparing the ablation results of FAM and its belief inference network\\nin the Predator-Prey environment.\\npredators and adaptation to other predators’ strategy to capture\\npreys cooperatively. However, independent IA2C is difficult\\nto achieve. Moreover, MAA2C employs a centralized critic\\nthat utilizes global information to guide the policy learning\\nof agents and achieves better performance than IA2C. But the\\nperformance of LIAM and SMA2C falls behind the centralized\\ncritic. The possible reason is that the centralized critic provides\\nmore effective information for guiding policy learning under\\npartially observability. IPPO and MAPPO exhibit similar av-\\nerage episode return curves which can be attributed to the\\neffectiveness of the PPO algorithm. This finding aligns with\\nprevious studies[26] and [25].\\nIn general, in the experimental settings of CN andPP,\\nwe found it interesting that the agent modeling method can\\nquickly and effectively improve the learning efficiency and\\nlearn adaptive collaboration strategies to obtain higher rewards\\nafter other agents learn a certain strategy. The good news is\\nthat it didn’t hinder the agent’s strategy learning before this.\\nC. Ablation Results\\nThe fact-based belief inference (FBI) network in FAM\\nutilizes a variational autoencoder (VAE) architecture, whose\\ninput and reconstruction target are the local information of\\nthe agent. To investigate the impact of the following factors\\nin FBI network: 1) decoder input and 2) reconstruction tar-\\ngets, we conducted ablation experiments in the Predator-Prey\\nenvironment.\\nThe ablation experiments included the following varia-\\ntions: i). FAM_wo_in_oa, where the decoder input only\\nconsisted of the representation of other agents’ policies. ii).\\nFAM_wo_rec_obs, where the decoder only reconstructed re-\\nwards. iii). FAM_wo_rec_rew, where the decoder only re-\\nconstructed observations. The average episode return curves\\nof the Predator-Prey task are plotted in Figure 4. These\\nablation experiments aim to examine the contributions of\\ndifferent components in FBI. By comparing the performance\\nof these variations with the FAM, we can gain insights into\\nthe importance of decoder input and the reconstruction of\\nobservations and rewards.\\nThe decoder in FBI network takes the agent’s local observa-\\ntions, actions and inferred representation as input. We denote\\nthe agent’s local observations and actions as \"oa\". To compare\\nthe impact of the decoder input, we denote the decoder input\\nas FAM_wo_in_oa only for other agent policy representations\\nz and keep the reconstruction fact unchanged. As shown\\nin Figure 4, FAM is generally better than FAM_wo_in_oa.\\nAlthough FAM and FAM_wo_in_oa have similar performance\\nin the early stage, both can effectively improve the efficiency\\nof policy learning. But when struggling partial observability,\\nFAM has an advantage. The possible reason is that the decoder\\ndesign of FBI is better for the agent to understand the\\ndynamics of the environment.\\nTo compare the impact of the decoder reconstruction targets,\\nwe compare the training results of FAM_wo_rec_obs and\\nFAM_wo_rec_rew on PP task, as shown in Figure 4. It\\ncan be seen that the performance of FAM_wo_rec_rew is\\nbetter, but it is weaker than FAM_wo_rec_obs in the early\\nstage. The possible reason is that the team reward helps\\nto extract other agents’ policy representations, but this is a\\nspurious reward signal, which may also hinder the learning of\\nother agents’ policies. The observation can directly represent\\nthe movement information of the surrounding agents, which\\nprovides rich verification information for each individual.\\nMoreover, FAM has the advantages of FAM_wo_rec_rew and\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n8\\nFAM\\nFAM\\nFAM\\nIPPO\\nIPPO\\nIPPO\\nMAPPO\\nMAPPO\\nMAPPO\\nFig. 5. The navigation trajectories and immediate reward curves of three evaluation scenarios of FAM, IPPO and MAPPO under Cooperative Navigation task.\\nFAM_wo_rec_obs by reconstructing observations and rewards,\\nand has the best performance both in the early stage of training\\nand in the stage of struggling partial observability.\\nOverall, the decoder reconstructs observations and rewards\\nby inputting its own observations, actions, and policy repre-\\nsentations to better help the agent understand the dynamics of\\nthe environment.\\nD. Strategy Analysis\\nIn order to understand the representation of other agents\\nlearned by FAM, we conducted evaluations and compared and\\nanalyzed the cooperative strategies of FAM agents with IPPO\\nand MAPPO agents in the Cooperative Navigation. Figure\\n5 shows the evaluated navigation trajectories and immediate\\nreward curves of the three methods of FAM, IPPO and\\nMAPPO in CN.\\nIt can be seen that all FAM agents can move near the\\nlandmark or successfully occupy the landmark, and the im-\\nmediate rewards are the best. However, the IPPO agent and\\nthe MAPPO agent can not due to the goal conflict between the\\nagents. We summarize the cooperative skills learned by FAM\\nagents, including i). Communicate without communication\\n(CWC), ii). Avoiding goal conflict and competition (AGCC),\\niii). Giving up the small to keep the big (GSKB).\\nCommunicate without communication: Once the strategy\\nof other agents is found to change, it will change its own\\nstrategy in time to meet the needs of the task. As shown in\\nFigure 5a), both agent 1 and agent 2 want to occupy landmark\\n1, and there is a goal conflict. However, agent 2 has an\\nadvantage in distance when occupying landmark 1, so agent 1\\nhas to change the landmark to 2. At this time, Agent 4 wants\\nto occupy landmark 4, but it infers that the strategy of Agent\\n1 is occupy landmark 2 which is farther away, which makes\\nit take longer to complete the task. There, agent 4 changes\\nthe navigation landmark to 2 and agent 1 changes its own\\nnavigation landmark to 4 for shorest complete time. However,\\nAgent 1 and Agent 4 of IPPO and MAPPO have landmark\\nconflict and competition, as shown in Figure 5 b) and c).\\nAvoiding goal conflict and competition: When it is found\\nthat the goals of other agents conflict with itself, it will\\nchange its own goals according to the actual situation to\\navoid competition. As shown in Figure 5d), there is a goal\\nconflict between agent 1 and agent 3 bacause they want to\\noccupy landmark. However, agent 1 occupys landmark 4 is\\nmore advantageous, because agent 3 is closer to unoccupied\\nlandmark 2. Therefore, agent 3 changes the landmark to avoid\\ngoal conflict can promote overall cooperation and complete\\nthe task faster. In contrast, IPPO agent and MAPPO agent\\nfailed to achieve this. As shown in Figure 5e), there is a\\ngoal conflict and goal competition between Agent 1 and\\nAgent 5. Agent 5 cannot observe landmark 2 due to partial\\nobservability. Therefore, agent 5 cannot effectively occupy\\nlandmark. A possible effective method is that agent 2 changes\\nits own landmark to 2. This collaborative strategy is reflected\\nby MAPPO, as shown in Figure 5f). But it takes longer to\\ncomplete the task.\\nGive up the small to keep the big: When there is a goal\\nconflict or the goal can be occupied by a more advantageous\\nagent, the agent will change its own goal to shorten the time\\nto complete the task. This skill is demonstrated in Figure 5g),\\nwhere Agent 2, although closer to landmark 1 and 4, chooses\\nthe farther landmark 5 to facilitate faster landmark occupation\\nby Agent 1 and Agent 5. In contrast, IPPO and MAPPO agents\\nfail to achieve the goal of conflict-free. In Figure 5h) and 5i),\\nthere is a landmark conflict and competition between agents.\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n9\\na)\\nb)\\nc)\\nd)\\ne)\\nf)\\nFig. 6. a) and d) are trajectory plots of the agents in the CN. b) and c) are t-SNE projections of the embedding vectors learned by agent 1 and agent 2 in\\ntrajectory a). And e) and f) are t-SNE projections of the embedding vectors learned by agent 1 and agent 2 in trajectory b), respectively. We use a maximum\\ntime step of 25 to visualize the embedding vector of each time step, and use circles with different colors for each cluster.\\nE. Encoder Evaluation\\nAfter evaluating the advantages of the FBI module in FAM\\nfor adaptive strategy learning, we analyzed the embedding\\nvectors learned by the RNN-based encoder in FBI to gain\\ndeeper insights into the proposed method. We addressed the\\nquestion of whether FAM encode the strategies about other\\nagents. We visualized the embedding vectors of the RNN-\\nencoder and analyzed the learned embeddings. To facilitate\\nthe understanding of the encoded embeddings of other agents,\\nwe conducted experiments in CN with N = 2 agents, L = 2\\nlandmarks. Figure 6 visualizes the evaluation results.\\nWe observed that points corresponding to adjacent time\\nsteps tend to form clusters, and each cluster is correlated with\\nthe agent’s motion state. From Figure 6a), we can see that\\nAgent 2 moves towards the bottom right direction, approaching\\nthe landmark and hovering around it. These two processes\\nform two distinct clusters in Figure 6b). And Agent 1’s\\nmotion consists of four steps, with the first three steps marked\\nby arrows and the final step involving hovering around the\\nlandmark. These four steps correspond to the four distinct\\nclusters formed in Figure 6c). Based on these observations, we\\nhypothesize that different clusters represent different aspects\\nof the modeled agent’s motion, including the magnitude and\\ndirection of motion.\\nAdditionally, we speculate that the encoder embedding\\nvectors also include the positional information of the modeled\\nagents. From the trajectory in Fig .6d), it can be seen that the\\nagent 2 moves upward first, then moves upward to the right and\\ngradually approaches landmark and hovers around it. These\\nthree processes also form three different clusters in Fig .6e).\\nCompared to the red cluster, the blue cluster is closer to the\\ngreen cluster. We can see that there are only three clustering\\nresults in Fig.6f), and its motion process has four steps. The\\npossible reason is that the last movement close to the landmark\\nis close to the position hovering near the landmark, and they\\nare classified into the same cluster. In addition, in the same\\ncluster, the distance between points at adjacent moments is\\nsmall, while the distance between points at multiple moments\\nis large.\\nVI. CONCLUSION\\nWe have proposed a Fact-based Agent Modeling (FAM) for\\nmulti-agent learning that build FBI to reconstruct facts for\\nachieving agent modeling without accessing local information\\nof other agents. By considering the policy of other agents\\nduring decision-making, FAM outperforms baseline methods\\nand achieving higher rewards in complex mixed scenarios.\\nExtensive experimental is conducted to verify the effectiveness\\nand feasibility of the proposed FAM and analyse the encoder\\ninformation of FBI.\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n10\\nREFERENCES\\n[1] C. Berner, G. Brockman, B. Chan, V. Cheung, P. D˛ebiak, C. Dennison,\\nD. Farhi, Q. Fischer, S. Hashme, C. Hesse et al., “Dota 2 with large\\nscale deep reinforcement learning,” arXiv preprint arXiv:1912.06680,\\n2019.\\n[2] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,\\nJ. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev et al., “Grand-\\nmaster level in starcraft ii using multi-agent reinforcement learning,”\\nNature, vol. 575, no. 7782, pp. 350–354, 2019.\\n[3] X. Yu, J. Jiang, W. Zhang, H. Jiang, and Z. Lu, “Model-based opponent\\nmodeling,” Advances in Neural Information Processing Systems, vol. 35,\\npp. 28 208–28 221, 2022.\\n[4] S. V. Albrecht and P. Stone, “Autonomous agents modelling other agents:\\nA comprehensive survey and open problems,” Artificial Intelligence, vol.\\n258, pp. 66–95, 2018.\\n[5] H. He, J. Boyd-Graber, K. Kwok, and H. Daumé III, “Opponent\\nmodeling in deep reinforcement learning,” in International conference\\non machine learning.\\nPMLR, 2016, pp. 1804–1813.\\n[6] Z.-W. Hong, S.-Y. Su, T.-Y. Shann, Y.-H. Chang, and C.-Y. Lee, “A\\ndeep policy inference q-network for multi-agent systems,” in Proceed-\\nings of the 17th International Conference on Autonomous Agents and\\nMultiAgent Systems, 2018, pp. 1388–1396.\\n[7] B. Chen, “Local information based attentional opponent modelling in\\nmulti-agent reinforcement learning,” 2022.\\n[8] G. Papoudakis, F. Christianos, and S. Albrecht, “Agent modelling under\\npartial observability for deep reinforcement learning,” Advances in\\nNeural Information Processing Systems, vol. 34, pp. 19 210–19 222,\\n2021.\\n[9] G.\\nPapoudakis\\nand\\nS.\\nAlbrecht,\\n“Variational\\nautoencoders\\nfor\\nopponent modeling in multi-agent systems,” Feb. 2020, aAAI 2020\\nWorkshop on Reinforcement Learning in Games, AAAI20-RLG ;\\nConference date: 08-02-2020 Through 08-02-2020. [Online]. Available:\\nhttp://aaai-rlg.mlanctot.info/index.html\\n[10] F. A. Oliehoek and C. Amato, A concise introduction to decentralized\\nPOMDPs.\\nSpringer, 2016.\\n[11] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradi-\\nent methods for reinforcement learning with function approximation,”\\nAdvances in neural information processing systems, vol. 12, 1999.\\n[12] R. J. Williams, “Simple statistical gradient-following algorithms for\\nconnectionist reinforcement learning,” Reinforcement learning, pp. 5–\\n32, 1992.\\n[13] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick,\\nS. Mohamed, and A. Lerchner, “beta-vae: Learning basic visual concepts\\nwith a constrained variational framework,” in International conference\\non learning representations, 2017.\\n[14] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster,\\nand S. Whiteson, “Monotonic value function factorisation for deep\\nmulti-agent reinforcement learning,” The Journal of Machine Learning\\nResearch, vol. 21, no. 1, pp. 7234–7284, 2020.\\n[15] T. Rashid, G. Farquhar, B. Peng, and S. Whiteson, “Weighted qmix:\\nExpanding monotonic value function factorisation for deep multi-agent\\nreinforcement learning,” Advances in neural information processing\\nsystems, vol. 33, pp. 10 199–10 210, 2020.\\n[16] M. Gallici, M. Martin, and I. Masmitja, “Transfqmix: Transformers for\\nleveraging the graph structure of multi-agent reinforcement learning\\nproblems,” arXiv preprint arXiv:2301.05334, 2023.\\n[17] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, “Multi-\\nagent actor-critic for mixed cooperative-competitive environments,” in\\nProceedings of the 31st International Conference on Neural Information\\nProcessing Systems, 2017, pp. 6382–6393.\\n[18] S. Iqbal and F. Sha, “Actor-attention-critic for multi-agent reinforcement\\nlearning,”\\nin\\nProceedings\\nof\\nthe\\n36th\\nInternational\\nConference\\non\\nMachine\\nLearning,\\nser.\\nProceedings\\nof\\nMachine\\nLearning\\nResearch,\\nK.\\nChaudhuri\\nand\\nR.\\nSalakhutdinov,\\nEds.,\\nvol.\\n97.\\nPMLR,\\n09–15\\nJun\\n2019,\\npp.\\n2961–2970.\\n[Online].\\nAvailable:\\nhttps://proceedings.mlr.press/v97/iqbal19a.html\\n[19] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, “Agent modeling\\nas auxiliary task for deep reinforcement learning,” in Proceedings of\\nthe AAAI conference on artificial intelligence and interactive digital\\nentertainment, vol. 15, no. 1, 2019, pp. 31–37.\\n[20] J. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and\\nI. Mordatch, “Learning with opponent-learning awareness,” in Proceed-\\nings of the 17th International Conference on Autonomous Agents and\\nMultiAgent Systems, 2018, pp. 122–130.\\n[21] M. Al-Shedivat, T. Bansal, Y. Burda, I. Sutskever, I. Mordatch, and\\nP. Abbeel, “Continuous adaptation via meta-learning in nonstationary\\nand competitive environments,” in International Conference on Learning\\nRepresentations, 2018.\\n[22] D. K. Kim, M. Liu, M. D. Riemer, C. Sun, M. Abdulhai, G. Habibi,\\nS. Lopez-Cot, G. Tesauro, and J. How, “A policy gradient algorithm for\\nlearning to learn in multiagent reinforcement learning,” in International\\nConference on Machine Learning.\\nPMLR, 2021, pp. 5541–5550.\\n[23] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv\\npreprint arXiv:1312.6114, 2013.\\n[24] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-\\nforcement learning,” in International conference on machine learning.\\nPMLR, 2016, pp. 1928–1937.\\n[25] G. Papoudakis, F. Christianos, L. Schäfer, and S. V. Albrecht, “Bench-\\nmarking multi-agent deep reinforcement learning algorithms in cooper-\\native tasks,” arXiv preprint arXiv:2006.07869, 2020.\\n[26] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and\\nY. WU, “The surprising effectiveness of ppo in cooperative multi-agent\\ngames,” in Advances in Neural Information Processing Systems,\\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\\nEds., vol. 35.\\nCurran Associates, Inc., 2022, pp. 24 611–24 624.\\n[Online].\\nAvailable:\\nhttps://proceedings.neurips.cc/paper_files/paper/\\n2022/file/9c1535a02f0ce079433344e14d910597-Paper-Datasets_and_\\nBenchmarks.pdf\\nBaofu Fang received Ph.D. degree in Computer\\nApplication Technology from Harbin Institute of\\nTechnology, China in 2013. He joined Department of\\nComputer Science and Technology, School of Com-\\nputer Science and Information Engineering, Hefei\\nUniversity of Technology in 2000, and An Asso-\\nciate Professor in 2010, and Master’s Supervisor in\\n2011. His current research interests include multi\\nrobot/agent system, emotion/self-interest robot and\\nmachine learning. He is the Technology Chair of\\nAnhui Robot Competition, Member of Standing\\nCommittee of China Association of Artificial Intelligence (CAAI) Young\\nCommittee, Member of Standing Committee of China Association of Artificial\\nIntelligence (CAAI) Robot and Culture Committee.\\nCaiming Zheng received the B.Eng degree in com-\\nputer science and technology from Ningbo Univer-\\nsity of Technology, China in 2021. He is currently\\npursuing a M.S. degree in computer science and\\ntechnology at the School of Computer Science and\\nInformation Engineering, Hefei University of Tech-\\nnology, China. His research interests include multi-\\nagnet systems, reinforcement learning and multi-\\nagent reinforcement learning.\\nHao Wang received the B.Eng degree from Shang-\\nhai Jiao Tong University in 1984, and received M.S.\\ndegree and Ph.D. degree from Hefei University of\\nTechnology in 1989 and 1997, respectively. He is\\ncurrently a Professor and Doctoral Supervisor with\\nthe School of Computer Science and Information\\nEngineering, Hefei University of Technology. His\\nresearch interests include intelligent computing the-\\nory and software, distributed intelligent systems,\\ncomplex system theory and modeling, etc.\\n'),\n",
              " Document(metadata={'Published': '2015-12-07', 'Title': 'Software Agents Interaction Algorithms in Virtual Learning Environment', 'Authors': 'Zahi A. M. Abu Sarhan', 'Summary': 'This paper highlights the multi-agent learning virtual environment and agents\\ncommunication algorithms. The researcher proposed three algorithms required\\nsoftware agents interaction in virtual learning information system environment.\\nThe first proposed algorithm is agents interaction localization algorithm, the\\nsecond one is the dynamic agents distribution algorithm (load distribution\\nalgorithm), and the third model is Agent communication algorithm based on using\\nagents intermediaries. The main objectives of these algorithms are to reduce\\nthe response time for any agents changes in virtual learning environment (VLE)\\nby increasing the information exchange intensity between software agents and\\nreduce the overall network load, and to improve the communication between\\nmobile agents in distributed information system to support effectiveness.\\nFinally the paper describe the algorithms of information exchange between\\nmobile agents in VLE based on the expansion of the address structure and the\\nuse of an agent, intermediary agents, matchmaking agents, brokers and their\\nentrepreneurial functions', 'entry_id': 'http://arxiv.org/abs/1403.5734v2', 'published_first_time': '2014-03-23', 'comment': None, 'journal_ref': 'The World of Computer Science and Information Technology Journal\\n  (WSCIT). 2014, Volume 4, Issue 2. pp. 18.25', 'doi': None, 'primary_category': 'cs.MA', 'categories': ['cs.MA', 'cs.CY'], 'links': ['http://arxiv.org/abs/1403.5734v2', 'http://arxiv.org/pdf/1403.5734v2']}, page_content=\"World of Computer Science and Information Technology Journal (WCSIT)  \\nISSN: 2221-0741 \\nVol. 4, No. 2, 18-25, 2014 \\n18 \\nSoftware Agents Interaction Algorithms in Virtual \\nLearning Environment \\n \\nDr. Zahi A.M. Abu Sarhan \\nFaculty of Information Technology: Software Engineering Department \\nApplied Science Private University \\nAmman, Jordan \\n \\n \\n \\n \\nAbstract— This paper highlights the multi-agent learning virtual environment and agent's communication algorithms. The \\nresearcher proposed three algorithms required software agent’s interaction in virtual learning information system environment. The \\nfirst proposed algorithm is agents interaction localization algorithm, the second one is the dynamic agents distribution algorithm \\n(load distribution algorithm), and the third model is Agent communication algorithm based on using agents intermediaries. The \\nmain objectives of these algorithms are to reduce the response time for any agents’ changes in virtual learning environment (VLE) \\nby increasing the information exchange intensity between software agents and reduce the overall network load, and to improve the \\ncommunication between mobile agents in distributed information system to support effectiveness. Finally the paper describe the \\nalgorithms of information exchange between mobile agents in VLE based on the expansion of the address structure and the use of \\nan agent, intermediary agents, matchmaking agents ,brokers and their entrepreneurial functions.  \\n \\nKeywords- multi-agent system; agent interaction models; Intermediary Agents; Virtual Learning Environment; Brokering Agents; \\nMatchmaking Agents. \\n \\n \\nI. \\nINTRODUCTION \\nAt present, most of information system in various fields \\nbased on software agents technology, these systems can \\nimplemented as distributed information systems as centralized \\ninformation systems, and one of these complex systems is \\ndistance or electronic learning systems. For the e-learning \\nsystems wildly used centralized technology development and \\ninformation systems operation. The situation is still likes the \\n90's of the last century, when there were a large number of \\nscattered, technically and semantically mixed databases, used \\nlocally. Different kinds of attempts to create a central system \\nfor utilization all information system resources to achieve \\nvaluable functionality and flexibility have yielded little success. \\nThe wildly used today's Internet technologies and software \\nagents techniques with web access, solves the problem \\npartially. In this way, there is a distributed access to \\ninformation and the information resources and by the way it \\nkeeps it to be centralized, which makes them not always \\nsufficiently relevant, high redundancy, technological, and \\nsemantic diversity [1]. \\nIn this context, it is urgent the task to think about creating a \\nfully decentralized peer information systems to support e-\\nlearning process, enabling flexible integration into virtual and \\nlogical platform intelligently accomplish requested task and \\nusefully invoking all available information resources, thus \\ncreating a unified virtual learning platform for fully efficient \\ninteraction of all learning subjects. Effective technology for \\nimplementation distributed information systems of this class is \\nthe mobile software agents’ technology [1, 2]. These systems \\nmust not only distribute access to information, but also \\ndecentralized data storage and processing, and solving \\nproblems related with information resources semantic diversity.  \\nThe developed multi-agent system implements a virtual \\nlearning environment (VLE), in which real learning processes \\nrelated with development and implementation of all \\ncomprehensive forms of academic technological innovation in \\nlearning and education and any comprehensive ideas that can \\nbe implemented as relevant information processes[4]. The \\nsubjects of learning can be presented as software agents that \\ninteract with each other in a single VLE and perform the \\ninterest of its owners, forming an open multi-agent system with \\na decentralized architecture. \\nSoftware agent is computer system, which is found in some \\nenvironment and is capable of autonomous action in this \\nenvironment in order to meet its design objectives [4]. Software \\nagents have characteristics that make them suitable for complex \\nfunctions. Such features include: autonomy, interaction, \\nreactivity, activity, intelligence and mobility [5, 6].  \\nWCSIT 4 (2), 18 -25, 2014 \\n19 \\nThis paper presents the software agents interaction models \\nand the algorithms for learning activities in the virtual learning \\ninformation systems environment, enabling faster response to \\nchanges in the agents virtual environment simulating \\ninnovative field by increasing the intensity of information \\nexchange between the agents and to reduce the overall load on \\nthe network. Indicating ways to enhance the interaction among \\nmobile agents in an apportioned multi-agent system of \\ninformation to back up learners and teachers action. \\nII. \\nPROBLEM STATEMENT \\nMulti-agent systems (MAS) technology - a new pattern of \\ninformation technology, focused on the sharing of scientific \\nand technological achievements and benefits which provide \\nideas and methods of artificial intelligence (AI), the current \\nlocal and global computer networks, distributed databases and \\ndistributed computing, hardware and software tools to support \\nthe theory of distribution and transparency. \\nRelevance of distributed AI and MAS, in accordance with \\nthe  works [7, 9], determined by the complexity of diversity \\nmodern organizational and technical systems, complexity, and \\ntasks distributions, large volumes of Information flow and \\ninformation processing time. Agent-oriented approach is \\nwidely used in various fields that required solutions for \\ncomplex distributed tasks such as combined product design, re-\\nengineering information processes and the construction of \\nvirtual systems, and e-commerce systems, distributed computer \\nprograms development. \\nThe greatest complexity in the theoretical studies and \\npractical modern MAS implementations are issues related to \\nthe agents’ interaction processes in the collective solving \\nproblems with high practical complexity and relevance, as each \\nagent solves specific subtask, has only a partial idea about the \\ngeneral task and should continuously interact with other \\nagents. Therefore, at present the most relevant in the MAS \\ntheory and technology is the issues related with creating \\nintelligent agents interaction models in the MAS. \\nIn inter-host software agents interaction in a virtual learning \\nenvironment, arise a set of issues related with increasing the \\nnetwork load, and reducing the information exchange intensity \\nbetween agents which depends on the speed and bandwidth of \\nthe network connections. Increasing the request delivery time \\nand the needs time to find the correct agent for interaction. As a \\nsuitable solution for these problems, proposed a method based \\non the decomposition a common information space in which \\nagents operate in virtual platform (platform represents some \\nseparate network host) and move intensively interacting agents \\nto these platforms in order to combining agents in coalition. \\nThe proposed solution can be implemented in two mutually \\ncomplementary algorithms software agents’ interaction: the \\nalgorithm \\nof \\ninter-host \\nsoftware \\nagent’s \\ninteraction \\n(Transformation the inter-host agent’s interaction to intra-host \\ninteraction) and software agent’s dynamic allocation algorithm \\n(balancing load between system hosts) [10]. \\nHowever, the architecture design of decentralized open \\ninformation systems, arise problem related with determining \\nthe location of dynamically distributed mobile agents, that are \\nmoving between network hosts over time, as well as \\nminimizing the data losses possibility and the transmitting \\nmessages between agents delay time. The proposed solution in \\nthis paper based on the expansion of the agent address structure \\nand the information about its current location in the network \\nand local caching this information on the system hosts, which \\nwill allow sending messages directly from the sender agent to \\nthe recipient agent, as well as provide the ability to search \\nagents with the joint action through intermediary agents, using \\ntheir brokering services [11] and matchmaking services [12] \\nfunctions. Algorithm searching agents’ initiators executed in \\nthe agents intermediaries address space. \\nIII. \\nMULTI-AGENT VIRTUAL LEARNING ENVIRONMENT \\nThe modern virtual learning environment (VLE) is a \\nsoftware system designed to support teaching and learning. \\nVLEs generally function on the World Wide Web, and, \\ntherefore, they can be utilized both on and off-campus, \\nprovided that the users are authorized and can access the \\nInternet [12]. This surmount over the restriction of traditional \\nface-to-face learning, and guarantees that learning is neither \\nlimited to location nor time. VLEs can contains complex of \\ninteractions between teachers, learners and learning contents, \\nby adding agents and environment to this structure, the VLE \\ncan be presented as a complex of agents interaction and every \\nagent represents the interest of the learning structure \\ncomponents, such as learner agent, teacher agent, content \\nagent, and every agent interacts with other agents to \\naccomplish requested task. However the set of interacted \\nagents can contain “learner –learner interaction agent, learner-\\nteacher interaction agent, learner-content interaction agent, \\nteacher-teacher interaction agent, teacher-content interaction \\nagent, content-content interaction agent, learner-environment \\ninteraction agent, teacher-environment interaction agent, \\ncontent-environment interaction agent”, VLEs become more \\npopular and included in many college organizations all over the \\nworld. It is not only because of their versatility, but also \\nbecause they provide an extensive range of tools or features, \\nsuch as content distribution, evaluation, emails[14, 15]. \\nBased \\non \\nthe \\nlearning \\nprocess \\ncomponents \\nthe \\nrepresentation of VLE can be presented as a set of agents that \\ninteract together, shown in figure 1. \\nLearner-Content \\nInteraction Agent\\nTecher-Content \\nInteraction Agent\\nLearner-Learner Interaction \\nagent\\nLearner-Teacher \\nInteraction Agent\\nTeacher-Teacher Interaction \\nagent\\nContent-Content Interaction \\nagent\\n \\nFigure 1.  Virtual learning environment based on multi-agents architecture.  \\nWCSIT 4 (2), 18 -25, 2014 \\n20 \\nIV. \\nAGENTS COMMUNICATION MODEL \\nAgents Communication Model in an open multi-agent \\ninformation system that supports e-learning process, according \\nto the work [14], can be classified by the interaction character \\nin \\nthe \\nfollowing \\ncategories: \\ndirect \\n(point-to-point \\ncommunication), \\ninteraction \\nand \\nmoderating \\n(throw \\nintermediaries or server agents) interaction. In point to point \\nagents’ model communication message delivered directly from \\nagent sender to the agent receiver, even if the agent receiver is \\na mobile agent (a message can be sent through a set of \\ncomputer network host’s). In the intermediary agent model \\ncommunication mainly uses agents brokering services or \\nmatchmaking services agent’s intermediaries, which allow \\nagents to communicate with each other, this model provides \\nmultipoint and anonymous agent’s interaction. These two types \\nof services have their advantages and disadvantages: brokering \\nservices are more efficient, while matchmaking services are \\nmore flexible [10]. \\nDepending on the agent-receiver location in the network \\nagent’s communication model, according to [6, 9], may be also \\nclassified as follows: inter-host communication and intra-host \\ncommunication. When the agent-sender and agent-receiver \\nlocated on the same network host, the messages transmission \\nbetween these both agents called intra-host communication. \\nWhen they were located on two different network hosts, the \\nmessages transmission between agents delivered via the \\nnetwork nods, and this type of communication called the inter-\\nhost communication. Even if the Internet data rate has greatly \\nincreased, inter-host agent’s communication still would take \\ngreater time than interaction with intra-host communication. \\nTherefore, it is advisable to reform agent’s inter-host \\ncommunication to intra-host communication, which would \\nreduce the time required for messages transmission and \\nincrease the interaction intensity in the agent’s communication \\nprocess. \\nThe main disadvantages of presented agent’s interaction \\nmodels can be listed as follows: \\n– At inter-host interaction: the network over load  increases, \\nand the information exchange intensity between agents reduced \\ndue to increasing message delivery times, agents reaction for \\ndynamically changing events of environment or the absence of \\nany response to change the environment in message losing \\ncase; \\n– At the agents’ migration between system hosts there was \\na problem of the needed responder agent location determining \\nfor interaction during any time moment, which in its turn \\nresults to increasing in messages loss probability during their \\ntransfer and reducing the effectiveness of synchronized agent’s \\ninteraction. \\nV. \\nAGENT-BASED ALGORITHMS VIRTUAL LEARNING  \\nPLATFORMS FORMATION \\nThe higher effect obtained from using the virtual learning \\nenvironment (VLE), based on the biggest of its internal volume \\n(the number of registered requests), Agents representing the \\nLearning subjects interests, and placement of information \\ndatabases hosts [17]. However, the naturally growth of the \\nsystem volume leads to increase the information elements \\nsearch task complexity, and the selection of learning structures \\noptions due polynomial growth of alternatives quantity. In \\norder that the system no longer runs under its own unrestricted \\ngrowth, needed some self-organization algorithms, allowing \\ndynamically reorganize its internal structure to reduce the \\namount of processed and transmitted over the communication \\nlines data during the learning offerings placement and retrieval \\non the media hubs, and  forming a potentially effective learning \\nstructures.  \\nSelf-organization is the automatic generation within-VLE \\nvirtual learning platforms (VLP), combining agents with \\nsimilar interests in groups. Formation the VLP based on a the \\nregister distribution support method  for peer-to-peer hosts with \\nthe implicit treelike organization [9] in which as an organizing \\nstructure tree uses the hierarchical learning domain model. \\nLearning platforms formation carried out by displaying the \\nagent’s purposes on the treelike conceptual domain model, \\nSubsequent localization the main part of search and other \\nagents requests inside the group and further analysis the \\ncommunications activity with each other. The interests \\nsimilarity leads to the fact that the most active and informative \\nagents communication focused inside learning platform, \\nwhereas outside platform the information exchange is less \\nactive, thus an exchange object generalized (smaller by \\nvolume) agents learning offers or requests [18]. \\nThis approach allows not only reducing the total \\ncommunication amount between agents, but also due to the use \\nof mobile agents to transform inter-host in intra-host \\ncommunication. This, in turn, reduces the network load in case \\nof the distributed system implementation. The main agent \\nbased algorithms that enhance e-leaning systems efficiency are \\nagent interactions localization algorithm within hosts by \\nforming groups actively communicates agents (virtual learning \\nplatforms) and dynamic transfer load redistribution by \\nimplementing agents group migration algorithm. These \\nalgorithms provide ultimately, the conversion agents inter host \\ninteractions in intra-host. In order to convert the inter-host \\ninteractions in intra host interactions, the system agents   can be \\ndistributed the network hosts depending on their interaction \\nnature and behavior. Since the agents interaction nature can be \\ncontinuously changed, the agents distribution on the network \\nhosts should be dynamic. In this case, the agents automatically \\ndistributed on multiple network hosts, depending on their \\ninteraction nature, some hosts may be overwhelmed by the \\nseveral migrating agents to them. Therefore, the proposed \\nagents dynamic distribution algorithm, that based on their \\ninteraction behavior should be complemented by another \\nagents distribution algorithm which support uniform load \\ndistribution between network hosts. \\nIn the proposed multi-agent system [7], each host equipped \\nwith an agent-based platform (agent representative), which is a \\nagents local execution environment, in which software agents \\noperate and interact with each other. Implemented agents \\ninteraction algorithms involve a sequence of similar operating \\nphases and use common components that are present on each \\nagent platform. \\nWCSIT 4 (2), 18 -25, 2014 \\n21 \\nTo arrange a dynamic agents distribution on each agent \\nsystem platform the following components should operate. The \\nMessage Manager (MM), coordinating the messages exchange \\nbetween the system agents; and the System Monitor (SM), \\nperiodically checking the load on the current network host; and \\nthe Agent Distribution Manager (ADM), is responsible for the \\nagents dynamic distribution and analysis the models of agents \\ninteraction behavior and character; and finally the Agent \\nMigration Manager (AMM), displacing agents on other \\nplatforms (hosts) and managing agents migration between \\nnetwork hosts. \\nA. Agents interaction Localization algorithm  \\nThe agent’s interaction localization algorithm will contain \\nfour phases as shown in figure 2. \\nADM Host P1\\nStart\\nEvaluate inter-host and \\nintra-host \\ncommunications \\nintensity\\nCollect Information \\nabout Agent Sender And \\nAgent Receiver Host\\nEvaluate communication \\ndependency \\nDone?\\nNo\\nYes\\nADM Host P1\\nSystem Monitor\\nMessage Manager \\nAdd Agent I to group G \\nYes\\nNo\\nADM P1 Requests \\navailable recourses on \\nHost P2 \\nCalculate the \\ncommunication \\ndependency coefficient \\nADM Host P2\\nEnough recourses on \\nHost P2?\\nADM P2 permits \\nmigration\\nYes\\nNo\\nMigrate Agents group to \\nhost P2 \\nEnd\\nADM Host P1\\nAnalysis Phase\\nDistribution phase\\nInteraction phase\\nMigration phase\\n \\nFigure 2.  Agent’s interaction localization algorithm  \\n \\n\\uf0b7 \\nAnalysis phase. The Agent’s Distribution Manager \\n(ADM) evaluates inter-host and intra-host system \\nagents’ communications intensity, involving in this \\nstage system monitor and messages manager. ADM \\nalso uses all information as about agent-sender, as \\nabout agent-receiver host. ADM periodically evaluates \\ncommunication \\ndependency \\n\\uf028\\uf029\\nij\\nD\\nt\\n at \\ntime \\n\\uf05b\\n\\uf05d\\n1\\n2\\n,\\n\\uf0ce\\n\\uf03d\\nt\\nT\\nt t\\n between the agent i  and agents host \\nj  as follows: \\n( )\\n(\\n)\\n(\\n)\\nij\\nij\\nij\\nik\\nk\\nR (t)\\nD\\nt =\\n+ 1-\\nD\\nt -1\\nR (t)\\nd\\nd\\næ\\nö÷\\nç\\n÷\\nç\\n÷\\nç\\n÷\\nç\\n÷\\nç\\n÷\\nç\\n÷÷\\nçè\\nø\\nå\\n       (1) \\nWhere  \\n\\uf028\\uf029\\nij\\nD\\nt  - the number of messages sent by the agent \\ni  to host j  agents over the time period\\n1 2\\n,\\nt t\\nT \\uf0e9\\n\\uf0f9\\n\\uf0eb\\n\\uf0fb, \\uf064 - is the \\ncoefficient, which characterizes the relative dependency of new \\ninformation in relation to the outdated (expired) and used to \\nignore the temporary intensive interaction with the agents in \\nparticular agent platform; \\n\\uf028\\n\\uf029\\nij\\nD\\nt -1  - is defined as the value \\nof the same communication dependency in the previous time \\ninterval. \\n\\uf0b7 \\nAgents Distribution Phase. After a specified number of \\nanalysis stage repetitions \\nADM calculates the \\ncommunication dependency coefficient between the \\ncurrent host agent n and all other hosts in the system. \\nCommunication dependency coefficient \\nij\\nM  between \\nagent i and  host j agents can be  given by: \\n \\n,\\n\\uf0e6\\n\\uf0f6\\n\\uf03d\\n\\uf0b9\\n\\uf0e7\\n\\uf0f7\\n\\uf0e8\\n\\uf0f8\\nij\\nij\\nin\\nD\\nM\\nj\\nn\\nD\\n \\n \\n (2) \\nWhen the maximum value of the communication \\ndependency coefficient over a predefined edge \\uf062, ADM \\ncurrent host includes under consideration the agent in a group \\nof agents, located on a remote system host: \\n \\n\\uf028\\n\\uf029\\uf028\\n\\uf029\\narg max\\n\\uf062\\n\\uf06a\\n\\uf03d\\n\\uf0d9\\n\\uf03e\\n\\uf0ae\\n\\uf0ce\\nj\\nij\\nik\\ni\\nk\\nB\\nM\\nM\\nG     (3) \\nWhere \\uf06ai is an agent i , \\nk\\nG  indicates the agents group k , \\nand under arg j  hereinafter understood operation that returns \\nthe value of j , where the ratio of communication dependence \\ncoefficient \\nij\\nM  takes the maximum value. \\n\\uf0b7 \\nInteraction Phase. Before moving the selected agents \\ngroup from the host \\n1P to the receiver host \\n2P  ADM \\nhost \\n1P  interacts with ADM host \\n2P . ADM \\n2P checks \\nthe current memory host state, CPU utilization, and the \\nWCSIT 4 (2), 18 -25, 2014 \\n22 \\nnumber of agents hosted in this host, by using system \\nmonitor. If the host \\n2P  has enough free system \\nresources for new agents, ADM host \\n2P   authorizes \\nthe agents group migration from host \\n1P to host \\n2P . \\n\\uf0b7 \\nAgents’ Migration Phase. Accomplishing the agents’ \\nselection operation for migration and receiving positive \\nresponse from ADM receiver host, the sender host \\nADM jointly with Agents Migration Manager (AMM) \\ninitiates the selected agents’ group migration to the \\nreceiver host. \\nAgents’ interaction localization algorithm assumes the \\nanalysis of dynamic changes in interaction patterns (models) \\nbetween agents, but this algorithm can overloaded some system \\nhosts due to the large number of agents which can be moved to \\nthe hosts. \\nWhen a host is overloaded, the system monitor detects this \\nstate and activates the agents’ redistribution process, which is \\nnot only based on individual agent movement between network \\nhosts, but the whole interacting agents group intensively \\ninteracts with each other.  \\nB. The agents dynamic distribution algorithm  \\nThe \\nagents \\ndynamic \\ndistribution \\nalgorithm \\n(load \\ndistribution algorithm) among system hosts consists five \\nphases: Analysis phase agents grouping phase, agent groups \\ndistribution phase, agents interaction phase and the agents \\nmigration phase.  \\n\\uf0b7 \\nAnalysis Phase. Each system host monitor periodically \\nchecks the agent platform state in which agents \\ninteracting on this host. And by calling special system \\nfunctions collects information about the host physical \\nresources such as (current CPU load and free memory \\nspace). When the system monitor defines that the host \\nis overloaded, it will activate the agents’ distribution \\nprocedure between system hosts. As a host overload \\ncriterion can be offered the maximum number of \\nagents operated on the host.  \\nWhen ADM gets notification about congestion from the \\nsystem monitor, it starts the local agents interaction monitor \\nprocedure, preliminary in order to locally splitting interacted \\nand cooperated agents into groups.  \\nThe group formed from agents that have most similar \\ninterests and goals, which are intensively, interact with each \\nother by exchanging messages. Information about agents and \\nagents groups registered in the special register. The information \\nanalysis provided on the register, allows to evaluate the load on \\nthe system hosts and determine the intensity between agents \\nand intergroup and inter-host communication levels, which, in \\nturn, allows to select loaded and unloaded hosts in the system, \\nand implement the dynamic agents redistribution and agents \\ngroups among the system hosts, in other words the agents or \\nagents groups movement with certain characteristics from \\noverloaded hosts to less loaded hosts in the system, containing \\nsimilar potential agents interests. After a predetermining the \\ntime interval ADM updates communication dependences \\nbetween the agents and the locally formed agents groups on \\ndifferent network hosts [10]. \\nCommunication dependence of \\n\\uf028\\uf029\\nij\\nD\\nt\\nbetween agent i  \\nand agents group j  at the moment t  can be determined by the \\nformula: \\n\\uf028\\uf029\\n\\uf028\\uf029\\n\\uf028\\uf029\\n\\uf028\\n\\uf029\\n\\uf028\\n\\uf029\\n1\\n1\\nij\\nij\\nij\\niK\\nK\\nR\\nt\\nD\\nt\\nD\\nt\\nR\\nt\\n\\uf064\\n\\uf064\\n\\uf0e6\\n\\uf0f6\\n\\uf03d\\n\\uf02b\\n\\uf02d\\n\\uf02d\\n\\uf0e7\\n\\uf0f7\\n\\uf0e7\\n\\uf0f7\\n\\uf0e8\\n\\uf0f8\\n\\uf0e5\\n    (4) \\nWhere \\n\\uf028\\uf029\\nij\\nD\\nt  - is the number of messages sent by the \\nagent i  to the agents group j - over the time period \\n1 2\\n,\\nt t\\nT \\uf0e9\\n\\uf0f9\\n\\uf0eb\\n\\uf0fb, \\n\\uf064 - is the coefficient, which characterizes the relative \\nimportance of new information in relation to the outdated \\n(expired) and used. In this case the expression \\n\\uf028\\uf029\\niK\\nKR\\nt\\n\\uf0e5\\n \\nshows the number of messages sent by agent i  to any agent \\njointly functioning on the common host. \\n\\uf0b7 \\nAgents Grouping Phase. After accomplishing a number \\nof monitoring phases repetitions each i  agent is \\noverridden in local agents group with new index \\n*j  \\nand given by: \\n\\uf028\\uf029\\n\\uf028\\n\\uf029\\n*\\n*\\n \\nj\\nij\\ni\\nj\\nj\\narg max M\\nt\\nA\\n\\uf06a\\n\\uf03d\\n\\uf0ae\\n\\uf0ce\\n               (5) \\nWhere \\n*j\\nA\\n indicates \\n*j - local group agents. \\nMonitoring and agents grouping phases are repeated several \\ntimes. After each agents grouping phase information about \\nlocal communication dependencies between agents zeroed. \\n\\uf0b7 \\nAgent Groups Distribution Phase. After performing a \\ncertain number repetitions of monitoring and agents \\ngrouping phases, ADM depending on agent platform \\nstatus takes the decision to move the agents group to \\nother system host. The selection of moved agents based \\non the communication dependence between agent \\ngroups and the system hosts. \\n Communication Dependence \\nij\\nW  between i - agents group \\nand the j  system host based on the summation of the \\ncommunication dependence between all members of agents \\ngroup and the system host: \\n\\uf028\\uf029\\n\\uf028\\uf029\\ni\\nij\\nkj\\nk A\\nW\\nt\\nC\\nt\\n\\uf0ce\\n\\uf03d\\uf0e5\\n                                  (6) \\nWhere \\niA - all agents indexes set, belonging to i - agents \\ngroup, and \\n\\uf028\\uf029\\nkj\\nC\\nt\\n - communication dependence between \\nagent k  and j  system host in the current time t . The agents \\ngroup *i , which have a smallest dependence on the current \\nhost  and can be selected by the following rule: \\nWCSIT 4 (2), 18 -25, 2014 \\n23 \\n*\\n,\\nij\\ni\\nj j n\\nin\\nW\\ni\\narg max\\nW\\n\\uf0b9\\n\\uf0e6\\n\\uf0f6\\n\\uf03d\\n\\uf0e7\\n\\uf0f7\\n\\uf0e7\\n\\uf0f7\\n\\uf0e8\\n\\uf0f8\\n\\uf0e5\\n            \\n(7) \\nWhere n  is the current host (Agent platform) number [10]. \\nThe receiving agent platform \\n*j , selected by i  agents \\ngroup determined using communication dependence between i  \\nagents group and \\n*j   system host as follows: \\n\\uf028\\n\\uf029\\n*\\n \\n, \\nj\\nij\\nj\\narg max W\\nj\\nn\\n\\uf03d\\n\\uf0b9\\n \\n       (8) \\nWhere n  is the current host (Agent platform) number. \\n\\uf0b7 \\nAgents Interaction Phase. When in the network defined \\nthe receiving host for agent groups, the sender host \\nADM begins interaction with the corresponding \\nreceiving host ADM. If the receiving ADM allows \\nagents movement, sending host ADM starts the agents \\nmigrating phase. Otherwise, the sending host ADM \\npolls other system hosts ADM as long as it finds a \\nsuitable host for a scheduled agents migration \\n(movement). If no host is unable to take the agents \\ngroup, the interaction phase fails and after a certain \\ntime period the agents distribution (allocation) \\nalgorithm on the sending host restarted, and the process \\nrepeats again. \\nAgents interaction phase of this algorithm is similar to the \\nagents interaction localization algorithm. Yet, the interaction \\nlevel between these two algorithms is very different: agents’ \\ninteraction localization algorithm occurs at the agent level, \\nwhereas the algorithm load distribution between the system \\nhosts implemented at the agent groups level. If the agent \\nreceiver-platform \\nrepresentation \\nhas \\nsufficient \\nsystem \\nresources for all agents belong to the selected local agents \\ngroup, ADM agent receiver-platform representation may \\nauthorize the agents’ group movement. Otherwise, the system \\nreceiver host rejects the request to move the selected agents \\ngroup; the host is unable to accept only part of agents group. \\n\\uf0b7 \\nAgents Migration Phase. When ADM sender host \\nreceives a response with acceptance to move the agents \\ngroup from ADM receiver host, ADM sender host \\ninitiates migration the selected local agents group to \\nthe receiver host. \\nEach agent-based representation independently performs its \\nown agents distribution algorithm in accordance with the \\navailable information about the load on the system host, where \\nit is located, and the agents interaction character functioning in \\nagents platform [10]. The agents dynamic distribution \\nalgorithm shown in figure 3. \\nADM sender Host\\nStart\\nEvaluate agents platform \\nstate\\nCollect Information \\nabout Host physical \\nresources\\nIs the host overloaded?\\nNo\\nYes\\nSystem Monitor\\nADM sender Requests \\nmigration validation \\nfrom ADM receiver \\nHost \\nValid response?\\nRequest migration \\nvalidation from another \\nADM Host \\nYes\\nNo\\nMigrate Agents group to \\nselected host \\nEnd\\nADM receiver Host\\nActivate Agents \\ndistribution procedure\\nLocally split interacted \\nand cooperated agents \\ninto groups\\nUpdate communication \\ndependency \\nDone?\\nNo\\nYes\\nADM\\nADM\\nCreate local Agents \\ngroup \\nGet communication \\ndependency equals to \\nzero\\nCalculate the \\ncommunication \\ndependence\\nSelect Agent receiving \\nhost\\nSelect Agent Group \\nFor movment\\nAnalysis Phase\\nDistribution phase\\nInteraction phase\\nMigration phase\\nGrouping phase\\n \\nFigure 3.  Agent’s dynamic distribution algorithm  \\nC. Agent communication model based on using agents \\nintermediary \\nIn open multi-agent systems with extremely dynamic \\ncharacter (in any time may come in or/and go out new agents, \\nWCSIT 4 (2), 18 -25, 2014 \\n24 \\nand be connect new hosts), each individual agent cannot have \\nfull information about all other agents in the same information \\nand communication environment. In this environment, a joint \\noperation of network services agents intermediaries (brokerage \\nand matchmaking services) are highly effective in finding \\npotential agents for interaction [19, 22]. Software agents can \\nfind the other agents names using matchmaking services or \\nsend messages to other agents using brokering services, using \\ntheir attributes, such as methods, operation modes, features or \\nnicknames instead of their real names. To register the agents \\nnames in distributed virtual learning environment can be used \\nthe dedicated server for agents’ names. Agent Intermediary \\nfunctions implemented as a separate component in its \\narchitecture, or they can be implemented as an independent \\nspecialized application called agents intermediaries [24]. \\nInteraction mediating between the agents includes brokers’ \\nagents or matchmakers’ agents. Brokers forward the messages \\ngiven by the agent sender to the network hosts where the agent \\nreceiver located, whereas matchmakers’ only provides the \\nagents intending to send messages, all information about the \\nagent receiver location, in other words the final message was \\ndelivered to the agent receiver brokers, while the matchmakers \\nonly help sender agents to deliver messages to the agent \\nreceiver, \\nproviding \\ninformation \\nabout \\ntheir \\nlocations. \\nConsidering the number of steps within transmitting massages, \\nit can be concluded that the brokerage services are more \\neffective than matchmakers, since brokerage services typically \\nrequire two steps to transmit massage, while matchmakers \\nrequires three steps. \\nUsing shared memory area, that called data fragments \\nspace, which controlled by intermediary agents, agents can \\nregister their attributes together with the names in this area, and \\nthey can communicate each other by using the attributes \\ninformation about other agents, this information can be \\nextracted from this area. The basis of most developed \\nintermediary agents is Linda-model [9, 10]. The proposed \\nmodel presented in figure 4.  \\nMassages Queue\\nMsg 1\\nMsg 2\\nMsg n\\nData \\nFragments \\nArea\\nTasks Queue\\nTask 1\\nTask 2\\nTask n\\n…\\n…\\nMatchmakers\\nData Fragment \\nManager\\nBrokers\\nOR\\nFigure 4. Agent communication  model based on using agents intermediary \\nThe model allows software agents to use intermediary \\nagents, and the provided services by them to search agents with \\nthe similar interests and provides the basis method of using \\nintermediary agents and their functions. The intermediaries \\nagents don’t only control the data fragments, but can be also \\nused as own agents search algorithms “joint activity” and \\nalgorithms provided by agents’ initiators. This is the main \\ndifference between the proposed method of using intermediary \\nagents and their features compared to the existing ones.  \\nVI. \\nCONCLUSION \\nThis paper discusses separated agent-based algorithms for \\nVirtual learning environment information system effective \\nfunctioning and distributed hosts interaction algorithms. \\nDescribed various implementation approaches of agents \\ncommunication algorithms. Analyzed the existent software \\nagents communication models. Identified weaknesses of these \\nmodels, based on that proposed approaches to improve the \\nmobile agents cooperation in distributed Virtual learning \\nenvironments. \\nBased on the modification of existing agents interaction \\nmodels, developed algorithms for data exchange among mobile \\nsoftware agents that increase the agents efficiency for \\ndistributed data processing and reduce the total transmitted data \\namount over the network: \\n\\uf0b7 \\nAgent communication algorithm based on the using \\nagents brokers and their functions; \\nHowever, developed software agents interaction algorithms \\nfor \\ninformation \\nsystem \\nactivities \\nin \\nvirtual \\nlearning \\nenvironments, that allowing faster agents response for \\nenvironment changes in which an agent functioning, by \\nincreasing information exchange intensity between agents and \\nreduce the overall load on the network, those algorithms are \\nAgents interaction localization algorithm transformation inter-\\nhost communication to interact as intra-host communication; \\nand agents dynamic distribution algorithm (balancing load \\nbetween the system hosts). \\nBased on developed software agents interaction models and \\nalgorithms, proposed the minimizing inter-host communication \\nin problem-oriented distributed systems method. The method is \\nbased on software agents classification in semantic space, \\npresented as conceptual domain model, and agents inter-host \\ncommunication transformation in intra-host. Implementation \\nthis \\nmethod \\nprovides \\na \\nreduction \\nin \\ncommunication \\ninfrastructure load and improving availability coefficient for \\nsoftware agents application services. \\nACKNOWLEDGEMENT \\nThe author is grateful to the Applied Science Private \\nUniversity, Amman, Jordan, for the financial support granted to \\ncover the publication fee of this research article \\nREFERENCES \\n[1] \\nА. \\nМаслобоев, \\nМультиагентная \\nтехнология \\nформирования \\nвиртуальных \\nбизнес-площадок \\nв \\nедином \\nинформационно-\\nкоммуникационном пространстве развития инноваций, Publisher, \\nCity, 2009. \\n[2] B.M. Balachandran, M. Enkhsaikhan, Developing multi-agent e-commerce \\napplications with JADE, in:  Knowledge-Based Intelligent Information \\nand Engineering Systems, Springer, 2007, pp. 941-949. \\n[3] D. Tavangarian, M.E. Leypold, K. Nölting, M. Röser, D. Voigt, Is e-\\nlearning the Solution for Individual Learning, Publisher, City, 2004. \\nWCSIT 4 (2), 18 -25, 2014 \\n25 \\n[4] T. Erl, A. Karmarkar, P. Walmsley, H. Haas, L.U. Yalcinalp, K. Liu, D. \\nOrchard, A. Tost, J. Pasley, Web service contract design and versioning \\nfor SOA, Prentice Hall, 2009. \\n[5 F.P. Brooks Jr, The design of design: Essays from a computer scientist, \\nPearson Education, 2010. \\n[6] E. Platon, N. Sabouret, S. Honiden, Tag Interactions in MultiAgent \\nSystems: Environment Support, Publisher, City, 2005. \\n[7] S. Alouf, F. Huet, P. Nain, Forwarders vs. centralized server: an evaluation \\nof two approaches for locating mobile agents, Publisher, City, 2002. \\n[8] N. Carriero, D. Gelernter, Linda in context, Publisher, City, 1989. \\n[9] B. Chen, H.H. Cheng, J. Palen, Integrating mobile agent technology with \\nmulti-agent systems for distributed traffic detection and management \\nsystems, Publisher, City, 2009. \\n[10] А.В. Маслобоев, Модели и алгоритмы взаимодействия программных \\nагентов в виртуальной бизнес-среде развития инноваций, Publisher, \\nCity, 2009. \\n[11] K. Malik, Use of knowledge brokering services in the innovation process, \\nin:  Management of Innovation and Technology (ICMIT), 2012 IEEE \\nInternational Conference on, IEEE, 2012, pp. 273-278. \\n[12] T.A. Farrag, A.I. Saleh, H.A. Ali, Semantic web services matchmaking: \\nSemantic distance-based approach, Publisher, City, 2013. \\n[13] Z. Xu, Z. Yin, A. El Saddik, A Web Services Oriented Framework for \\nDynamic E-Learning Systems, Publisher, City. \\n[14] A. Targamadze, R. Petrauskiene, Classification of distance learning \\nagents, Publisher, City, 2010. \\n[15] E.M. Van Raaij, J.J. Schepers, The acceptance and use of a virtual \\nlearning environment in China, Publisher, City, 2008. \\n[16] A. Di Stefano, C. Santoro, Locating mobile agents in a wide distributed \\nenvironment, Publisher, City, 2002. \\n [17] W.F. McComas, Virtual Learning Environment, in:  The Language of \\nScience Education, Springer, 2014, pp. 110-110. \\n[18] E. Sangineto, An Adaptive E-Learning Platform for Personalized Course \\nGeneration, Publisher, City, 2008. \\n[19] S.L. Greenspan, E. Hadar, Using cloud brokering services for an \\nopportunistic cloud offering, in, Google Patents, 2013. \\n[20] S. Muthaiyah, L. Kerschberg, Brokering Web Services via a Hybrid \\nOntology Mediation Approach Using Multi Agent Systems (MAS), \\nPublisher, City, 2010. \\n[21] P. Khanna, B. Babu, Cloud Computing Brokering Service: A Trust \\nFramework, in:  CLOUD COMPUTING 2012, The Third International \\nConference on Cloud Computing, GRIDs, and Virtualization, 2012, pp. \\n206-212. \\n[22] J.-b. LI, C.-h. LI, Research on the Development of the Cooperation \\namong Governments, Enterprises, Universities, Research Institutes, \\nFinancial Units and Intermediary Agents [J], Publisher, City, 2010. \\n \\nAUTHORS PROFILE \\n \\nMIS Department at Applied Science University. Since 2008, I am an Assistant \\nProfessor at the Faculty of Information technology, Applied Science \\nUniversity in Jordan. Research interests include: Information system \\nreengineering, Service oriented architecture, software agents, agents \\ntheory, agents behavior.  \\n \\n \\nDr. Zahi A. M. Abu Sarhan Received the \\nM.S. and PhD degrees in Computerized \\nControl \\nAutomated \\nSystems \\nand \\nProgressive Information Technologies \\nfrom Kharkov National University of \\nRadio Electronics, Kharkov  in 1998 \\nand 2004, respectively. During 2004-\\n2008, I was an Assistant Professor at the \\nEconomics and Administrative  science/ \\n\"),\n",
              " Document(metadata={'Published': '2021-10-07', 'Title': 'Appearance', 'Authors': 'Rachel McDonnell, Bilge Mutlu', 'Summary': 'Socially interactive agents (SIAs) are no longer mere visions for future user\\ninterfaces, as 20 years of research and technology development has enabled the\\nuse of virtual and physical agents in day-to-day interfaces and environments.\\nThis chapter of the ACM \"The Handbook on Socially Interactive Agents\" reviews\\nresearch on and technologies involving socially interactive agents, including\\nvirtually embodied agents and physically embodied robots, focusing particularly\\non the appearance of socially interactive agents. It covers the history of the\\ndevelopment of these technologies; outlines the design space for the appearance\\nof agents, including what appearance comprises, modalities in which agents are\\npresented, and how agents are constructed; and the features that agents use to\\nsupport social interaction, including facial and bodily features, those that\\nexpress demographic characteristics, and issues surrounding realism, appeal,\\nand the uncanny valley. The chapter concludes with a brief discussion of open\\nquestions surrounding the appearance of socially interactive agents.', 'entry_id': 'http://arxiv.org/abs/2110.04083v1', 'published_first_time': '2021-10-07', 'comment': '38 pages, 16 figures, appears in the \"The Handbook on Socially\\n  Interactive Agents: 20 Years of Research on Embodied Conversational Agents,\\n  Intelligent Virtual Agents, and Social Robotics Volume 1: Methods, Behavior,\\n  Cognition,\" published by the ACM', 'journal_ref': 'In B. Lugrin, C. Pelachaud, D. Traum (Eds.), The Handbook on\\n  Socially Interactive Agents, Volume 1 (pp. 107-146). ACM books (2021)', 'doi': '10.1145/3477322.3477327', 'primary_category': 'cs.HC', 'categories': ['cs.HC', 'cs.GR', 'cs.RO'], 'links': ['http://dx.doi.org/10.1145/3477322.3477327', 'http://arxiv.org/abs/2110.04083v1', 'http://arxiv.org/pdf/2110.04083v1']}, page_content='Author note:\\nThis is a preprint. The final article is published in \\n“The Handbook on Socially Interactive Agents” \\nby ACM books.\\nCitation information: \\nMcDonnell, R., and Mutlu, B. (2021) Appearance. In B. Lugrin, C. Pelachaud, D. Traum (Eds.), Handbook on Socially \\nInteractive Agents – 20 Years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social \\nRobotics, Volume 1: Methods, Behavior, Cognition (pp. 107-146). ACM books.\\nDOI of the final chapter: 10.1145/3477322.3477327\\nDOI of volume 1 of the handbook: 10.1145/3477322\\n \\n \\n \\nAppearance\\nRachel McDonnell and Bilge Mutlu\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCorrespondence concerning this chapter should be addressed to Rachel McDonnell & Bilge Mutlu, \\nramcdonn@scss.tcd.ie, bilge@cs.wisc.edu\\narXiv:2110.04083v1  [cs.HC]  7 Oct 2021\\n1 Appearance\\nRachel McDonnell, Bilge Mutlu\\n1.1\\nWhy appearance?\\nOne might question why we need appearance for the metaphor to work, as voice assistants\\ncan effectively express characteristics of a metaphor solely through behavior. We argue that,\\nalthough disembodied agents can effectively serve as computer-based assistants in speciﬁc\\nscenarios of use, for example, involving driving and visually impaired users, appearance\\nprovides a “locus of attention” [Cassell 2001] for the cognitive and interactive faculties of the\\nuser of the system. Additionally, human communication mechanisms, such as mutual gaze,\\nturn-taking, body orientation, necessitate the presence of appropriate visual cues to properly\\nfunction, making appearance a necessity for agent design. Studies of human-human, human-\\nagent, and human-robot interaction provide strong evidence that such mechanisms work\\nmore effectively when parties provide appearance-based cues. The mere presence of a form\\nof embodiment in interacting with an agent improves social outcomes, such as motivation\\n[Mumm and Mutlu 2011]. As the scale and modality of appearance get closer to that of\\nthe metaphor, these outcomes further improve; human-scale and physical agents have more\\nperceived presence [Kiesler et al. 2008] and persuasive ability [Bainbridge et al. 2011] than\\nscaled-down and virtual agents.\\n1.2\\nHistory\\nAgents with virtual and physical embodiments follow different historical trajectories. Virtual\\nagents, also called embodied conversational agents, a term coined by Cassell [2000], are\\n“computer-generated cartoonlike characters that demonstrate many of the same properties\\nas humans in face-to-face conversation, including the ability to produce and respond to\\nverbal and nonverbal communication.” Early visions for virtual agents involved characters\\ninvolved re-played recordings of human performers, such as the intelligent personal agent\\nincluded in the Knowledge Navigator concept developed by Apple in 1987 [Colligan 2011\\n(accessed June 30, 2020] (Figure 1.1). First implementations of virtual agents were stylized\\nnonhuman or human characters that were generated through 3D modeling and rendering\\nand were embedded within virtual environments. An example of early nonhuman characters\\nincluded Herman the Bug, an animated pedagogical agent embedded within a virtual learning\\nenvironment [Lester et al. 1997]. Another early example is Rea, a real-estate agent that\\nfollowed a stylized humanlike design and appeared within a simulated home environment\\n1\\n1.2 History\\n2\\nFigure 1.1\\nEarly examples of virtual embodiments. Left: The Rea real-estate agent [Cassell 2000]; Right:\\nthe personal assistant envisioned for Knowledge Navigator [Sculley 1989].\\n[Cassell 2000]. Although these examples represent agents that are controlled and visualized\\nby computer systems, the design of such nonhuman and human characters have a long history\\nin shadow puppetry, dating back to the ﬁrst millennium BC [Orr 1974]. These characters were\\ndesigned for storytelling and entertainment, and the character designs reﬂected historical or\\ncultural ﬁgures as well as characters developed with backstories. The design of the characters\\nalso include stylizations and ornamentations that reﬂect their ethnic and cultural context, such\\nas the character Karag¨oz that followed a stylized human design with clothing and storyline\\nfrom the 16-19th century Ottoman Empire [Scarce 1983].\\nThe design of agents with robotic embodiments date back to mechanical humanoid au-\\ntomata designed as early as the 10th century BC [Hamet and Tremblay 2017]. As did with\\nFigure 1.2\\nEarly physical agents. Left: Mechanical Turk automata by Joseph Racknitz (1789), image\\ncourtesy of Humboldt University Library; Right: a tea-serving Karakuri puppet, Karakuri\\nningyo (c) 2016 Donostia/San Sebastian.\\n1.2 History\\n3\\nvirtual characters and shadow puppetry, the physical appearance of these early automata also\\nfollowed stylized humanlike forms. Examples, shown in Figure 1.2, include the design of the\\nMechanical Turk, a covertly human-controlled chess-playing machine that integrated a hu-\\nmanoid chess player on a wooden chest where the human operator hid [Simon et al. 1999].\\nKarakuri puppets, mechanical automata designed in the 17-19th century Japan to be used, for\\nexample, to ceremonially serve tea, followed a stylized humanlike appearance and traditional\\nJapanese clothing [Yokota 2009]. Although the appearance of robotic agents has overwhelm-\\ningly followed a human form with some level of stylization, robotic agents also commonly fol-\\nlow nonhuman morphologies. Examples of nonhuman appearances include the doglike robot\\nAibo designed by Sony in 1999 [Pransky 2001], a robotic seal designed for therapy in assisted\\nliving settings [Wada et al. 2005], and Keepon, a robot whose appearance resembled that of a\\nchick [Kozima et al. 2009]. Finally, robots have also been envisioned as cartoonish characters\\nthat blend features from different sources, such as the design of the WALL·E robot by Pixar,\\na trash compactor with features that suggested humanlike eyes and arms [Whitley 2012].\\nIn the 1960s, the ﬁeld of computer graphics and animation started to gain momentum, and\\nby the 1970s most of the building blocks of 3D computer animation were laid, such as surface\\nshading by Gouraud [1971] and Phong [1975] and texture mapping by Catmull [1974]. It\\nwas not long until computer generated characters began to appear in feature-ﬁlms such as\\nFutureworld (1979, Richard T. Heffron), which was ﬁrst to showcase a computer animated\\nhand and face, with both wireframe and 3D shading, while the well-known ﬁlm Tron (1982,\\nSteven Lisberger) followed soon after with a whole 15 minutes of computer generated content.\\nFully animated characters also started to appear in other areas such as music videos (e.g., Mick\\nJagger’s Hard Woman).\\nTen years later, the technology was developed even further and adopted in ﬁlms such as\\nTerminator 2: Judgment Day (1991, James Cameron) The Lawnmower Man (1992, Brett\\nLeonard), and Jurassic Park (1993, Steven Spielberg). This was the start of 3D animation\\nreceiving a widespread commercial success and it was not long until Pixar Animation Studios\\nreleased the ﬁrst entirely computer-animated feature-length ﬁlm Toy Story (1995, John Las-\\nseter). Toy Story was a massive success, largely due to the use of appealing cartoon-characters\\nwith plastic appearance, which computer graphics shading was perfectly suited to at that time.\\nIn the 2000s, more technology was being developed to support the growing industry and\\nPixar’s Monsters Inc. (2001, Pete Docter) showed impressive results with simulated fur de-\\npicting the subtle secondary motion on the coats of the monster characters. The Lord of the\\nRings: The Fellowship of the Ring (2001, Peter Jackson) pushed new boundaries with realistic\\ncrowd simulation, while in the same year Final Fantasy: The Spirits Within (2001, Hironobu\\nSakaguchi) attempted to create the ﬁrst photo-realistic virtual humans. While the near-lifelike\\nappearance of the characters in the ﬁlm was well received, some commentators felt the char-\\nacter renderings appeared unintentionally creepy. Films The Polar Express (2004, Robert Ze-\\nmeckis) and Beowulf(2007, Robert Zemeckis) marked further milestones in photorealism, but\\n1.2 History\\n4\\nagain received poor audience reactions. Photorealistic rendering was used more successfully\\nfor fantasy creatures such as the character Gollum from The Lord of the Rings: The Fellow-\\nship of the Ring, the ﬁrst full CGI character in a live-action movie. The actor that drove the\\nmovements of Gollum (Andy Serkis) even went on to win the ﬁrst performance-capture Oscar\\nfor his acting in later ﬁlms. Similar success was achieved with the photorealistic fantasy Navi\\ncharacters in Avatar (2009, James Cameron).\\nMore recent advancements in 3D scanning, Deep learning, and performance capture have\\nallowed actors to play realistic-depictions of their younger selves (Bladerunner 2049 (2017,\\nDenis Villeneuve), The Irishman (2019, Martin Scorsese), Gemini Man (2019, Ang Lee)) or\\neven to play virtual roles after they have passed-away (Peter Cushing in Star Wars: Rogue one\\n(2017, Gareth Edwards) and Paul Walker Fast and Furious 7 (2015, James Wan)).\\nIn the 1980 and 90s, there was also a shift towards interactive media such as games, where\\nreal-time animation was employed. This posed new challenges for character creation due to\\nthe additional requirements of character responsiveness and agency.\\nGame characters were thus less visually complex than ﬁlm characters of the time due\\nto the higher computation cost. The ﬁrst attempts in the 1980s were in the form of simple\\n2D sprites such as Pac-Man (Namco), Sonic the Hedgehog (Sega), and Mario (Nintendo).\\nWith the advent of home console systems and consumer-level graphics processing units, there\\nwas a shift from 2D to 3D in games such as Quake, The Legend of Zelda: Ocarina of Time,\\nTomb Raider, and Star Wars Jedi Knight: Dark Forces II. Characters started to appear more\\nsophisticated and used texture mapping techniques for materials and linear blend skinning for\\nanimation.\\nIn the 2000s, many games utilized cut scenes of cinematic sequences which could achieve\\nhigher photo-realism and conversation while disabling the interactive element of the game\\n(e.g., LA Noire, Heavy Rain, etc.). Nowadays, with real-time raytracing available in game\\nengines, there is no longer a need for photorealism to be restricted to cut-scenes, and we are\\nseeing incredibly realistic depictions of humans and environments in real-time (e.g., Detroit:\\nBecome Human and Hellblade: Senua’s Sacriﬁce).\\nThroughout the years, the graphics and game components have developed rapidly, allowing\\nprogressively more realistic depictions every year, though characters with advanced facial\\nanimation and conversational capabilities are rarely seen. In commercial games, conversing\\nwith non-player characters (NPCs) is usually achieved by selecting predeﬁned conversation\\ntexts on the screen, to progress the conversation. There is scope in the future for truly\\nconversational NPCs. Additionally, as virtual reality becomes ever more immersive, we could\\nbe about to see the next evolution for the media with higher levels of realism, conversational\\ncapabilities and social presence with NPCs.\\n1.3 Design\\n5\\n1.3\\nDesign\\n1.3.1\\nWhat is appearance?\\nWhen we say “appearance” for agents, we refer to the virtual or physical embodiment that\\nusers can experience using their visual faculties. Most agents, from simple static visual repre-\\nsentations that accompany chatbots to human surrogates, follow a metaphoric design, that is,\\nthe design of the agent takes inspiration or reference from a familiar an existing or envisioned\\nbiological entity (e.g., a human, a dog, a grasshopper) or hybrid entity (e.g., a “trash can”\\nin appearance, but a cartoonish human in behavior). The expression of a metaphor involves\\ntwo key dimensions: appearance and behavior. Metaphoric designs can follow consistent or\\ninconsistent implementations across these two dimensions. For example, an agent that follows\\nthe metaphor of a dog and appears and behaves like a dog involves a consistent implementa-\\ntion, whereas a dog that speaks involves an inconsistent implementation, integrating dog-like\\nappearance with human-like behavior. The power of agents as a family of computer interfaces\\ncomes from metaphoric design, which jumpstarts user mental models and expectations of the\\nsystem using a familiar representation. For example, a computer system that uses speech as\\nthe mode of user interaction and follows a humanlike agent metaphor signals to the user that\\nthe system is capable of human mechanisms of communication, such as speech. Similarly, a\\nrobot designed to follow the metaphor of a maid or a butler is expected to be competent in\\nhousehold work.\\nA common approach to designing the appearance of agents is metaphorical design, where\\nthe design follows a well-known metaphor to elicit familiarity and jumpstart user mental\\nmodels of the agent’s capabilities. For example, a virtual agent designed to review hospital\\ndischarge procedures with patients followed the metaphor of a nurse, appearing on the screen\\nas a nurse in scrubs [Bickmore et al. 2009]. The design of most agents follow a singular\\nmetaphor, such as the ASIMO humanoid robot designed to appear as an astronaut wearing\\na spacesuit [Sakagami et al. 2002], although some designs blend multiple metaphors [Deng\\net al. 2019], such as the MiRo robot, which integrates multiple animal features chosen to\\nimprove perceptions of its friendliness and feelings of companionship [Prescott et al. 2017].\\nMetaphorical design provides not only morphological features for the design of the agent, but\\nit also provides additional behavioral and physical features such as clothing and environmental\\ncontext to further support the expression of the metaphor. An example of such features is the\\ndesign of Valerie the Roboceptionist, a receptionist robot situated in a receptionist’s cubicle,\\nequipped with a backstory that was consistent with the design of the character, and dressed\\nin clothing that was consistent with the backstory and the metaphor that the agent’s design\\nfollowed [Gockley et al. 2005]. Figure 1.3 illustrates examples of metaphorical design: the\\nParo, the Keepon and the iCat robots that followed the metaphors of a seal, a chick and a cat,\\nrespectively.\\n1.3 Design\\n6\\nFigure 1.3\\nExample metaphors used in the design of robotic agents. Left to right: PARO Therapeutic\\nRobot (c) 2014 PARO Robots U.S.; the Keepon robot that followed the metaphor of a chick\\n(c) 2007 BeatBots LLC, [Kozima et al. 2009]; the iCat robot designed to follow the metaphor\\nof a cat [van Breemen et al. 2005].\\nVirtual agents are also designed to follow different metaphors, most frequently of instruc-\\ntors or experts. For example, a digital double replica of a real doctor [Dai and MacDorman\\n2018] was found to be effective at delivering cues of warmth and competence (Figure 1.4).\\nMore importantly, the virtual doctor’s recommendations also signiﬁcantly inﬂuenced the de-\\ncisions of participants in the same manner as the real doctor, implying effectiveness at persua-\\nsion.\\nIn an educational context, a study on learning outcomes found that a human lecturer is\\npreferable, but that robotic and virtual agents may be viable alternatives if designed properly\\n[Li et al. 2016]. It was also shown that having a stereotypically knowledgeable appearance of\\nthe pedagogical agent inﬂuenced learning [Veletsianos 2010].\\nVirtual agents have also been used extensively as assistants. For example, as a navigation\\nassistant in a crash-landing scenario in a study by Torre et al. [2018, 2019], where they had\\nto persuade participants to accept their recommendations about items required for survival.\\nParticipants explicitly preferred interacting with a cartoon-like agent than a photorealistic\\none, and were more inclined to accept the cartoon-agents suggestions. Note that the photo-\\nrealistic agent was rated low on attractiveness, and since persuasion and attractiveness have\\nbeen linked in previous work (e.g., Suzanne R. Pallak and Koch [1983]) it may be the case\\nthat a more attractive virtual human may have been more persuasive.\\nAnother study compared digital avatars, humans and humanoid robots to determine the\\ninﬂuence of appearance on trust and identifying expert advice [Pan and Steed 2016]. They\\nfound that participants were less likely to choose advice from the avatar, irrespective of\\nwhether or not the avatar was an expert. In contrast, experts represented by the robot or by a\\nreal person were identiﬁed reliably.\\n1.3 Design\\n7\\nDepiction\\nCharacter\\nLow Realism\\nHigh Realism\\nHigh Warmth\\nLow Warmth\\nFigure 1.4\\nAn example of agent design by replicating human experts [Dai and MacDorman 2018].\\n1.3.2\\nModalities\\nAppearance can be expressed in graphical, virtual, video-mediated, physical, and hybrid\\nmodalities (Figure 1.5). Agents in graphical modalities are static or dynamic two-dimensional\\nrepresentations, such as a photo, drawing, or animation of a character. For example, “Laura,”\\na virtual nurse designed to support low-literacy patients appeared as a two-dimensional\\nrendering [Bickmore et al. 2009]. Virtual embodiments usually involve three-dimensional\\nsimulations that are rendered in real time or replays of rendered animations. An example\\nvirtual embodiment is MACH, a virtual interview coach that is rendered in real-time in\\na virtual environment and presented on a two-dimensional display [Hoque et al. 2013].\\nSuch representations can also be presented in virtual-reality and mixed-reality modalities\\n[Garau et al. 2005], which provide the user with a more immersive experience of the agent’s\\nembodiment. Agents with a physical appearance involve a robotic embodiment, such as the\\nRobovie robot designed as a shopping mall assistant [Iwamura et al. 2011] or the Geminoid\\ndesigned to serve as a human surrogate [Nishio et al. 2007]. Users of agents with physical\\nembodiments can also experience the appearance of the agent over video [Kiesler et al. 2008].\\n1.3 Design\\n8\\nFigure 1.5\\nModalities in which agents are expressed. Left to right, top to bottom: the nurse agent Laura\\nrendered as a graphical agent [Bickmore et al. 2009]; the MACH virtual interview coach\\n[Hoque et al. 2013]; the hybrid robot Spritebot with a physical body and a graphical face\\n[Deng et al. 2019]; the hybrid FurHat robot with a physical head and a projected face (c)\\n2021 Furhat Robotics; the Pepper physical robot (c) 2021 SoftBank Robotics; the Geminoid\\nF android robot [Watanabe et al. 2015].\\nFinally, hybrid embodiments bring physical and graphical or virtual features together, such\\nas a graphical face appearing on a physical body or graphical features that are projected\\non the surface of a physical body. Example of hybrid appearances include the FurHat robot\\n[Al Moubayed et al. 2012] or Valerie/Tank, a receptionist robot [Lee et al. 2010].\\nThe modality in which an agent is presented affects user perceptions of and experience\\nwith the agent. A large body of literature has aimed to compare interaction outcomes across\\ndifferent modalities toward testing the “embodiment hypothesis:” that physical embodiment\\nhas a measurable effect on user performance and perceptions in interactions with an agent.\\nThis body of work shows that, in general, users respond more favorably to agents with\\nstronger embodiments and human-scale sizes. In this context, “strong” embodiment refers\\nto modalities that elicit a strong sense of presence, such as physical or hybrid modalities, and\\n“weak” embodiment describes modalities such as graphical or virtual that may not elicit a\\nsense of presence at such an extent. Deng et al. [2019] systematically analyzed 65 studies\\nthat compared virtual and physical agents in measures of perceptions of the agent and task\\nperformance. The analysis showed that 78.5% of these studies involved improvements in\\nat least one of these categories of measures, consistent with the embodiment hypothesis,\\n15.4% involved no change, and 6.1% involved worsening in at least one of the categories of\\n1.3 Design\\n9\\nmeasures. Among the studies included in this analysis, the most comprehensive comparison\\nwas performed by Kiesler et al. [2008], who compared a collocated robot, a lifesize video\\nprojection of a remote robot, a lifesize projection of the virtual version of the robot, and the\\nvirtual robot on a computer screen. The measured interaction outcomes generally decreased\\nin this order, the participants responding to the robot more favorably than the virtual agent\\nand the collocated robot more than the projected robot.\\nThe modality in which the agent is presented not only affects user interaction with the\\nagent, but it also presents different sets of affordances. For example, even if the behaviors of\\na virtual character and a physical robot are controlled by the same algorithm, the behaviors\\ndemonstrated by the agents might look very different due to the differences inherent in the\\nmodalities. Unlike virtual characters, physical robots are subject to mechanical limitations\\nand bound by the physical properties of the real world, which might affect the speed with\\nwhich the agent displays a desired behavior (unbounded in virtual characters, bounded\\nby actuator performance in robots), the sounds that the agent makes (e.g., sound artifacts\\nproduced by robots executing motion), the detail with which agent features can be fabricated\\n(bound by modeling and rendering limitations in virtual characters and by physical fabrication\\nlimitations in robots), and so on. Physical robots and hybrid agents afford touch interactions\\nand offer texture and material hardness as additional cues. The scale in which the agent\\nis presented is another factor that affects affordances and interaction outcomes. Across all\\nmodalities, the closer the agent is presented to human scale, the more likely the agent will\\nsupport human communication mechanisms. For example, a robot that is expected to be\\nhugged by users must have a size that affords hugging.\\n1.3.3\\nAgent Construction\\nAn important factor that shapes agent appearance is how agents are constructed, which due to\\nhistorical as well as practical reasons varies based on the modality of the agent. For example,\\nphysical agents are constructed using processes and practices from industrial design, and\\ntheir designs are affected by factors such as manufacturing limitations, product safety, and\\nmaterial choice. On the other hand, the construction of virtual characters borrows processes\\nand practices from animated ﬁlmmaking and game design, and their designs are affected by\\nfactors including character backstory, the environment in which the agent will be presented,\\nand the mechanisms with which the agent interacts with its environment, the user, and the\\nuser’s environment. The paragraphs below outline some of these processes and practices.\\n1.3.3.1\\nConstruction of virtual characters\\nVirtual characters have fewer constraints in terms of design than robots, and can be pro-\\ngrammed to take on a multitude of different appearances, using a variety of modelling, and\\nrendering techniques. For modelling, virtual characters are typically visualised in 3D using\\na mesh of consecutive planar polygons which approximate the surface of the human’s body.\\n1.3 Design\\n10\\nPolygons are very simple building blocks, and so can be used to describe many different\\nshapes. They are also very quick to render on graphics hardware. The construction of 3D mod-\\nels is an established industry with many sophisticated packages available for model-building\\n(e.g., 3D Studio Max, Maya, Blender, Houdini, etc.). Creating detailed 3D virtual characters\\nusing these packages is a highly skilled and labour-intensive task primarily due to the fact\\nthat 3D models are created using a 2D display and a high level of geometric detail is required\\nto create convincing virtual characters. Generating 3D data for virtual characters can also be\\naccomplished by scanning real people using a range of techniques such as photogrammetry,\\nstructured light scanning or laser scanning. Photogrammetry is a type of scanning whereby\\na collection of still photographs from regular DSLR cameras taken from various angles is\\nall that is required to create a 3D model. Software then analyzes the photographs, matching\\ncharacteristic points of the object on the images. This creates a point cloud of vertices which\\ncan later be converted into a mesh. It is the most commonly used tool nowadays for scanning\\nhumans in the visual effects industry, where the number and quality of cameras used in the rig\\ncontribute to the accuracy of the recovered mesh.\\n3D scanning can also be performed using sophisticated 3D scanning devices to project\\nstructured patterns of light or lasers onto the surface of the human to reproduce a 3D model\\nthat is a copy of the original.\\nFor more stylized characters, artists can sculpt characters out of clay and then use one of\\nthe mentioned forms of 3D scanning to gather the data onto the computer.\\nProfessional grade 3D scanners are expensive, but there are also more affordable,\\nconsumer-grade technologies such as depth-sensor based 3D scanning (e.g., Microsoft Kinect)\\nand low-cost photogrammetry, which use regular cameras, but results are generally of lower\\nquality and suitable only for low ﬁdelity non-player characters. In the industry, there are a\\nnumber of rapid character creation products that only require a single photo and create a\\nvirtual human within seconds on a tablet or phone [Didimo 2019, itSeez3D 2020, Loom.ai\\n2020, Pinscreen 2019]. These methods are improving in quality and speed with recent ad-\\nvancements in computer vision and deep learning [Hu et al. 2017, Nagano et al. 2018, Saito\\net al. 2017, Thies et al. 2016, Yamaguchi et al. 2018].\\nOnce a 3D representation of a human character has been created, a number of different\\ntechniques can be utilised in order to add detail and realism. A wide variety of render\\nstyles from photorealistic to non-photorealistic can be achieved using rasterization for local\\nillumination or ray-tracing for more realistic global illumination [Marschner and Shirley\\n2016]. While the rasterizer is the current standard for real-time, recent GPU optimization\\nallows for ray-tracing in real-time, and we expect to see much higher realism in virtual\\ncharacters in the future with global-illumination.\\nBesides the underlying rendering approach, there are many other methods for adding\\nrealism such as texture mapping, and approximating the surface reﬂectance through shad-\\ning [Masson 2007]. Diffuse texture mapping enhances the character by adding image-based\\n1.3 Design\\n11\\nFigure 1.6\\nLeft: Wireframe render of a character with no texture mapping, Center: diffuse textures\\napplied, Right: high quality rendering including normal maps, specular map, subsurface\\nscattering, global illumination, etc.\\ninformation to its geometry, while entailing only a small increase in computation. The basic\\nidea is to map the colour of the image or ‘texture’ onto the corresponding colour of an object\\nat each pixel [Catmull 1974] which adds the illusion of detail to the model, such as clothing\\nmaterial and skin colour.\\nIn order to add colour detail to virtual characters, diffuse texture maps are used which\\ndeﬁne the color of diffused light (Figure 1.6). Additionally, there are situations where surfaces\\nare not smooth and roughness needs to be added if it is not present in the geometry. For\\nexample, skin is not a smooth surface as it has imperfections such as pores and wrinkles.\\nThese details are best added using normal maps which perturb the surface normals to add\\ndetail or displacement maps which add geometric detail.\\nIn modern computer graphics, surface properties are governed by shaders, the code snip-\\npets describing how a surface should react to incident light. Many physically-based shaders\\nhave been developed to produce realistic materials with different Bidirectional Reﬂectance\\nDistribution Functions (BRDF) [Nicodemus et al. 1992] (the function that relates the incident\\nto the reﬂected light). More recently, with the rapid advancements in graphics hardware, more\\ncomplex shading effects approximating a wide range of BRDFs can now be achieved in real-\\ntime. For example, subsurface light transport in translucent materials [Jensen et al. 2001] for\\nrealistic scattering of light on the skin was once a technique only used in off-line high-end\\nvisual effects, but real-time methods [Jimenez et al. 2009, 2010] are now used to enhance the\\nrealism in real-time.\\nHair for interactive virtual characters has traditionally been modelled using card-based\\nrendering, where images of chunks of the hair are mapped onto large ﬂat sheets, to approxi-\\nmate the shape of a much larger number of individual hairs. Later advancements allowed for\\nmodelling each individual hair which dramatically improves realism. For rendering of hair,\\n1.3 Design\\n12\\nphysically-based ﬁber reﬂectance models are used, based on a combination of an anisotropic\\nspecular and a diffuse component [Kajiya and Kay 1989]. More recently, the scattering distri-\\nbution of the hair ﬁber is split into different lobes based on the number of internal reﬂections\\nwithin the ﬁber [Marschner et al. 2003].\\nThe use of physically-based simulations is ubiquitous for realism in virtual clothing, where\\nfast mass-spring models [Liu et al. 2013] or more complex implicitly integrated continuum\\ntechniques [Baraff and Witkin 1998] are used in the state-of-the-art. Implementing realistic\\ncloth and hair dynamics in real-time applications still represents a signiﬁcant challenge for\\ndevelopers since simulation dynamics need to be solved at run-time, and are required to be\\nfast and stable. Based on this, depictions of stiff clothing and hair with little secondary-motion\\neffects are still commonplace for interactive virtual characters across a range of applications\\nfrom video games to virtual assistants.\\n1.3.3.2\\nIndustrial design of robots\\nThe paragraphs above have discussed design approaches, e.g., metaphorical design, to and the\\nresources used, e.g., facial features, for the development of agent appearance. Another factor\\nthat signiﬁcantly affects agent appearance is the industrial design of physical agents or the\\nphysical platforms in which virtual or hybrid agents are presented, including form, material\\nuse, scale, color choice, and so on. Although there are no systematic studies of how these\\nfactors affect agent appearance or how they must be designed to maximize user experience,\\nthe HRI literature includes reports of the design process for the appearance of speciﬁc robot\\nplatforms. For example, Lee et al. [2009] described the design process for Snackbot, a robot\\ndesigned to deliver snacks in an academic building, including the form of the housing of the\\nrobotic hardware and the snack tray that the robot would carry; the material and colors used\\nto construct the housing and the tray; the height of the robot; and the expressive features of\\nthe head and face of the robot. Another example is the design of the Simon humanoid robot,\\nwhere the research team explored the proportions that the robot’s head and body should follow,\\nthe placement of the eyes on the head, facial features that would achieve the appearance\\nof a “friendly doll,” and the interplay between the design of the housing and structural or\\nmechanical elements of the robot’s head [Diana and Thomaz 2011]. Hegel et al. [2010]\\ndocumented and reported on the industrial design of the social robot Flobi, which included an\\nexploration of the design of the robot’s head to follow a “baby face” schema; effective color\\ncombinations of the robot’s face, hair, lips, and eyebrows; and how blushing on the robot’s\\ncheeks could be achieved using LEDs placed behind the surface of the face. A ﬁnal example is\\nthe design of Kip1, a peripheral robotic conversation companion, involving form and material\\nexploration through sketches and mock-ups [Hoffman et al. 2015]. Figure 1.7 illustrates the\\nsketches and mock-ups generated in the industrial design of some of these examples.\\nIn all of the examples discussed above, the research team engaged professional industrial\\ndesigners or members of the research team with training in industrial design as well as an\\n1.4 Features\\n13\\nFigure 1.7\\nSketches and models generated during the industrial design of the Snackbot [Lee et al. 2009]\\n(top-left), Simon [Diana and Thomaz 2011] (top-right), and Kip1 [Hoffman et al. 2015]\\n(bottom) robots.\\niterative design process. The literature does not include any discussion of such considerations\\nfor virtual characters, and characters designed for research and commercial use all utilize\\nexisting display platforms, such as mobile phones, tablet computers, computer monitors,\\nlarge displays, or virtual- or mixed-reality environments. Overall, there is a great need for\\nsystematic research on the industrial design of the appearance of agents, including the effects\\nof the physical design of the agent itself and the environment within which virtual agents are\\npresented on user interaction and experience.\\n1.4\\nFeatures\\nThe design approaches described above draw on a rich space of features, shaped by the\\nmetaphor followed by the design (e.g., humanlike features included in the design of a virtual\\nhuman), functional requirements of the agent (e.g., light displays placed on physical robots\\nto convey the agent’s status), and/or aesthetic and experiential goals of the design (e.g.,\\nmaterial, color, and texture choices for a robot). The paragraphs below provide an overview\\nof this space, focusing on facial and bodily features as well as features that communicate\\ndemographic characteristics of virtual and physical agent embodiments.\\n1.4.1\\nFacial features\\nThe face of an agent serves as the primary interface between the agent and its user, and facial\\nfeatures make up a substantial portion of the design space for agents. Even when designs lack\\n1.4 Features\\n14\\nanthropomorphic or zoomorphic faces, people attribute facial features to them, highlighting\\nthe importance of faces in the perception of non-living objects [K¨uhn et al. 2014]. Designers\\nof virtual and physical agents draw on this human propensity and create faces that can display\\nconversational cues, express affect, and communicate direction of attention.\\nIn order to convey a true feeling of life in a character, the appearance of the eye is highly\\nimportant. Rendering techniques such as adding specular and reﬂection maps can be very\\nuseful for this purpose to increase the appearance of wetness and to reﬂect the environment.\\nAdditionally, more advanced techniques such as ambient occlusion allow for soft shadowing,\\nand refraction to replicate the refraction of light that passes through the eyeball, which is\\nﬁlled with ﬂuid. Creating the geometry of the eye is a difﬁcult task, due to the complexity\\nof the surface but there exist special photogrammetry rigs for capturing the visible parts of\\nthe eye—the white sclera, the transparent cornea, and the non-rigidly deforming colored iris\\n[B´erard et al. 2014]. Computer generated eyes used in computer graphics applications are\\ntypically gross approximations of the actual geometry and material of a real eye. This is\\nalso true for facial expressions, which typically take a simple approach of linearly blending\\npre-generated expression meshes (blendshapes) to create new expressions and motion [Anjyo\\n2018]. However, little is known about how these approximations affect user perception of the\\nappearance of virtual characters.\\nSimilar to the studies on real humans, virtual humans with narrow eyes have been rated\\nas more aggressive and less trustworthy for both abstract creatures [Ferstl et al. 2017] and\\nmore realistic depictions [Ferstl and McDonnell 2018] (Figure 1.8). It should be noted that\\nfor realistic eye size alterations, the size of the eyes themselves should not be scaled as this\\nwill be quickly perceived as eerie and artiﬁcial [Wang et al. 2013]. Instead, the shape of the\\neyelids can be changed as protruding eyes appear larger, whereas hooded and monolid eyes\\nappear smaller.\\nIn contrast to human face studies, wider faces were not judged as less trustworthy, and\\nwere perceived as less aggressive compared to narrow faces for realistic [Wang et al. 2013]\\nand abstract virtual characters [Ferstl et al. 2017], even when a particularly masculine rather\\nthan a babyface appearance was presented [Ferstl and McDonnell 2018]. The results of\\nthese studies support the notion that virtual faces are perceived differently from real human\\nfaces. A potential explanation could be the tendency of villains in animated movies to be\\nportrayed with narrow, long, sharp facial features (e.g., Captain Hook in Peter Pan (Clyde\\nGeronimi, 1953), Scar in The Lion King (Roger Allers, 1994), Maleﬁcent in Sleeping Beauty\\n(Clyde Geronimi, 1959)). This tendency could inﬂuence the perception of computer-generated\\ncharacters towards automatic association of narrow faces with dangerous characters.\\nOther work has addressed the perception of rather unusual facial proportions for realistic\\ncharacters and their inﬂuence on perceived appeal. Seyama and Nagayama [2007] studied eye\\nsize by morphing between photographs of real people and dolls, and found that characters\\nwere judged as unpleasant if the eyes had strong deviations from their original size. Partic-\\n1.4 Features\\n15\\nipants were more sensitive to the alterations for real faces than for artiﬁcial faces. Several\\nstudies conﬁrmed that altering facial parts lowers perceived appeal, especially for humanlike\\ncharacters. Green et al. [2008] demonstrated that not only proportions, but also the placement\\nof facial parts may negatively affect the perceived appeal. The measured effect was greater for\\nthe humanlike and more attractive faces. Additionally, it has been demonstrated that a mis-\\nmatch of realism between facial parts negatively affects appeal [Burleigh et al. 2013, Mac-\\nDorman et al. 2009].\\nFigure 1.8\\nLeft: Examples of eye and head shape manipulations on abstract characters (based on [Ferstl\\net al. 2017]), Right: More subtle facial feature manipulations on realistic virtual characters\\n(adapted from [Ferstl and McDonnell 2018]).\\nPrior work in HRI includes a large body of literature on the facial features of robotic\\nagents. A number of studies aimed to characterize the design space for robot faces. Blow et al.\\n[2006a] characterized this space as varying across the dimensions of abstraction, from low to\\nhigh abstraction, and realism, from realistic to iconic, borrowing from literature on the design\\nof cartoon faces [McCloud 1993]. DiSalvo et al. [2002] carried out an analysis of 48 robots\\nand conducted an exploratory survey that resulted in a number of design recommendations\\nto improve human perceptions of humanlike robots: (1) the head and the eye space should\\nbe wide; (2) facial features should dominate the face with minimal space for a forehead and\\na chin, (3) the design should include eyes with sufﬁcient complexity; (4) the addition of a\\nnose, a mouth, and eyelids improve perceptions of humanlikeness; and (5) the head should\\ninclude a skin or a casing that core the electromechanical components. A similar analysis\\nwas carried out by Kalegina et al. [2018] of 157 rendered robot faces—physical robots that\\n1.4 Features\\n16\\nFigure 1.9\\nThe 157 faces analyzed by Kalegina et al. [2018] (left), their analysis of facial features used\\nin the design of the robot faces (right-top), and the spectrum of facial realism (right-bottom).\\nCopyright Information: Images included in this paper under ACM guidelines on Fair Use\\nare equipped with a screen-based face and facial features that are virtually rendered on the\\nscreen—who coded the faces for 76 different features and conducted a survey to understand\\nhow each feature affected user perceptions of the robot (Figure 1.14). The study found that\\nfaces with no pupils and no mouth were consistently ranked as being unfriendly, machinelike,\\nand unlikable; those with pink or cartoon-style cheeks were perceived as being feminine; and\\nfaces with detailed blue eyes were found to be friendly and trustworthy. Survey participants\\nalso expressed preferences for robots with speciﬁc facial features for speciﬁc contexts of use,\\ne.g., selecting robots with no pupils and no mouth for security work and faces with detailed\\nblue eyes for entertainment applications. Consistently, Goetz et al. [2003] argued that there is\\nnot a universally preferred design for the facial features of a robot, but that people prefer\\nappearances that match the robot’s task. They varied the robot’s appearance across three\\nstylistic dimensions—human vs. machine, youth vs. adult, and male vs. female—and found\\nthat user preferences for facial features presented in these styles depended on the robot’s task.\\nIn a follow-up study, Powers and Kiesler [2006] showed that the length of the robot’s chin\\nand the fundamental frequency of its voice predicted whether participants expressed interest\\nin following advice from the robot.\\nThe literature also includes reports of the process for the design and development of\\nfaces for several robot platforms. For example, the design of the iCub social robot primarily\\ninvolved the mechanical replication of human anatomical mechanisms to achieve realistic eye\\nand head movements and the design of the rest of the face to follow a “toy-like” appearance\\n[Beira et al. 2006]. The design speciﬁcations for the face of the KASPAR social robot included\\na sufﬁciently expressive but minimal design, an iconic overall design (as opposed to a realistic\\none), a humanlike appearance, and the ability to express autonomy, communicate attention,\\nand display projected expressions [Blow et al. 2006b, Dautenhahn et al. 2009]. The design\\nof the humanoid robot HUBO integrated an abstract body with the overall appearance of an\\n1.4 Features\\n17\\nastronaut and an highly humanlike face using elastomer-based materials that appeared and\\nmoved similar to human skin and a 28-degree-of-freedom mechanism to achieve humanlike\\nfacial movements [Oh et al. 2006]. The faces of robots including the Flobi [L¨utkebohle et al.\\n2010], Melvin [Shayganfar et al. 2012], and iCat [van Breemen 2004] featured pairs of ﬂexible\\nactuators that served as the robot’s lips and pairs of eyebrows to express emotion. As discussed\\nearlier, the design of the face of the Flobi robot, shown in Figure 1.14, additionally included\\nsophisticated mechanisms for emotion expression, such as lights placed behind the cheeks to\\nenable the appearance of blushing. These reports illustrate how different facial features come\\ntogether in the design of different robot systems and point to speciﬁc examples in the design\\nspace of facial features for robots.\\n1.4.2\\nBodily features\\nWhile the face serves as the primary interface for human-agent interaction, the remainder of an\\nagent’s body also contributes to the appearance of the agent. The design space for an agent’s\\nbody primarily includes several bodily features, how these features come together structurally,\\nand how they are represented.\\nA virtual agent’s body can be presented in a range of different styles, from low-detailed\\nstick-ﬁgures or point-light displays to photorealistic bodies or anthropomorphised creatures,\\nand there have been some studies aimed at investigating the effect of the body representation\\non perception of the agent’s appearance and actions. Most studies apply motion captured\\nanimations to a virtual character and map the motion onto a range of bodies and assess if\\nthe different bodies change the meaning of the motion. Typically, factors such as emotion,\\ngender, and biological motion are chosen since these have all been shown to be identiﬁable\\nsolely through motion cues (e.g.,[Cutting and Kozlowski 1977, Johansson 1973, Kozlowski\\nand Cutting 1977]) thus allowing the contribution of the bodies appearance to be assessed.\\nBeginning with a study by Hodgins et al. [1998], the amount of detail in a virtual charac-\\nter’s representation has been studied to investigate the effect on perception. Their study found\\nthat viewers’ perception of motion characteristics is affected by the geometric model used for\\nrendering. They observed higher sensitivity to changes in motion when applied to a polygonal\\nmodel, than a stick ﬁgure. Chaminade et al. [2007] also found an effect on motion perception,\\nwhere character anthropomorphism decreased the tendency to report their motion as biologi-\\ncal, while another study found that emotions were perceived as less intense on characters with\\nlower geometric detail [McDonnell et al. 2009b].\\nBody shape has also been investigated where it was found that a virtual character’s\\nbody does not affect recognition of body emotions, even for extreme characters, such as a\\nzombie with decomposing ﬂesh [McDonnell et al. 2009b] (Figure 1.10). Fleming et al. [2016]\\nevaluated the appeal and realism of female body shapes, which were created as morphs\\nbetween a realistic character and stylized versions following design principles of major\\ncomputer animation studios. Surprisingly, the most appealing characters were in-between\\n1.4 Features\\n18\\nFigure 1.10\\nDifferent structural and material representations for agent body [McDonnell et al. 2009c].\\nmorphs, where 33% morphs had the highest scores for realism and appeal and 66% morphs\\nwere rated as equally appealing, but less realistic (Figure 1.11).\\nThe perception of sex of a virtual character’s walking motion has also been found to be\\naffected by body shape. Adding stereotypical indicators of sex to the body shapes of male and\\nfemale characters inﬂuences sex perception. Exaggerated female body shapes inﬂuenced sex\\njudgements more than exaggerated male shapes [McDonnell et al. 2009a].\\nIn virtual reality, embodiment of virtual characters is where the user is positioned virtually\\ninside the body of a virtual avatar, where they have agency over that virtual body. The\\ncharacter model used for the virtual avatar can affect the behaviour of the user, from becoming\\nmore conﬁdent when embodied in a taller avatar, more friendly as an attractive avatar [Yee\\nand Bailenson 2009], to reducing implicit racial bias by embodying an avatar of a different\\nrace [Banakou et al. 2016]. This powerful effect is referred to as the Proteus effect [Yee and\\nBailenson 2007] (named after the Greek god known for his ability to take on many different\\nphysical forms). The use of self-avatars or virtual doppelgangers has also been shown to affect\\noutcomes, with generally a positive inﬂuence on aspects such as cognitive load [Steed et al.\\n2016], pain modulation [Romano et al. 2014] and embodiment [Fribourg et al. 2020, Kilteni\\net al. 2012]. These effects describe to some extent the dynamism of interactions between users\\nand avatars.\\nThe design of a physical robot’s body is shaped by a number of factors, including the\\nmetaphor that the design follows, the functional requirements of the robot, and environmental\\nconstraints that the design must consider. The ﬁrst factor, the design metaphor, might dictate\\nhow the body of the robot is structured and the features that are articulated in the design.\\n1.4 Features\\n19\\nFor example, the Paro robot [Wada and Shibata 2007] follows the metaphor of a baby\\nseal, and the design of the robot’s body roughly follows the form of a seal, including fore\\nand hind ﬂippers. The functional requirement of the robot might include speciﬁc forms of\\nmobility, such as holonomic movement, climbing stairs, or movement across rough terrain,\\nor prehensile manipulation involving a single arm or two arms. Depending on such design\\nrequirements, the design of the body of a robot might follow a humanoid design including\\nhumanlike limbs attached to a torso, such as the ASIMO robot [Sakagami et al. 2002], or a\\nsingle arm attached on a mobile base, such as the Fetch robot [Wise et al. 2016]. Finally, the\\nenvironment that the robot is designed for can dictate the bodily features of the robot, such\\nas requiring that a robot that crawls into tight spaces has a low proﬁle and limbs that can be\\ntucked away, such as a Packbot robot [Yamauchi 2004] used in search-and-rescue scenarios.\\nIn addition to bodily features borrowed from the design metaphor, such as the hind ﬂippers\\nof a seal or the legs of a human, the design of physical robots also utilize features that\\nfacilitate speciﬁc functions. These functions include communication, and features that support\\ncommunication include lights that communicate the robot’s affective states using different\\ncolors [Bethel and Murphy 2007] or light arrays that convey information about the robot’s\\ndirection of motion using light patterns [Szaﬁr et al. 2015]. Features of a robot’s body may\\nalso support transferring items, such as a tray that the Snackbot robot held to carry food items\\n[Lee et al. 2009] and the different conﬁgurations of carts that hospital delivery robots pull to\\ntransport materials [Ozkil et al. 2009].\\nFigure 1.11\\nStylization applied at different levels (33%, 66%, 100%) to captured performer body (0%) in\\nMarvel and Disney styles (image based on Fleming et al. [2016]).\\n1.4 Features\\n20\\nFigure 1.12\\nSix body shapes with indicators of gender [McDonnell et al. 2009a].\\nAn agent’s body can also include bodily features, such as clothing or furniture, designed to\\nsupport the agent’s character or backstory or eventually improve user experience with the\\nagent. For example, the Roboreceptionist robot was placed in a booth that resembled an\\ninformation booth and wore clothes that were consistent with the gender and the backstory of\\nits character [Gockley et al. 2005]. The Geminoid robot, a highly realistic android developed\\nto serve as a robotic surrogate to support remote communication, was constructed to resemble\\nits creator and dressed in similar fashion [Nishio et al. 2007]. Figure 1.13 illustrates examples\\nof bodily features that support speciﬁc functions, such as a tray, and that support the agent’s\\ncharacter, such as clothing.\\nFigure 1.13\\nBodily features that support speciﬁc functions, such as a tray that the robot uses to delivery\\nsnacks [Lee et al. 2009] (left) and light arrays that a ﬂying robot uses to communicate\\ndirection [Szaﬁr et al. 2015] (left-center), and that support the agent’s character, such as a\\nbooth and clothing for a receptionist robot [Lee et al. 2010] (right-center) and clothing for a\\nsurrogate robot [Watanabe et al. 2015] (right).\\n1.4 Features\\n21\\nFigure 1.14\\nFacial features of the Flobi robot that provide the robot with different demographic\\ncharacteristics. Left: neutral male (top) and smiling female (bottom) faces; Center: the\\nphysical parts that represent facial features; Right: different hair and lip styles. Adapted from\\nL¨utkebohle et al. [2010].\\n1.4.3\\nFeatures expressing demographic characteristics\\nAgent appearance communicates other attributes of the character of the agent, such as gender,\\nage, race, and ethnicity. Virtual agents are usually designed as distinctive characters, such as\\nthe two female nurse characters, one middle-aged Caucasian and one middle-aged African\\nAmerican, designed by Bickmore et al. [2009] to match user patient demographics. Physical\\nagents, on the other hand, are designed as characters with ambiguous features and interchange-\\nable parts that highlight speciﬁc character attributes, such as the interchangeable hair and lips\\nof the Flobi robot that communicate a male or female gender [L¨utkebohle et al. 2010] (Figure\\n1.14).\\nA large body of research on human-agent interaction has shown such character attributes to\\nsigniﬁcantly shape interaction outcomes. For example, Siegel et al. [2009] asked participants\\nto make an optional donation to a robot that used pre-recorded male or female voices, which\\nresearch has shown to be sufﬁcient to trigger gender stereotypes [Nass et al. 1997], and found a\\nsigniﬁcant interaction between robot and participant gender over the proportion of participants\\nwho donated any amount, e.g., men consistently donating more to a female robot. Eyssel and\\nHegel [2012] manipulated the gender of the Flobi robot by varying the robot’s appearance\\nvia its interchangeable parts for hair and lips and found that participant perceptions of the\\nmale and female robots closely followed gender stereotypes. The male robot was perceived\\nas having more agency and being more suitable for stereotypically male tasks (e.g., repair),\\nand the female robot was perceived as being more communal and being more suitable for\\nstereotypically female tasks (e.g., childcare).\\nThe effect of stereotypes has also been studied for virtual characters, mostly in the context\\nof embodiment in virtual reality. The Proteus Effect, as mentioned previously, has addition-\\n1.4 Features\\n22\\nally shown that users conform to stereotypes associated with their avatar’s appearance. For\\nexample, embodiment in female avatars made players more likely to conform to female-typed\\nlanguage norms [Palomares and Lee 2010] and made them more likely to engage in healing\\nactivities [Yee et al. 2011]. Interestingly, these effects were observed regardless of the actual\\ngender of the player, indicating a tendency to conform to expectations associated with the\\nvirtual gender.\\nIn other work, Zibrek et al. [2015] explored gender bias on different types of emotions\\napplied on male and female virtual characters. They found that emotion biases gender per-\\nception according to gender stereotypes: an angry motion is seen as more male, while fear\\nand sadness are seen as less male motions, and they observed a contrast effect where anger\\nwas seen as more male when viewed on a female model than when viewed on a male model.\\nSimilar effects were found for real humans [Hess et al. 2004], indicating that virtual humans\\nfollow similar stereotyping effects.\\n1.4.4\\nRealism, Appeal, Uncanny Valley\\nMetaphorical design involves the application of a familiar metaphor to the design of an agent,\\nsuch a virtual human following the metaphor of a human. In practice, metaphors are applied\\nat different levels of abstraction due technical limitations (e.g., inability to closely replicate\\nthe original metaphor) and design choices (e.g., stylization). Deng et al. [2019] argued that\\ndesigns follow discrete metaphors (e.g., a “baby seal” metaphor) but the realism in which\\nthese metaphors are applied to vary along a spectrum of abstraction (e.g., a stylized or abstract\\nhousehold robot vs. a highly realistic robotic surrogate). The design choices of metaphor and\\nabstraction result in differences in user perceptions of the agent and experience with it.\\nIn the classic textbook “Disney Animation: The Illusion of Life,” Thomas and Johnston\\n[1995] use the term appeal to describe well designed, interesting and engaging characters.\\nThis is contrary to many face perception studies, which use the term appeal and attractiveness\\ninterchangeably. Appeal is an essential ingredient for virtual characters in video games\\nand movies, as well as for avatars, agents, and robots, to ensure audience engagement\\nand positive interactions. Creating highly detailed, photorealistic virtual characters does\\nnot necessarily produce appealing results [Geller 2008], and it is often the case that more\\nstylized approximations evoke more positive audience responses and engagement [Zell et al.\\n2019]. However, additional factors are the context of the interaction and how appropriate the\\nappearance is under the circumstances. For example, having a fun cartoon-appearance may\\nbe less appropriate for a more serious application such as a for a business meeting [Junuzovic\\net al. 2012] or medical training [Volante et al. 2016], etc. Perception of appeal of virtual\\ncharacters is an ongoing area of research, with the ultimate goal to speed-up or automate the\\nprocess of producing appealing characters, and avoid negative reactions from audiences.\\nThe term Uncanny Valley (UV) is often used to describe the negative reactions that can\\noccur towards virtual characters. It is a feeling of repulsion produced by artiﬁcial agents that\\n1.4 Features\\n23\\nFigure 1.15\\nLeft: Examples of manipulating material (y-axis) and shape (x-axis) to vary character\\nrealism and appeal, image based on Zell et al. [2015], Right: Examples of brightness and\\nshadow alterations on cartoon characters displaying emotion which were shown to change the\\nperceived intensity of emotion [Wisessing et al. 2020].\\nappear close to human-form but not quite real. This UV phenomenon was ﬁrst hypothesized\\nby in the 1970s by robotics professor Mori [1970]. Mori’s original hypothesis states that\\nas a robot’s appearance becomes more human, humans evoke more positive and empathetic\\nresponses, until a point where the response quickly becomes strongly negative resulting\\nin feelings of disgust, eeriness and even fear. Once the robot’s appearance becomes less\\ndistinguishable from a human being, the emotional response becomes positive once again.\\nThis negative response has been attributed to many causes such as motion errors or lack of\\nfamiliarity or a mismatch in realism between elements of character design. More recently,\\nthe UV hypothesis has been transferred to virtual humans in computer graphics, and has been\\nexplored directly in some studies [Bartneck et al. 2009, MacDorman et al. 2009]. Virtual faces\\nin particular are difﬁcult to reproduce as humans are very adept at perceiving, and recognising\\nother faces and facial emotions.\\nAs discussed previously, the appearance of a character can be separated into texture,\\nmaterials, shape and lighting. Various studies have attempted to isolate these factors and\\nindependently examine the effect on appeal and UV.\\nWallraven et al. [2007] studied the perceived realism, recognition, sincerity, and aesthetics\\nof real and computer-generated facial expressions using 2D ﬁlters to provide brush, cartoon,\\nand illustration styles and found that stylization caused differences in recognition accuracy\\nand perceived sincerity of expressions. Additionally, their realistic computer-generated faces\\nscored high aesthetic rankings, which is contrary to the UV theory. Pejsa et al. [2013] addi-\\n1.4 Features\\n24\\ntionally found no effect on appeal or lifelikeness between a character with human proportions\\nand one with stylized geometry including large eyes, while other studies found realistic and\\ncartoon depictions to be equally appealing when expressing personality [Ruhland et al. 2015]\\nand when a user had agency over their movements [Kokkinara and McDonnell 2015].\\nIn order to investigate the effect of stylization in more detail, McDonnell et al. [2012] cre-\\nated a range of appearances from abstract to realistic by altering the rendering style (texture,\\nmaterial and lighting) of a realistically modelled male character while keeping the shape and\\nmotion constant. They analyzed subjective ratings of appeal and trustworthiness and found\\nthat the most realistic character was often rated as equally appealing or pleasant as the car-\\ntoon characters, and equally trustworthy in a truth-telling task. A drop in appeal occurred for\\ncharacters in the middle of the scale (rated neither abstract nor realistic), which was attributed\\nto the difﬁculty in categorizing these characters due to their uncommon appearance [Saygin\\net al. 2012]. Other studies of the UV that used still images generated by morphing between\\nphotographs and animated characters also found valleys in participant ratings of uncanniness\\nfor intermediate morphs [Green et al. 2008, Hanson 2005, Seyama and Nagayama 2007]. This\\nidea was further developed in the categorization ambiguity hypothesis [Cheetham and Jancke\\n2013, Yamada et al. 2013], where it was shown that this response is more prominent when\\nthe morph is between a real human and an inanimate object or representation of a human.\\nStudies focusing on neurocognitive mechanisms attribute negative evaluation to a competing\\nvisual-category representations during recognition [Ferrey et al. 2015].\\nThis effect was also investigated in a study by Carter et al. [2013] where they created a\\nrealistic, cartoon, and robot female character and assessed subjective pleasantness ratings as\\nwell as analyzing eye-tracking as a psychophysiological measure. Contrary to the UV theory,\\nthey found higher ratings of unpleasantness for their cartoon than for their realistic character,\\nand that ﬁxations were affected by subjective perceptions of pleasantness.\\nInvestigating yet more parameters, Zell et al. [2015] independently examined the dimen-\\nsions of shape, texture, material and lighting, by creating a range of stimuli of characters with\\nvarious levels of realism and stylization (Figure 1.15 (left)). Their study identiﬁed that the\\nshape of the character’s face is the main descriptor for realism, and material increases real-\\nism only for realistic shapes. Also, that strong mismatches in stylization between material\\nand shape made characters unappealing and eerie, in particular abstract shapes with realistic\\nmaterials were perceived as highly eerie, validating the design choices of some horror movies\\nwith living puppets. Finally, blurring or stylizing a realistic texture can achieve a make-up\\neffect, increasing character appeal and attractiveness, without reducing realism. The opposite\\nwas found in a study on body stylization, where the stylization of body shape predicted appeal\\nratings rather than improvements to render quality [Fleming et al. 2016].\\nMore recently, Wisessing et al. [2020] carried out an in-depth analysis of the effect\\nof lighting on appeal, particularly brightness and shadows, and found that increasing the\\nbrightness of the key-light or lessening the key-to-ﬁll ratio (lighter shadows) increased the\\n1.4 Features\\n25\\nFigure 1.16\\nState-of-the-art real-time virtual humans in Unreal Engine 4 created by 3Lateral in\\ncollaboration with Cubic Motion, Epic Games, Tencent and Vicon. Left: Siren demo. Right:\\nvirtual replica of the actor Andy Serkis. With permission of Epic Games © 2020.\\nappeal ratings (Figure 1.15 (right)). They also found little effect of key-light brightness on\\neeriness but reported reduced eeriness as a consequence of lightening the shadows, which\\ncould be used to reduce UV effects of virtual characters. However, shadow lightening did not\\nimprove appeal for characters with realistic appearance, and thus key-light brightness alone\\nshould be used to enhance appeal for such characters.\\nSeveral studies in immersive VR have also examined the effect of character appearance on\\nviewer responses, focusing on co-presence, i.e., the sense that one is present and engaged in an\\ninterpersonal space with the character [Biocca 1997, Garau et al. 2003]. While some evidence\\nconﬁrms the importance of realistic appearance [Nowak 2001, Zibrek and McDonnell 2019],\\nothers put less importance on it [Garau et al. 2003, Slater and Steed 2002]. On the other\\nhand, a mismatch between the realism of behaviour and appearance has been often shown to\\nlower the feeling of co-presence [Bailenson et al. 2005]. There are a number of reasons why\\nmismatches may cause negative effects on the viewer. A mismatch between the physical and\\nemotional states of a character violate expectations and thus can result in a breakdown in how\\nusers experience agents [Vinayagamoorthy et al. 2006].\\n1.5 Summary\\n26\\n1.5\\nSummary\\nTechnical advancements are increasingly pushing the boundaries of how agents are designed\\nand developed, the capabilities of these agents, and their use in human environments. The\\nrapid development in real-time rendering technologies has enabled incredibly detailed, high-\\nquality virtual character appearances (Figure 1.16), often reaching photorealism [Epic Games,\\nInc. 2018, Seymour et al. 2017]. Deep learning is also improving the ease and speed at\\nwhich characters can be created, even from a single photograph [Yamaguchi et al. 2018].\\nAdditionally, animation and behaviours are starting to become easier and less expensive\\nto create, allowing virtual human technologies to be more accessible to a wider audience\\nthan ever before. With these advancements comes the increasing use of characters across\\ndifferent domains such as education, sales, therapy, entertainment, social media, and virtual\\nand augmented reality.\\nNew methods are also emerging for the construction of physical robots. Rapid fabrication\\nmethods, such as 3D printing, have led to the development of new robot morphologies,\\nincluding 3D printable robots inspired by “origami” [Onal et al. 2014] and robots with soft\\nskin that can change appearance and texture to communicate internal states to the user [Hu\\net al. 2018]. Mixed-reality technologies are also being utilized to facilitate human interaction\\nwith robots, including displaying cues that communicate the motion intent [Walker et al. 2018]\\nand and the ﬁeld of view [Hedayati et al. 2018] of the robot. Finally, robots are increasingly\\nbeing integrated into human environments across different domains, including manufacturing\\n[Saupp´e and Mutlu 2015], education [Belpaeme et al. 2018, Michaelis and Mutlu 2018], food\\nservices [Jennings and Figliozzi 2019], hospitality [Tussyadiah and Park 2018], surveillance\\n[Inbar and Meyer 2019], and healthcare [Miseikis et al. 2020, Mutlu and Forlizzi 2008]. As\\napplications proliferate, we will gain a better understanding of how the design space for agent\\nappearance is utilized to support each application domain, how the features of this space\\naffect user perceptions of and experience with these agents, and how the appearance of robotic\\nagents might be designed to support personalization, customization, and environmental ﬁt.\\nIn this chapter, we have shown that the choice of appearance can have implications for\\nhuman interactions in a number of ways, including changes to the perception of personality,\\nemotion, trust, and conﬁdence. Studies have shown that the many factors that constitute the\\nﬁnal appearance of an agent, such as the design metaphor, modality of representation, and\\nmethods of agent construction, including modelling, texturing, materials, and even lighting,\\nhave different effects on how people perceive and respond to it. This multidimensionality has\\nthe drawback that some factors might cancel each other out or amplify each other, leading\\nto inconsistent conclusions. Additionally, more frequent exposure to agents and increasing\\ntechnological sophistication may continuously change the way we perceive them, much like\\nhow we are becoming more and more sensitive to poor visual effects in movies [Tinwell et al.\\n1.5 Summary\\n27\\n2011]. The need for understanding the implications of different appearances of agents has\\ntherefore never been greater.\\nBibliography\\nS. Al Moubayed, J. Beskow, G. Skantze, and B. Granstr¨om. 2012. Furhat: a back-projected human-\\nlike robot head for multiparty human-machine interaction. In Cognitive behavioural systems, pp.\\n114–130. Springer.\\nK. Anjyo. 2018. Blendshape Facial Animation, pp. 2145–2155. Springer International Publishing,\\nCham. ISBN 978-3-319-14418-4. https://doi.org/10.1007/978-3-319-14418-4 2. DOI: 10.1007/978-\\n3-319-14418-4 2.\\nJ. N. Bailenson, K. R. Swinth, C. L. Hoyt, S. Persky, A. Dimov, and J. Blascovich. 2005. The indepen-\\ndent and interactive effects of embodied-agent appearance and behavior on self-report, cognitive, and\\nbehavioral markers of copresence in immersive virtual environments. Presence, 14(4): 379–393.\\nW. A. Bainbridge, J. W. Hart, E. S. Kim, and B. Scassellati. 2011. The beneﬁts of interactions with\\nphysically present robots over video-displayed agents. International Journal of Social Robotics, 3(1):\\n41–52.\\nD. Banakou, P. D. Hanumanthu, and M. Slater.\\n2016.\\nVirtual embodiment of white people in a\\nblack virtual body leads to a sustained reduction in their implicit racial bias. Frontiers in Human\\nNeuroscience, 10: 601. ISSN 1662-5161. https://www.frontiersin.org/article/10.3389/fnhum.2016.\\n00601. DOI: 10.3389/fnhum.2016.00601.\\nD. Baraff and A. Witkin.\\n1998.\\nLarge steps in cloth simulation.\\nIn Proceedings of the 25th\\nAnnual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’98, p. 43–54.\\nAssociation for Computing Machinery, New York, NY, USA. ISBN 0897919998. https://doi.org/10.\\n1145/280814.280821. DOI: 10.1145/280814.280821.\\nC. Bartneck, T. Kanda, H. Ishiguro, and N. Hagita. 2009. My robotic doppelg¨anger – a critical look at\\nthe uncanny valley. In Proc. of Robot and Human Interactive Communication, pp. 269–276.\\nR. Beira, M. Lopes, M. Prac¸a, J. Santos-Victor, A. Bernardino, G. Metta, F. Becchi, and R. Saltar´en.\\n2006. Design of the robot-cub (icub) head. In Proceedings 2006 IEEE International Conference on\\nRobotics and Automation, 2006. ICRA 2006., pp. 94–100. IEEE.\\nT. Belpaeme, J. Kennedy, A. Ramachandran, B. Scassellati, and F. Tanaka. 2018. Social robots for\\neducation: A review. Science robotics, 3(21).\\nP. B´erard, D. Bradley, M. Nitti, T. Beeler, and M. Gross. Nov. 2014. High-quality capture of eyes.\\nACM Trans. Graph., 33(6).\\nISSN 0730-0301.\\nhttps://doi.org/10.1145/2661229.2661285.\\nDOI:\\n10.1145/2661229.2661285.\\nC. L. Bethel and R. R. Murphy.\\n2007.\\nSurvey of non-facial/non-verbal affective expressions for\\nappearance-constrained robots.\\nIEEE Transactions on Systems, Man, and Cybernetics, Part C\\n(Applications and Reviews), 38(1): 83–92.\\nT. W. Bickmore, L. M. Pfeifer, and B. W. Jack. 2009. Taking the time to care: empowering low health\\nliteracy hospital patients with virtual nurse agents. In Proceedings of the SIGCHI conference on\\nhuman factors in computing systems, pp. 1265–1274.\\n29\\nBIBLIOGRAPHY\\n30\\nF. Biocca. 1997. The cyborg’s dilemma: Progressive embodiment in virtual environments [1]. Journal\\nof Computer-Mediated Communication, 3(2): 0–0.\\nM. Blow, K. Dautenhahn, A. Appleby, C. L. Nehaniv, and D. Lee. 2006a. The art of designing robot\\nfaces: Dimensions for human-robot interaction. In Proceedings of the 1st ACM SIGCHI/SIGART\\nconference on Human-robot interaction, pp. 331–332.\\nM. Blow, K. Dautenhahn, A. Appleby, C. L. Nehaniv, and D. C. Lee. 2006b. Perception of robot smiles\\nand dimensions for human-robot interaction design. In ROMAN 2006-The 15th IEEE International\\nSymposium on Robot and Human Interactive Communication, pp. 469–474. IEEE.\\nT. J. Burleigh, J. R. Schoenherr, and G. L. Lacroix. 2013. Does the Uncanny Valley exist? An empirical\\ntest of the relationship between eeriness and the human likeness of digitally created faces. Computers\\nin Human Behavior, 29(3): 759–771.\\nE. J. Carter, M. Mahler, and J. K. Hodgins. 2013. Unpleasantness of animated characters increases\\nviewer attention to faces. In Proceedings of the ACM Symposium in Applied Perception, pp. 35–40.\\nJ. Cassell. 2000. Embodied conversational interface agents. Communications of the ACM, 43(4): 70–78.\\nJ. Cassell. 2001. Embodied conversational agents: representation and intelligence in user interfaces. AI\\nmagazine, 22(4): 67–67.\\nE. Catmull. 1974. A subdivision algorithm for computer display of curved surfaces. PhD thesis, Dept.\\nof CS, University of Utah.\\nT. Chaminade, J. Hodgins, and M. Kawato.\\n2007.\\nAnthropomorphism inﬂuences perception of\\ncomputer-animated characters’ actions. Social Cognitive and Affective Neuroscience, 2(3).\\nM. Cheetham and L. Jancke. 2013. Perceptual and category processing of the uncanny valley hypothesis’\\ndimension of human likeness: some methodological issues. Journal of visualized experiments: JoVE,\\n(76).\\nB. Colligan.\\n2011 (accessed June 30, 2020).\\nHow the Knowledge Navigator video came about.\\nhttp://www.dubberly.com/articles/how-the-knowledge-navigator-video-came-about.html.\\nJ. E. Cutting and L. T. Kozlowski. 1977. Recognizing friends by their walk: Gait perception without\\nfamiliarity cues. Bulletin of the psychonomic society, 9(5): 353–356.\\nZ. Dai and K. F. MacDorman.\\n2018.\\nThe doctor’s digital double: how warmth, competence, and\\nanimation promote adherence intention. PeerJ Computer Science, 4: e168.\\nK. Dautenhahn, C. L. Nehaniv, M. L. Walters, B. Robins, H. Kose-Bagci, N. Assif, M. Blow, et al.\\n2009. Kaspar–a minimally expressive humanoid robot for human–robot interaction research. Applied\\nBionics and Biomechanics, 6(3, 4): 369–397.\\nE. Deng, B. Mutlu, and M. Mataric. 2019. Embodiment in socially interactive robots. arXiv preprint\\narXiv:1912.00312.\\nC. Diana and A. L. Thomaz. 2011. The shape of simon: creative design of a humanoid robot shell. In\\nCHI’11 Extended Abstracts on Human Factors in Computing Systems, pp. 283–298.\\nDidimo, 2019. The breathtaking reality of your digital you. https://mydidimo.com/. https://mydidimo.\\ncom/.\\nC. F. DiSalvo, F. Gemperle, J. Forlizzi, and S. Kiesler. 2002. All robots are not created equal: the\\ndesign and perception of humanoid robot heads. In Proceedings of the 4th conference on Designing\\ninteractive systems: processes, practices, methods, and techniques, pp. 321–326.\\nBIBLIOGRAPHY\\n31\\nEpic Games, Inc., Mar 2018. Siren. https://www.3lateral.com/projects/siren.html. https://www.3lateral.\\ncom/projects/siren.html.\\nF. Eyssel and F. Hegel. 2012. (s) he’s got the look: Gender stereotyping of robots 1. Journal of Applied\\nSocial Psychology, 42(9): 2213–2230.\\nA. E. Ferrey, T. J. Burleigh, and M. J. Fenske. 2015. Stimulus-category competition, inhibition, and\\naffective devaluation: a novel account of the Uncanny Valley. Frontiers in Psychology, 6: 249.\\nY. Ferstl and R. McDonnell. 2018. A perceptual study on the manipulation of facial features for trait\\nportrayal in virtual agents. In Proc. of Int. Conf. on Intelligent Virtual Agents (IVA), pp. 281–288.\\nDOI: 10.1145/3267851.3267891.\\nY. Ferstl, E. Kokkinara, and R. McDonnell. 2017. Facial features of non-player creatures can inﬂuence\\nmoral decisions in video games. ACM Transaction on Applied Perception, 15(1): 4:1–4:12. ISSN\\n1544-3558. DOI: 10.1145/3129561.\\nR. Fleming, B. J. Mohler, J. Romero, M. J. Black, and M. Breidt. 2016. Appealing female avatars from\\n3D body scans: Perceptual effects of stylization. In Int. Conf. on Computer Graphics Theory and\\nApplications (GRAPP).\\nR. Fribourg, F. Argelaguet, A. L´ecuyer, and L. Hoyet.\\n2020.\\nAvatar and sense of embodiment:\\nStudying the relative preference between appearance, control and point of view. IEEE Transactions\\non Visualization and Computer Graphics, 26(5): 2062–2072. DOI: 10.1109/TVCG.2020.2973077.\\nM. Garau, M. Slater, V. Vinayagamoorthy, A. Brogni, A. Steed, and M. A. Sasse. 2003. The impact\\nof avatar realism and eye gaze control on perceived quality of communication in a shared immersive\\nvirtual environment.\\nIn Proceedings of the SIGCHI conference on Human factors in computing\\nsystems, pp. 529–536. ACM.\\nM. Garau, M. Slater, D.-P. Pertaub, and S. Razzaque. 2005. The responses of people to virtual humans in\\nan immersive virtual environment. Presence: Teleoperators & Virtual Environments, 14(1): 104–116.\\nT. Geller. 2008. Overcoming the Uncanny Valley. IEEE Computer Graphics and Applications, 28(4):\\n11–17.\\nR. Gockley, A. Bruce, J. Forlizzi, M. Michalowski, A. Mundell, S. Rosenthal, B. Sellner, R. Simmons,\\nK. Snipes, A. C. Schultz, et al. 2005. Designing robots for long-term social interaction. In 2005\\nIEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 1338–1343. IEEE.\\nJ. Goetz, S. Kiesler, and A. Powers. 2003. Matching robot appearance and behavior to tasks to improve\\nhuman-robot cooperation. In The 12th IEEE International Workshop on Robot and Human Interactive\\nCommunication, 2003. Proceedings. ROMAN 2003., pp. 55–60. Ieee.\\nH. Gouraud. 1971. Continuous shading of curved surfaces. IEEE Transactions on Computers, C-20(6):\\n623–629.\\nR. D. Green, K. F. MacDorman, C.-C. Ho, and S. Vasudevan. 2008. Sensitivity to the proportions of\\nfaces that vary in human likeness. Computers in Human Behavior, 24(5): 2456–2474.\\nP. Hamet and J. Tremblay. 2017. Artiﬁcial intelligence in medicine. Metabolism, 69: S36–S40.\\nD. Hanson.\\n2005.\\nExpanding the aesthetics possibilities for humanlike robots.\\nIn Proc. of IEEE\\nHumanoid Robotics Conf., Special Session on the Uncanny Valley.\\nH. Hedayati, M. Walker, and D. Szaﬁr. 2018. Improving collocated robot teleoperation with augmented\\nreality. In Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interac-\\nBIBLIOGRAPHY\\n32\\ntion, pp. 78–86.\\nF. Hegel, F. Eyssel, and B. Wrede. 2010. The social robot ‘ﬂobi’: key concepts of industrial design. In\\n19th International Symposium in Robot and Human Interactive Communication, pp. 107–112. IEEE.\\nU. Hess, R. B. Adams, and R. E. Kleck. 2004. Facial appearance, gender, and emotion expression.\\nEmotion, 4(4): 378–388.\\nJ. K. Hodgins, J. F. O’Brien, and J. Tumblin. Dec. 1998. Perception of human motion with different\\ngeometric models.\\nIEEE Transactions on Visualization and Computer Graphics, 4(4): 101–113.\\nhttp://graphics.cs.berkeley.edu/papers/Hodgins-PHM-1998-12/.\\nG. Hoffman, O. Zuckerman, G. Hirschberger, M. Luria, and T. Shani-Sherman. 2015. Design and\\nevaluation of a peripheral robotic conversation companion. In 2015 10th ACM/IEEE International\\nConference on Human-Robot Interaction (HRI), pp. 3–10. IEEE.\\nM. Hoque, M. Courgeon, J.-C. Martin, B. Mutlu, and R. W. Picard.\\n2013.\\nMach: My automated\\nconversation coach. In Proceedings of the 2013 ACM international joint conference on Pervasive and\\nubiquitous computing, pp. 697–706.\\nL. Hu, S. Saito, L. Wei, K. Nagano, J. Seo, J. Fursund, I. Sadeghi, C. Sun, Y.-C. Chen, and H. Li. Nov.\\n2017. Avatar digitization from a single image for real-time rendering. ACM Transactions on Graphics\\n(TOG), 36(6): 195:1–195:14. ISSN 0730-0301.\\nY. Hu, Z. Zhao, A. Vimal, and G. Hoffman. 2018. Soft skin texture modulation for social robotics. In\\n2018 IEEE International Conference on Soft Robotics (RoboSoft), pp. 182–187. IEEE.\\nO. Inbar and J. Meyer. 2019. Politeness counts: Perceptions of peacekeeping robots. IEEE Transactions\\non Human-Machine Systems, 49(3): 232–240.\\nitSeez3D, 2020. Turn your mobile device into a powerful 3d scanner. https://itseez3d.com/. https:\\n//itseez3d.com/.\\nY. Iwamura, M. Shiomi, T. Kanda, H. Ishiguro, and N. Hagita.\\n2011.\\nDo elderly people prefer a\\nconversational humanoid as a shopping assistant partner in supermarkets? In Proceedings of the 6th\\ninternational conference on Human-robot interaction, pp. 449–456.\\nD. Jennings and M. Figliozzi. 2019. Study of sidewalk autonomous delivery robots and their potential\\nimpacts on freight efﬁciency and travel. Transportation Research Record, 2673(6): 317–326.\\nH. W. Jensen, S. R. Marschner, M. Levoy, and P. Hanrahan. 2001. A practical model for subsurface\\nlight transport. In Proc. of SIGGRAPH, pp. 511–518.\\nJ. Jimenez, V. Sundstedt, and D. Gutierrez. 2009. Screen-space perceptual rendering of human skin.\\nACM Transactions on Applied Perception, 6(4): 23:1–23:15.\\nJ. Jimenez, T. Scully, N. Barbosa, C. Donner, X. Alvarez, T. Vieira, P. Matts, V. Orvalho, D. Gutierrez,\\nand T. Weyrich. 2010. A practical appearance model for dynamic facial color. ACM Transactions on\\nGraphics, 29(6): 141:1–141:10.\\nG. Johansson. 1973. Visual perception of biological motion and a model for its analysis. Perception &\\nPsychophysics, 14(2): 201–211.\\nS. Junuzovic, K. Inkpen, J. Tang, M. Sedlins, and K. Fisher.\\n10 2012.\\nTo see or not to see: A\\nstudy comparing four-way avatar, video, and audio conferencing for work.\\npp. 31–34.\\nDOI:\\n10.1145/2389176.2389181.\\nBIBLIOGRAPHY\\n33\\nJ. T. Kajiya and T. L. Kay. 1989. Rendering fur with three dimensional textures. SIGGRAPH Computer\\nGraphics, 23(3): 271–280.\\nA. Kalegina, G. Schroeder, A. Allchin, K. Berlin, and M. Cakmak. 2018. Characterizing the design\\nspace of rendered robot faces. In Proceedings of the 2018 ACM/IEEE International Conference on\\nHuman-Robot Interaction, pp. 96–104.\\nS. Kiesler, A. Powers, S. R. Fussell, and C. Torrey. 2008. Anthropomorphic interactions with a robot\\nand robot–like agent. Social Cognition, 26(2): 169–181.\\nK. Kilteni, R. Groten, and M. Slater. 11 2012. The sense of embodiment in virtual reality. Presence\\nTeleoperators and Virtual Environments, 21. DOI: 10.1162/PRES a 00124.\\nE. Kokkinara and R. McDonnell. 2015. Animation realism affects perceived character appeal of a\\nself-virtual face. In Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games, pp.\\n221–226. Acm.\\nH. Kozima, M. P. Michalowski, and C. Nakagawa. 2009. Keepon. International Journal of Social\\nRobotics, 1(1): 3–18.\\nL. T. Kozlowski and J. E. Cutting. 1977. Recognizing the sex of a walker from a dynamic point-light\\ndisplay. Perception & Psychophysics, 21(6): 575–580.\\nS. K¨uhn, T. R. Brick, B. C. M¨uller, and J. Gallinat. 2014. Is this car looking at you? how anthropomor-\\nphism predicts fusiform face area activation when seeing cars. PloS one, 9(12): e113885.\\nM. K. Lee, J. Forlizzi, P. E. Rybski, F. Crabbe, W. Chung, J. Finkle, E. Glaser, and S. Kiesler. 2009. The\\nsnackbot: documenting the design of a robot for long-term human-robot interaction. In Proceedings\\nof the 4th ACM/IEEE international conference on Human robot interaction, pp. 7–14.\\nM. K. Lee, S. Kiesler, and J. Forlizzi. 2010. Receptionist or information kiosk: how do people talk with\\na robot? In Proceedings of the 2010 ACM conference on Computer supported cooperative work, pp.\\n31–40.\\nJ. C. Lester, C. B. Callaway, B. Stone, and S. G. Towns. 1997. Mixed initiative problem solving with\\nanimated pedagogical agents. In Workshop on Pedagogical Agents, volume 19.\\nJ. Li, R. Kizilcec, J. Bailenson, and W. Ju.\\n2016.\\nSocial robots and virtual agents as lec-\\nturers for video instruction.\\nComputers in Human Behavior, 55: 1222 – 1230.\\nDOI:\\nhttps://doi.org/10.1016/j.chb.2015.04.005.\\nT. Liu, A. W. Bargteil, J. F. O’Brien, and L. Kavan. Nov. 2013. Fast simulation of mass-spring systems.\\nACM Transactions on Graphics, 32(6): 209:1–7.\\nhttp://cg.cis.upenn.edu/publications/Liu-FMS.\\nProceedings of ACM SIGGRAPH Asia 2013, Hong Kong.\\nLoom.ai, 2020. 3d avatar platform for enterprise and developers. https://loomai.com/. https://loomai.\\ncom/.\\nI. L¨utkebohle, F. Hegel, S. Schulz, M. Hackel, B. Wrede, S. Wachsmuth, and G. Sagerer. 2010. The\\nbielefeld anthropomorphic robot head “ﬂobi”. In 2010 IEEE International Conference on Robotics\\nand Automation, pp. 3384–3391. IEEE.\\nK. F. MacDorman, R. D. Green, C.-C. Ho, and C. T. Koch. 2009. Too real for comfort? Uncanny\\nresponses to computer generated faces. Computers in Human Behavior, 25(3): 695–710.\\nS. Marschner and P. Shirley. 2016. Fundamentals of Computer Graphics. CRC Press.\\nBIBLIOGRAPHY\\n34\\nS. R. Marschner, H. W. Jensen, M. Cammarano, S. Worley, and P. Hanrahan. 2003. Light scattering\\nfrom human hair ﬁbers. ACM Transaction on Graphics, 22(3): 780–791. ISSN 0730-0301.\\nT. Masson. 2007. CG 101: A Computer Graphics Industry Reference. Digital Fauxtography.\\nS. McCloud. 1993. Understanding comics: The invisible art. Northampton, Mass.\\nR. McDonnell, S. J¨org, J. K. Hodgins, F. Newell, and C. O’sullivan. 2009a. Evaluating the effect of\\nmotion and body shape on the perceived sex of virtual characters. ACM Transactions on Applied\\nPerception (TAP), 5(4): 20.\\nR. McDonnell, S. J¨org, J. McHugh, F. N. Newell, and C. O’Sullivan. 2009b. Investigating the role of\\nbody shape on the perception of emotion. ACM Transactions on Applied Perception (TAP), 6(3): 14.\\nR. McDonnell, M. Larkin, B. Hernandez, I. Rudomin, , and C. O’Sullivan. 2009c. Eye-catching Crowds:\\nsaliency based selective variation. ACM Transaction on Graphics, 28(3): 55:1 – 55:10.\\nR. McDonnell, M. Breidt, and H. H. B¨ulthoff. 2012. Render me real? Investigating the effect of render\\nstyle on the perception of animated virtual humans. ACM Transaction on Graphics, 31(4): 91:1–\\n91:11.\\nJ. E. Michaelis and B. Mutlu. 2018. Reading socially: Transforming the in-home reading experience\\nwith a learning-companion robot. Science Robotics, 3(21).\\nJ. Miseikis, P. Caroni, P. Duchamp, A. Gasser, R. Marko, N. Miseikiene, F. Zwilling, C. de Castelbajac,\\nL. Eicher, M. Fruh, et al. 2020. Lio–a personal robot assistant for human-robot interaction and care\\napplications. arXiv preprint arXiv:2006.09019.\\nM. Mori. 1970. The uncanny valley. Energy, 7(4): 33 – 35.\\nJ. Mumm and B. Mutlu. 2011. Designing motivational agents: The role of praise, social comparison,\\nand embodiment in computer feedback. Computers in Human Behavior, 27(5): 1643–1650.\\nB. Mutlu and J. Forlizzi. 2008. Robots in organizations: the role of workﬂow, social, and environmental\\nfactors in human-robot interaction. In 2008 3rd ACM/IEEE International Conference on Human-\\nRobot Interaction (HRI), pp. 287–294. IEEE.\\nK. Nagano, J. Seo, J. Xing, L. Wei, Z. Li, S. Saito, A. Agarwal, J. Fursund, H. Li, R. Roberts, et al.\\n2018. pagan: real-time avatars using dynamic textures. ACM Trans. Graph., 37(6): 258–1.\\nC. Nass, Y. Moon, and N. Green. 1997. Are machines gender neutral? gender-stereotypic responses to\\ncomputers with voices. Journal of applied social psychology, 27(10): 864–876.\\nF. E. Nicodemus, J. C. Richmond, J. J. Hsia, I. W. Ginsberg, and T. Limperis. 1992. Geometrical\\nconsiderations and nomenclature for reﬂectance. 160: 4.\\nS. Nishio, H. Ishiguro, and N. Hagita. 2007. Geminoid: Teleoperated android of an existing person.\\nHumanoid robots: New developments, 14: 343–352.\\nK. Nowak. 2001. The inﬂuence of anthropomorphism on social judgment in social virtual environments.\\nIn Annual Convention of the International Communication Association, Washington, DC.\\nJ.-H. Oh, D. Hanson, W.-S. Kim, Y. Han, J.-Y. Kim, and I.-W. Park. 2006. Design of android type\\nhumanoid robot albert hubo. In 2006 IEEE/RSJ International Conference on Intelligent Robots and\\nSystems, pp. 1428–1433. IEEE.\\nC. D. Onal, M. T. Tolley, R. J. Wood, and D. Rus. 2014. Origami-inspired printed robots. IEEE/ASME\\ntransactions on mechatronics, 20(5): 2214–2221.\\nBIBLIOGRAPHY\\n35\\nI. C. Orr. 1974. Puppet theatre in asia. Asian Folklore Studies, pp. 69–84.\\nA. G. Ozkil, Z. Fan, S. Dawids, H. Aanes, J. K. Kristensen, and K. H. Christensen. 2009. Service\\nrobots for hospitals: A case study of transportation tasks in a hospital. In 2009 IEEE international\\nconference on automation and logistics, pp. 289–294. IEEE.\\nN. A. Palomares and E.-J. Lee. 2010. Virtual gender identity: The linguistic assimilation to gendered\\navatars in computer-mediated communication. Journal of Language and Social Psychology, 29(1):\\n5–23. DOI: 10.1177/0261927X09351675.\\nY. Pan and A. Steed. 2016. A comparison of avatar-, video-, and robot-mediated interaction on users\\ntrust in expertise. Frontiers in Robotics and AI, 3: 12. DOI: 10.3389/frobt.2016.00012.\\nT. Pejsa, B. Mutlu, and M. Gleicher. 2013. Stylized and performative gaze for character animation.\\nComputer Graphics Forum, 32(2pt2): 143–152. https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.\\n12034. DOI: 10.1111/cgf.12034.\\nB. T. Phong. 1975. Illumination for computer generated pictures. Communications of ACM, 18(6):\\n311–317.\\nPinscreen, 2019.\\nThe most advanced ai-driven personalized avatars.\\nhttps://www.pinscreen.com/.\\nhttps://www.pinscreen.com/.\\nA. Powers and S. Kiesler. 2006. The advisor robot: tracing people’s mental model from a robot’s\\nphysical attributes. In Proceedings of the 1st ACM SIGCHI/SIGART conference on Human-robot\\ninteraction, pp. 218–225.\\nJ. Pransky. 2001. Aibo–the no. 1 selling service robot. Industrial robot: An international journal.\\nT. J. Prescott, B. Mitchinson, and S. Conran. 2017. Miro: An animal-like companion robot with a\\nbiomimetic brain-based control system. In Proceedings of the Companion of the 2017 ACM/IEEE\\nInternational Conference on Human-Robot Interaction, pp. 50–51.\\nD. Romano, C. Pfeiffer, A. Maravita, and O. Blanke. 3 2014. Illusory self-identiﬁcation with an avatar\\nreduces arousal responses to painful stimuli. Behavioural Brain Research, 261: 275–281. ISSN\\n01664328. DOI: 10.1016/j.bbr.2013.12.049.\\nK. Ruhland, K. Zibrek, and R. McDonnell. 2015. Perception of personality through eye gaze of realistic\\nand cartoon models. In Proc. of Symp. on Applied Perception, pp. 19–23. ACM.\\nS. Saito, L. Wei, L. Hu, K. Nagano, and H. Li. 2017. Photorealistic facial texture inference using\\ndeep neural networks.\\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pp. 5144–5153.\\nY. Sakagami, R. Watanabe, C. Aoyama, S. Matsunaga, N. Higaki, and K. Fujimura. 2002. The intelligent\\nasimo: System overview and integration. In IEEE/RSJ international conference on intelligent robots\\nand systems, volume 3, pp. 2478–2483. IEEE.\\nA. Saupp´e and B. Mutlu. 2015. The social impact of a robot co-worker in industrial settings. In\\nProceedings of the 33rd annual ACM conference on human factors in computing systems, pp. 3613–\\n3622.\\nA. P. Saygin, T. Chaminade, H. Ishiguro, J. Driver, and C. Frith. 2012. The thing that should not be:\\nPredictive coding and the Uncanny Valley in perceiving human and humanoid robot actions. Social\\nCognitive Affective Neuroscience, 7(4): 413–422.\\nJ. Scarce. 1983. Karagoz shadow puppets of turkey.\\nBIBLIOGRAPHY\\n36\\nJ. Sculley.\\n1989.\\nThe relationship between business and higher education: A perspective on the\\n21st century.\\nCommun. ACM, 32(9): 1056–1061.\\nhttps://doi.org/10.1145/66451.66452.\\nDOI:\\n10.1145/66451.66452.\\nJ. Seyama and R. S. Nagayama. 2007. The Uncanny Valley: Effect of realism on the impression of\\nartiﬁcial human faces. Presence: Teleoperators and Virtual Environments, 16(4): 337–351.\\nM. Seymour, C. Evans, and K. Libreri. 2017. Meet mike: epic avatars. In ACM SIGGRAPH 2017 VR\\nVillage, pp. 1–2.\\nM. Shayganfar, C. Rich, and C. L. Sidner. 2012. A design methodology for expressing emotion on robot\\nfaces. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 4577–4583.\\nIEEE.\\nM. Siegel, C. Breazeal, and M. I. Norton. 2009. Persuasive robotics: The inﬂuence of robot gender on\\nhuman behavior. In 2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp.\\n2563–2568. IEEE.\\nS. Simon, C. William, and G. Jan. 1999. Enlightened automata. In The Sciences in Enlightened Europe,\\nChicago and London. The University of Chicago Press.\\nM. Slater and A. Steed. 2002. Meeting people virtually: Experiments in shared virtual environments. In\\nThe Social Life of Avatars, pp. 146–171. Springer.\\nA. Steed, Y. Pan, F. Zisch, and W. Steptoe. 2016. The impact of a self-avatar on cognitive load in\\nimmersive virtual reality. pp. 67–76. DOI: 10.1109/VR.2016.7504689.\\nE. M. Suzanne R. Pallak and J. Koch. 1983. Communicator attractiveness and expertise, emotional\\nversus rational appeals, and persuasion: A heuristic versus systematic processing interpretation.\\nSocial Cognition, 2(2): 122–141.\\nD. Szaﬁr, B. Mutlu, and T. Fong. 2015. Communicating directionality in ﬂying robots. In 2015 10th\\nACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 19–26. IEEE.\\nJ. Thies, M. Zollh¨ofer, M. Stamminger, C. Theobalt, and M. Nießner. 2016. Face2face: Real-time face\\ncapture and reenactment of rgb videos. In Proceedings of the IEEE conference on computer vision\\nand pattern recognition, pp. 2387–2395.\\nF. Thomas and O. Johnston. 1995. The illusion of life: Disney animation. Hyperion New York.\\nA. Tinwell, M. Grimshaw, D. A. Nabi, and A. Williams. 2011. Facial expression of emotion and\\nperception of the Uncanny Valley in virtual characters. Computers in Human Behavior, 27(2): 741–\\n749.\\nI. Torre, E. Carrigan, K. McCabe, R. McDonnell, and N. Harte. 2018. Survival at the museum: A\\ncooperation experiment with emotionally expressive virtual characters. In Proceedings of the 2018\\non International Conference on Multimodal Interaction, pp. 423–427. ACM.\\nI. Torre, E. Carrigan, R. McDonnell, K. Domijan, K. McCabe, and N. Harte. 2019. The effect of mul-\\ntimodal emotional expression and agent appearance on trust in human-agent interaction. In Motion,\\nInteraction and Games, MIG ’19. Association for Computing Machinery, New York, NY, USA. ISBN\\n9781450369947. https://doi.org/10.1145/3359566.3360065. DOI: 10.1145/3359566.3360065.\\nI. P. Tussyadiah and S. Park. 2018. Consumer evaluation of hotel service robots. In Information and\\ncommunication technologies in tourism 2018, pp. 308–320. Springer.\\nBIBLIOGRAPHY\\n37\\nA. van Breemen, X. Yan, and B. Meerbeek. 2005. icat: an animated user-interface robot with personality.\\nIn Proceedings of the fourth international joint conference on Autonomous agents and multiagent\\nsystems, pp. 143–144.\\nA. J. van Breemen.\\n2004.\\nAnimation engine for believable interactive user-interface robots.\\nIn\\n2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No.\\n04CH37566), volume 3, pp. 2873–2878. IEEE.\\nG. Veletsianos. Sept. 2010. Contextually relevant pedagogical agents: Visual appearance, stereotypes,\\nand ﬁrst impressions and their impact on learning. Comput. Educ., 55(2): 576–585. ISSN 0360-1315.\\nhttps://doi.org/10.1016/j.compedu.2010.02.019. DOI: 10.1016/j.compedu.2010.02.019.\\nV. Vinayagamoorthy, M. Gillies, A. Steed, E. Tanguy, X. Pan, C. Loscos, and M. Slater. 2006. Building\\nExpression into Virtual Characters. In B. Wyvill and A. Wilkie, eds., Eurographics 2006 - State of\\nthe Art Reports. The Eurographics Association. DOI: 10.2312/egst.20061052.\\nM. Volante, S. V. Babu, H. Chaturvedi, N. Newsome, E. Ebrahimi, T. Roy, S. B. Daily, and T. Fasolino.\\n2016. Effects of virtual human appearance ﬁdelity on emotion contagion in affective inter-personal\\nsimulations. IEEE Transaction on Visualization and Computer Graphics, 22(4): 1326–1335.\\nK. Wada and T. Shibata. 2007. Living with seal robots—its sociopsychological and physiological\\ninﬂuences on the elderly at a care house. IEEE transactions on robotics, 23(5): 972–980.\\nK. Wada, T. Shibata, T. Saito, K. Sakamoto, and K. Tanie. 2005. Psychological and social effects of one\\nyear robot assisted activity on elderly people at a health service facility for the aged. In Proceedings\\nof the 2005 IEEE international conference on robotics and automation, pp. 2785–2790. IEEE.\\nM. Walker, H. Hedayati, J. Lee, and D. Szaﬁr.\\n2018.\\nCommunicating robot motion intent with\\naugmented reality. In Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot\\nInteraction, pp. 316–324.\\nC. Wallraven, H. H. B¨ulthoff, D. W. Cunningham, J. Fischer, and D. Bartz. 2007. Evaluation of real-\\nworld and computer-generated stylized facial expressions. ACM Transactions on Applied Perception,\\n4(3).\\nY. Wang, J. Geigel, and A. Herbert. 2013. Reading personality: Avatar vs. human faces. In Humaine\\nAssociation Conference on Affective Computing and Intelligent Interaction, pp. 479–484.\\nDOI:\\n10.1109/ACII.2013.85.\\nM. Watanabe, K. Ogawa, and H. Ishiguro. 2015. Can androids be salespeople in the real world? In\\nProceedings of the 33rd annual ACM conference extended abstracts on human factors in computing\\nsystems, pp. 781–788.\\nD. Whitley. 2012. The idea of nature in Disney animation: From Snow White to WALL-E. Ashgate\\nPublishing, Ltd.\\nM. Wise, M. Ferguson, D. King, E. Diehr, and D. Dymesich. 2016. Fetch and freight: Standard platforms\\nfor service robot applications. In Workshop on autonomous mobile service robots.\\nP. Wisessing, K. Zibrek, D. W. Cunningham, J. Dingliana, and R. McDonnell. Apr. 2020. Enlighten me:\\nImportance of brightness and shadow for character emotion and appeal. ACM Trans. Graph., 39(3).\\nISSN 0730-0301. https://doi.org/10.1145/3383195. DOI: 10.1145/3383195.\\nY. Yamada, T. Kawabe, and K. Ihaya. 01 2013. Categorization difﬁculty is associated with negative\\nevaluation in the “uncanny valley” phenomenon. Japanese Psychological Research, 55: 20–32. DOI:\\n10.1111/j.1468-5884.2012.00538.x.\\nBIBLIOGRAPHY\\n38\\nS. Yamaguchi, S. Saito, K. Nagano, Y. Zhao, W. Chen, K. Olszewski, S. Morishima, and H. Li.\\n2018. High-ﬁdelity facial reﬂectance and geometry inference from an unconstrained image. ACM\\nTransactions on Graphics (TOG), 37(4): 1–14.\\nB. M. Yamauchi. 2004. Packbot: a versatile platform for military robotics. In Unmanned ground vehicle\\ntechnology VI, volume 5422, pp. 228–237. International Society for Optics and Photonics.\\nN. Yee and J. Bailenson. 07 2007. The proteus effect: The effect of transformed self-representation on\\nbehavior. Human Communication Research, 33: 271 – 290. DOI: 10.1111/j.1468-2958.2007.00299.x.\\nN. Yee and J. N. Bailenson. 2009. The difference between being and seeing: The relative contribution of\\nself-perception and priming to behavioral changes via digital self-representation. Media Psychology,\\n12(2): 195–209. DOI: 10.1080/15213260902849943.\\nN. Yee, N. Ducheneaut, M. Yao, and L. Nelson. 05 2011. Do men heal more when in drag? conﬂicting\\nidentity cues between user and avatar. pp. 773–776. DOI: 10.1145/1978942.1979054.\\nY. Yokota. 2009. A historical overview of japanese clocks and karakuri. In International Symposium on\\nHistory of Machines and Mechanisms, pp. 175–188. Springer.\\nE. Zell, C. Aliaga, A. Jarabo, K. Zibrek, D. Gutierrez, R. McDonnell, and M. Botsch. 2015. To stylize or\\nnot to stylize?: The effect of shape and material stylization on the perception of computer-generated\\nfaces. ACM Transactions on Graphics, 34(6): 184:1–184:12.\\nE. Zell, K. Zibrek, and R. McDonnell. 2019. Perception of virtual characters. In ACM SIGGRAPH\\n2019 Courses, SIGGRAPH ’19. Association for Computing Machinery, New York, NY, USA. ISBN\\n9781450363075. https://doi.org/10.1145/3305366.3328101. DOI: 10.1145/3305366.3328101.\\nK. Zibrek and R. McDonnell. 2019. Social presence and place illusion are affected by photorealism in\\nembodied vr. In Motion, Interaction and Games, MIG ’19. Association for Computing Machinery,\\nNew York, NY, USA.\\nISBN 9781450369947.\\nhttps://doi.org/10.1145/3359566.3360064.\\nDOI:\\n10.1145/3359566.3360064.\\nK. Zibrek, L. Hoyet, K. Ruhland, and R. McDonnell. 2015. Exploring the effect of motion type and\\nemotions on the perception of gender in virtual humans. ACM Transactions on Applied Perception\\n(TAP), 12(3): 11.\\n'),\n",
              " Document(metadata={'Published': '2024-03-05', 'Title': 'Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention', 'Authors': 'Min Yang, Guanjun Liu, Ziyuan Zhou', 'Summary': \"Traditional multi-agent reinforcement learning algorithms are difficultly\\napplied in a large-scale multi-agent environment. The introduction of mean\\nfield theory has enhanced the scalability of multi-agent reinforcement learning\\nin recent years. This paper considers partially observable multi-agent\\nreinforcement learning (MARL), where each agent can only observe other agents\\nwithin a fixed range. This partial observability affects the agent's ability to\\nassess the quality of the actions of surrounding agents. This paper focuses on\\ndeveloping a method to capture more effective information from local\\nobservations in order to select more effective actions. Previous work in this\\nfield employs probability distributions or weighted mean field to update the\\naverage actions of neighborhood agents, but it does not fully consider the\\nfeature information of surrounding neighbors and leads to a local optimum. In\\nthis paper, we propose a novel multi-agent reinforcement learning algorithm,\\nPartially Observable Mean Field Multi-Agent Reinforcement Learning based on\\nGraph--Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention\\nmodule and a mean field module to describe how an agent is influenced by the\\nactions of other agents at each time step. This graph attention module consists\\nof a graph attention encoder and a differentiable attention mechanism, and this\\nmechanism outputs a dynamic graph to represent the effectiveness of\\nneighborhood agents against central agents. The mean--field module approximates\\nthe effect of a neighborhood agent on a central agent as the average effect of\\neffective neighborhood agents. We evaluate GAMFQ on three challenging tasks in\\nthe MAgents framework. Experiments show that GAMFQ outperforms baselines\\nincluding the state-of-the-art partially observable mean-field reinforcement\\nlearning algorithms.\", 'entry_id': 'http://arxiv.org/abs/2304.12653v2', 'published_first_time': '2023-04-25', 'comment': None, 'journal_ref': 'Drones 2023, 7(7), 476', 'doi': '10.3390/drones7070476', 'primary_category': 'cs.AI', 'categories': ['cs.AI'], 'links': ['http://dx.doi.org/10.3390/drones7070476', 'http://arxiv.org/abs/2304.12653v2', 'http://arxiv.org/pdf/2304.12653v2']}, page_content='Received: Added at production\\nRevised: Added at production\\nAccepted: Added at production\\nDOI: https://doi.org/10.3390/drones7070476\\nARTICLE TYPE\\nPartially Observable Mean Field Multi-Agent Reinforcement\\nLearning Based on Graph–Attention\\nMin Yang | Guanjun Liu* | Ziyuan Zhou\\n1Department of Computer Science, Tongji\\nUniversity, Shanghai, China\\nSummary\\nTraditional multi-agent reinforcement learning algorithms are difficultly applied in\\na large-scale multi-agent environment. The introduction of mean field theory has\\nenhanced the scalability of multi-agent reinforcement learning in recent years. This\\npaper considers partially observable multi-agent reinforcement learning (MARL),\\nwhere each agent can only observe other agents within a fixed range. This partial\\nobservability affects the agent’s ability to assess the quality of the actions of sur-\\nrounding agents. This paper focuses on developing a method to capture more effective\\ninformation from local observations in order to select more effective actions. Pre-\\nvious work in this field employs probability distributions or weighted mean field\\nto update the average actions of neighborhood agents, but it does not fully con-\\nsider the feature information of surrounding neighbors and leads to a local optimum.\\nIn this paper, we propose a novel multi-agent reinforcement learning algorithm,\\nPartially Observable Mean Field Multi-Agent Reinforcement Learning based on\\nGraph–Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention\\nmodule and a mean field module to describe how an agent is influenced by the\\nactions of other agents at each time step. This graph attention module consists of a\\ngraph attention encoder and a differentiable attention mechanism, and this mecha-\\nnism outputs a dynamic graph to represent the effectiveness of neighborhood agents\\nagainst central agents. The mean–field module approximates the effect of a neigh-\\nborhood agent on a central agent as the average effect of effective neighborhood\\nagents. We evaluate GAMFQ on three challenging tasks in the MAgents framework.\\nExperiments show that GAMFQ outperforms baselines including the state-of-the-art\\npartially observable mean-field reinforcement learning algorithms.\\nsectionAbstract\\nKEYWORDS:\\nGraph–Attention, Multi-agent reinforcement learning, Mean field theory, Partial observation\\n1\\nINTRODUCTION\\nReinforcement learning has been widely used in video games [26] and recently in education [7]. For multi-agent reinforce-\\nment learning (MARL) [33], it involves multiple autonomous agents that make autonomous decisions to accomplish some\\nspecific competitive or cooperative tasks by maximizing global reward, it has been applied in some real-world scenarios such as\\narXiv:2304.12653v2  [cs.AI]  5 Mar 2024\\n2\\nautonomous mobile [21] drone swarm confrontation[1] and multi-UAV collaboratively delivering goods [22]. For example, in\\nsome of the drone swarm adversarial tasks, drones need to make actions based on autonomous decisions. Due to the inevitable\\ndeath of some drones in the confrontation environment [38], the surviving drones must constantly evolve their strategies in\\nreal-time during the interaction with the environment to obtain the overall maximum reward. In order to make better interaction\\namong agents, it is required that each agent in the multi-agent system can effectively perceive environmental information and\\nfully acquire the information of surrounding agents.\\nHowever, the global communication cost among multiple agents is high, and in many practical tasks, each agent only observes\\npart of the environmental information. Take the task of Autonomous Driving as an example, each vehicle makes decision\\nin the limited observation space which is a typical local observation scene. Each agent can only rely on limited observation\\ninformation in the local observation environment, therefore the agent needs to learn a decentralized strategy. There are two\\ncommon decentralization strategies. One is Centralized Training and Decentralized Execution (CTDE), which requires agents\\nto communicate with each other during training and to independently make decisions based on their own observations during\\ntesting in order to adapt to large-scale multi-agent environments. Some classic algorithms using the CTDE framework such\\nas MADDPG [15], QMIX [19] and MAVEN [16]. Another one takes the policy of decentralized training and decentralized\\nexecution, in which each agent can only observe part of the information during the training and testing phases, which is closer\\nto the real environment with limited communication. Especially large-scale multi-agent environments are complex and non-\\nstationary [10], it is difficult for agents to observe the entire environment globally, limiting their ability to find the best actions.\\nFurthermore, as the number of agents increases, joint optimization of all information in a multi-agent environment may result\\nin a huge joint state-action space, which also brings scalability challenges. This paper focuses on the second strategy.\\nTraditional multi-agent reinforcement learning algorithms are difficult to be applied in large-scale multi-agent environments,\\nespecially when the number of agents is exponential. Recent studies address the scalability issues of multi-agent reinforcement\\nlearning [31, 30, 12] by introducing mean-field theory, i.e., the multi-agent problem is reduced to a simple two-agent problem.\\nHowever, Yang et al. [31] assumes that each agent can observe global information, which is difficult to apply in some real tasks.\\nTherefore, it is necessary to study large-scale multi-agent reinforcement learning algorithms in partially observable cases [3].\\nIn addition, researchers have intensively studied mean-field-based multi-agent reinforcement learning algorithms to improve\\nperformance in partially observable cases. One way is to further decompose the Q-function of the mean field-based multi-\\nagent reinforcement learning algorithm [34, 6]. Another way uses probability distribution or weighted mean field to update the\\nmean action of neighborhood agents [5, 37, 23, 28]. Hao [8] combined the graph attention with the mean field to calculate the\\ninteraction strength between agents when agents interact, but only considered the scene where the agent has a fixed relative\\nposition, and the agents can observe the global information. The difference is that when the agent is partially observable, we\\nconsider the dynamic change of the agent’s position and the death scene of the agent, and construct a more flexible partial\\nobservable graph attention network based on the mean field.\\nHowever, for partially observable multi-agent mean field reinforcement learning, the existing methods do not fully consider\\nthe feature information of the surrounding neighbors, which will lead to falling into local optimum. This paper focuses on\\nidentifying the neighborhood agents that may have the greater influence on the central agent in a limited observation space,\\nin order to avoid the local optimum issue. Since the graph neural network [29] can fully aggregate the relationship between\\nthe central agent and its surrounding neighbors, we propose a graph attention-based mechanism to calculate the importance of\\nneighbor agents to estimate the average action more efficiently.\\nThe main contributions of this paper are as follows:\\n• We propose a partially observable mean–field reinforcement learning based on the graph–attention (GAMFQ), which can\\nlearn a decentralized agent policy from an environment without requiring global information of an environment. In the\\ncase of partially observable large-scale agents, the judgment of the importance of neighbor agents is insufficient in our\\nGAMFQ.\\n• We theoretically demonstrate that the settings of the GAMFQ algorithm are close to Nash equilibrium.\\n• Experiments on three challenging tasks in the MAgents framework show that GAMFQ outperforms two baseline\\nalgorithms as well as the state-of-the-art partially observable mean-field reinforcement learning algorithms.\\n3\\n2\\nRELATED WORK\\nMost of the recent MARL algorithms for partial observability research are model-free reinforcement learning algorithms based\\non the CTDE framework. The most classic algorithm MADDPG [15] introduces critics that can observe global information\\nin training to guide actor training, but only use actors with local observation information to take actions in testing. QMIX[19]\\nuses a hybrid network to combine the local value functions of a single agent, and adds global state information assistance in the\\ntraining and learning process to improve the performance of the algorithm. MAVEN [16] is able to solve complex multi-agent\\ntasks by introducing latent spaces for hierarchical control by value-mixing and policy-based approaches. However, these multi-\\nagent reinforcement learning algorithm using the CTDE framework is difficult to scale to large-scale multi-agent environments,\\nbecause there will be hard-to-observe global information that prevents the agents from training better policies.\\nFor large-scale multi-agent environments, Yang et al. [31] introduced the mean–field theory, which approximates the inter-\\naction of many agents as the interaction between the central agent and the average effects from neighboring agents. However,\\npartially observed multi-agent mean–field reinforcement learning algorithms still have a space to improve. Some researchers\\nfurther decompose the Q-function of the mean field based multi-agent reinforcement learning algorithm. Zhang et al. [34]\\ntrained agents through the CTDE paradigm, transforming each agent’s Q-function into its local Q-function and its mean field\\nQ-function, but this approach is not strictly partially observable. Gu et al. [6] proposes a mean field multi-agent reinforcement\\nlearning algorithm with local training and decentralized execution. The Q-function is decomposed by grouping the observable\\nneighbor states of each agent in a multi-agent system, so that the Q-function can be updated locally. In addition, some researchers\\nhave focused on improving the mean action in mean field reinforcement learning. Fang et al. [5] adds the idea of mean field to\\nMADDPG, and proposes a multi-agent reinforcement learning algorithm based on weighted mean field, so that MADDPG can\\nadapt to large-scale multi-agent environment. Wang et al. [28] propose a weighted mean-field multi-agent reinforcement learn-\\ning algorithm based on reward attribution decomposition by approximating the weighted mean field as a joint optimization of\\nimplicit reward distribution between a central agent and its neighbors. Zhou et al. [37] uses the average action of neighbor agents\\nas a label, and trained a mean field prediction network to replace the average action. Subramanian et al. [23] proposed two multi-\\nagent mean field reinforcement learning algorithms based on partially observable settings: POMFQ(FOR) and POMFQ(PDO),\\nextracting partial samples from Dirichlet or Gamma distribution to estimate partial observable mean action. Although these\\nmethods achieve good results, they do not fully consider the feature information of surrounding neighbors.\\nGraph Neural Networks (GNNs) are able to mine graph structures from data for learning. In multi-agent reinforcement learn-\\ning, GNNs can be used to model interactions between agents. In recent work, graph attention mechanisms have been used for\\nmulti-agent reinforcement learning. Zhang et al. [32] integrated the importance of the information of surrounding agents based\\non the multi-head attention mechanism, effectively integrate the key information of the graph to represent the environment and\\nimprove the cooperation strategy of agents with the help of multi-agent reinforcement learning. DCG [2] decomposed the joint\\nvalue function of all agents into gains between pairs of agents according to the coordination graph, which can flexibly balance\\nthe performance and generalization ability of agents. Li et al. [13] proposed a deep implicit coordination graph (DICG) struc-\\nture that can adapt to dynamic environments and learn implicit reasoning about joint actions or values through graph neural\\nnetworks. Ruan et al. [20] proposed a graph-based coordination strategy, which decomposes the joint team strategy into a graph\\ngenerator and a graph-based coordination strategy to realize the coordination behavior between agents. MAGIC [17] more accu-\\nrately represented the interactions between agents during communication by modifying the standard graph attention network\\nand compatible with differentiable directed graphs.\\nIn the dynamic MARL system where competition and confrontation coexist, it is very difficult to directly apply the graph\\nneural network, because the agent will die, the graph structure of the constructed large-scale agent system has the problem of\\nlarge spatial dimension. However, graph neural networks can better mine the relationship between features, and the introduction\\nof mean-field theory can further improve the advantages of mean-field multi-agent reinforcement learning.\\nOur approach differs from related work above in that it uses a graph attention mechanism to select surrounding agents that\\nare more important to the central agent in a partially observable environment. GAMFQ uses a graph attention module and\\na mean field module to describe how an agent is influenced by the actions of other agents at each time step, where graph\\nattention consists of a graph attention encoder and a differentiable attention mechanism, and finally outputs a dynamic graph to\\nrepresent the effectiveness of the neighborhood agent to the central agent. The mean field module approximates the influence of\\na neighborhood agent on a central agent as the average influence of the effective neighborhood agents. Using these two modules\\ntogether is able to efficiently estimate the average action of surrounding agents in partially observable situations. GAMFQ does\\nnot require global information about the environment to learn decentralized agent policies from the environment.\\n4\\n3\\nMOTIVATION & PRELIMINARIES\\nIn this section, we represent discrete-time non-cooperative multi-agent task modeling as a stochastic game (SG). SG can be\\ndefined as a tuple < 𝑆, 𝐴1, … , 𝐴𝑁, 𝑟1, … , 𝑟𝑁, 𝑝, 𝛾>, where 𝑆represents the true state of the environment. Each agent 𝑗∈\\n{1, … , 𝑁} chooses an action at each time step 𝑎𝑗∈𝐴𝑗. The reward function for agent 𝑗is 𝑟𝑗∶𝑆× 𝐴1 × ⋯× 𝐴𝑁→𝑅.\\nState transitions are dynamically represented as 𝑝∶𝑆× 𝐴1 × ⋯× 𝐴𝑁→Ω(𝑆) . 𝛾is a constant representing the discount\\nfactor. It represents a stable state, and in this stable state, all agents will not deviate from the best strategy given to others. The\\ndisadvantage is that it cannot be applied to the coexistence of multiple agents. Yang et al. [31] introduced mean field theory, which\\napproximates the interaction of many agents as the interaction between the average effect of a central agent and neighboring\\nagents, and solves the scalability problem of SG.\\nThe Nash equilibrium of general and random games can be defined as a strategy tuple (𝜋1\\n∗, ⋯, 𝜋𝑁\\n∗\\n), for all 𝑠∈𝑆and\\n∀𝜋𝑖∈Π𝑖, there is 𝑣𝑗(𝑠, 𝜋1\\n∗, ⋯, 𝜋𝑖\\n∗, ⋯, 𝜋𝑁\\n∗\\n) ≥𝑣𝑗(𝑠, 𝜋1\\n∗, ⋯, 𝜋𝑖, ⋯, 𝜋𝑁\\n∗\\n). This shows that when all other agents are implementing\\ntheir equilibrium strategy, no one agent will deviate from this equilibrium strategy and receive a strictly higher reward. When all\\nagents follow the Nash equilibrium strategy, the Nash Q-function of agent 𝑗is 𝑄𝑗\\n∗(𝑠, 𝑎). Partially observable stochastic games can\\ngenerate a partially observable Markov decision process (POMDP), we review the partially observable Markov decision (Dec-\\nPOMDP) in Section 3.1 and analyze the partially observable model from a theoretical perspective. Section 3.2 first introduces\\nthe globally observable mean-field multi-agent reinforcement learning, and then introduces the partially observable mean-field\\nreinforcement learning algorithm (POMFQ) based on the POMDP framework, and analyzes the existing part of the observable\\nin detail. The limitation of mean-field reinforcement learning POMFQ(FOR)[23] is that the feature information of surrounding\\nneighbors is not fully considered. In a partially observable setting, each agent 𝑗observable neighborhood agent information\\n𝑜𝑗can be used to better mine the relationship between features through a graph attention network. Introducing graph attention\\nnetworks into partially observable mean-field multi-agent reinforcement learning can further improve their performance, and\\nSection 3.3 briefly introduces graph attention networks.\\n3.1\\nPartially observable Markov decision process\\nWe mainly study partially observable Markov decisions (Dec-POMDP) [3, 18, 33]. The partially observable Markov decision\\nprocess of 𝑛agents can be represented as a tuple ⟨𝑁, 𝑆, {𝐴𝑖}𝑛\\n𝑖=1 , 𝑇, 𝑍, 𝑅, 𝑂, 𝛾⟩, where 𝑁= {1, … , 𝑛} represents the set of\\nagents, 𝑆represents the global state, 𝐴𝑗represents the set of action spaces of the 𝑗-th agent, 𝑍represents the observation space\\nof the agents, and the agent 𝑗receives observation 𝑜𝑗∈𝑂𝑗through the observation function 𝑍(𝑠, 𝑗) ∶𝑆× 𝑁→𝑂, and the\\ntransition function 𝑇∶𝑆× 𝐴1 × … × 𝐴𝑛× 𝑆\\ue0b6→[0, 1] represents the environment transitions from a state to another one. At\\neach time step 𝑡, the agent 𝑗chooses an action 𝑎𝑗\\n𝑡∈𝐴𝑗, gets a reward 𝑟𝑗\\n𝑡∶𝑆× 𝐴𝑗\\ue0b6→𝑅w.r.t. a state and an action. 𝛾∈[0, 1]\\nis a reward discount factor. Agent 𝑗has a stochastic policy 𝜋𝑗conditioned on its observation 𝑜𝑗or action observation history\\n𝜏𝑗∈(𝑍× 𝐴𝑗), and according to the all agents’s joint policy 𝜋\\nΔ= [𝜋1, … , 𝜋𝑁], The value function of agent 𝑗under the joint\\nstrategy 𝜋is the value function 𝑣𝑗\\n𝜋(𝑠) = ∑∞\\n𝑡=0 𝛾𝑡𝐸𝜋,𝑝\\n[𝑟𝑗\\n𝑡|𝑠0 = 𝑠] can be obtained, and then the Q-function can be formalized as\\n𝑄𝑗\\n𝜋(𝑠, 𝑎) = 𝑟𝑗(𝑠, 𝑎) + 𝛾𝐸𝑠′∼𝑝\\n[𝑣𝑗\\n𝜋\\n(𝑠′)]. Our work is based on the POMDP framework.\\n3.2\\nPartially Observable Mean Field Reinforcement Learning\\nMean-field theory-based reinforcement learning algorithm [31] approximates interactions among multiple agents as two-agent\\ninteractions, where the second agent corresponds to the average effect of all other agents. Yang et al. [31] decomposes the\\nmulti-agent Q-function into pairwise interacting local Q-functions as follows:\\n𝑄𝑗\\n𝜋(𝑠, 𝑎) = 1\\n𝑁𝑗\\n∑\\n𝑘∈𝑁(𝑗)\\n𝑄𝑗\\n𝜋\\n(𝑠, 𝑎𝑗, 𝑎𝑘)\\n(1)\\nwhere 𝑁𝑗is the index set of the neighbors of the agent 𝑗and 𝑎𝑗represents the discrete action of the agent 𝑗and is represented\\nby one-shot coding. Mean field Q-function is cyclically updated according to Eq.2-5:\\n𝑄𝑗\\n𝜋\\n(𝑠𝑡, 𝑎𝑗\\n𝑡, ̄𝑎𝑗\\n𝑡\\n) = (1 −𝛼)𝑄𝑗\\n𝜋\\n(𝑠𝑡, 𝑎𝑗\\n𝑡, ̄𝑎𝑗\\n𝑡\\n) + 𝛼[𝑟𝑗\\n𝑡+ 𝛾𝑣𝑗(𝑠𝑡+1\\n)]\\n(2)\\n5\\nwhere\\n𝑣𝑗(𝑠𝑡+1\\n)=\\n∑\\n𝑎𝑗\\n𝑡+1\\n𝜋𝑗(𝑎𝑗\\n𝑡+1 ∣𝑠𝑡+1, ̃𝑎𝑗\\n𝑡\\n) 𝑄𝑗\\n𝜋\\n(𝑠𝑡+1, 𝑎𝑗\\n𝑡+1, ̃𝑎𝑗\\n𝑡\\n)\\n(3)\\n̄𝑎𝑗\\n𝑡= 1\\n𝑁\\n∑\\n𝑘≠𝑗\\n𝑎𝑘\\n𝑡, 𝑎𝑘\\n𝑡∼𝜋𝑘(⋅∣𝑠𝑡, ̄𝑎𝑘\\n𝑡−1\\n)\\n(4)\\n𝜋𝑗(𝑎𝑗\\n𝑡∣𝑠𝑡, ̄𝑎𝑗\\n𝑡−1\\n)=\\nexp (−𝛽𝑄𝑗\\n𝜋\\n(𝑠𝑡, 𝑎𝑗\\n𝑡, ̄𝑎𝑗\\n𝑡−1\\n))\\n∑\\n𝑎𝑗′\\n𝑡∈𝐴𝑗\\nexp\\n(\\n−𝛽𝑄𝑗\\n𝜋\\n(\\n𝑠𝑡, 𝑎𝑗′\\n𝑡, ̄𝑎𝑗\\n𝑡−1\\n))\\n(5)\\nwhere ̄𝑎𝑗\\n𝑡is the mean action of the neighborhood agent, 𝑟𝑗\\n𝑡is the reward for agent 𝑗at time step 𝑡, 𝑣𝑗is the value function of agent\\n𝑗, and 𝛽is the Boltzmann parameter. Literature [31] assumes that each agent has global information, and for the central agent,\\nthe average action of the neighboring agents is updated by Eq. 4. However, in a partially observable multi-agent environment,\\nthe way of calculating the average action in Eq. 4 is no longer applicable.\\nIn the case of partial observability, Subramanian et al. [23] take 𝑈samples from the Dirichlet distribution to update the average\\naction of Eq. 4, and achieve better performance than the mean field reinforcement learning algorithm. The formula is as follows:\\n𝐷𝑗(𝜃) ∝𝜃𝜂1−1+𝑐1\\n1\\n⋯𝜃𝜂𝐿−1+𝑐𝐿\\n𝐿\\n;\\ñ𝑎𝑗\\n𝑖,𝑡∼𝐷𝑗(𝜃; 𝜂+ 𝑐); ̃𝑎𝑗\\n𝑡= 1\\n𝑈\\n𝑖=𝑈\\n∑\\n𝑖=1\\ñ𝑎𝑗\\n𝑖,𝑡\\n(6)\\nwhere 𝐿denotes the size of the action space, 𝑐1, … , 𝑐𝐿denotes the number of occurrences of each action, 𝜂is the Dirichlet\\nparameter, 𝜃is the classification distribution. But the premise of the Dirichlet distribution is to assume that the characteristics of\\neach agent are independent to achieve better clustering based on the characteristics of neighboring agents. In fact, in many multi-\\nagent environments, the characteristics of each agent has a certain correlation, but the Dirichlet distribution does not consider\\nthis correlation, which makes it unable to accurately describe the central agent and the neighborhood agents. There will be some\\ndeviations in the related information. Figure 1 shows the process of a battle between the red and green teams, in which each\\nagent can observe the information of the friendly agent, and the action space of the agent is {𝑢𝑝, 𝑑𝑜𝑤𝑛, 𝑙𝑒𝑓𝑡, 𝑟𝑖𝑔ℎ𝑡}. The central\\nagent enclosed by the red circle is affected by the surrounding friendly agents. We use the Dirichlet distribution to simulate and\\ncalculate the probability of the central agent moving in each direction, as shown below:\\n⎧\\n⎪\\n⎪\\n⎨\\n⎪\\n⎪⎩\\n𝑝𝑢𝑝= 0.31\\n𝑝𝑑𝑜𝑤𝑛= 0.42\\n𝑝𝑙𝑒𝑓𝑡= 0.14\\n𝑝𝑟𝑖𝑔ℎ𝑡= 0.13\\n(7)\\nIt can be obtained that the probability of the agent moving down is the highest, which is essentially due to the large number\\nof agents moving 𝑑𝑜𝑤𝑛. However, moving 𝑢𝑝is the optimal action for the agent to form an encirclement trend with friends. The\\nDirichlet distribution results in a local optimal solution rather than finding the optimal action.\\nZhang et al. [35] believes that the correlation between two agents is crucial for multi-agent reinforcement learning. First, the\\npaper calculates the correlation coefficient between each pair of agents, and then shields the communication among weakly cor-\\nrelated agents, thereby reducing the dimensionality of the state-action value network in the input space. Inspired by Zhang et\\nal. [35], for large-scale partially observable multi-agent environments, it is more necessary to select the importance of neigh-\\nborhood agents. In our paper, we will adopt a graph attention method to filter out more important neighborhood agents, discard\\nunimportant agent information, and achieve more accurate estimation of the average actions of neighborhood agents.\\n3.3\\nGraph Attention Network\\nGraph neural network [29] can better mine the graph structure form between data. Graph Attention Network (GAT) [25] is\\ncomposed of a group of graph attention layers, each graph attention layer acts on the node feature vector of node 𝑖denoting as\\n𝑚𝑖through a weight matrix 𝑊, and then uses softmax to normalize the neighbor nodes of the central node:\\n𝑒𝑖𝑗= (𝑊𝑚𝑖‖𝑊𝑚𝑗)\\n(8)\\n6\\nleft\\n0.42\\n0.31\\n0.13\\n0.14\\nup\\nright\\ndown\\nFIGURE 1 A battle environment of the red and blue groups, where the red agent in the center is distributed by Dirichlet to\\ncalculate the action.\\n𝛼𝑖𝑗= softmax𝑗(𝑒𝑖𝑗\\n) =\\nexp (𝑒𝑖𝑗\\n)\\n∑\\n𝑘∈𝑁𝑗\\nexp (𝑒𝑗𝑘\\n)\\n(9)\\nwhere 𝑒𝑖𝑗is the attention coefficient of each node, indicating the importance of node 𝑖to node 𝑗. Finally, the output features are\\nobtained by weighting the input features ℎ𝑖, and the update rule for each node 𝑗is:\\n𝑒𝑗= 𝜎\\n⎛\\n⎜\\n⎜⎝\\n∑\\n𝑖∈𝑁𝑗\\n𝛼𝑖𝑗𝑊ℎ𝑖\\n⎞\\n⎟\\n⎟⎠\\n(10)\\nwhere 𝑒𝑗represents the feature of node 𝑗, 𝑁𝑗is the set of adjacent nodes of node 𝑗, and 𝜎(⋅) is a nonlinear activation function.\\n4\\nAPPROACH\\nIn this section, we propose a novel method called Partially Observable Mean Field Multi-Agent Reinforcement Learning based\\non Graph–Attention (GAMFQ), which can be applied to large-scale partially observable MARL tasks, where the observation\\nrange of each agent is limited, and the feature information of other agents in the fixed neighborhood is intelligently observed.\\nThe overall architecture of the GAMFQ algorithm is depicted in Figure 2, including two important components: the Graph\\nAttention Module and the Mean Field Module: (i) In our Graph–Attention Module, the information observed locally by each\\nagent is spliced firstly. Then the high-dimensional feature representations are obtained by a latent space mapping process which\\nfollowed by a one-layer LSTM network to obtain the time-series correlation of the target agent, and the hidden layer of the LSTM\\nis used as the input of the graph attention module to initialize the constructed graph nodes. Then to enhance the aggregation of\\nneighbor agents to target agent, a similar process is implemented as a FC mapping network followed by a GAT layer. After that,\\nthe final representation of agents are obtained by a MLP layer with the input of the representations of target agent and other\\nobservable agents. Finally, we adopt layer-normalized method to obtain the adjacency matrix {𝐺𝑡}𝑁\\n1 via Gumbel Softmax. (ii)\\nThe Mean Field Module utilizes the adjacency matrix {𝐺𝑡}𝑁\\n1 from Graph Attention Module to obtain adopting action from\\nimportant neighbor agents, in which the joint Q-function of each agent 𝑗approximates the Mean-Field Q-function 𝑄𝑗(𝑠, 𝑎) ≈\\n𝑄𝑗\\nPOMF\\n(𝑠, 𝑎𝑗, ̃𝑎𝑗) of important neighbor agents, where the Q-value is partially observable mean-field(POMF) Q-value, and ̃𝑎𝑗\\nis the average action of the important neighborhood a gents that is partially observable by agent 𝑗. Each component is described\\nin detail below.\\n7\\nGraph-Attention\\nModule\\nFeature Vector\\nEffective Feature Vector\\nAgent 1\\nAgent 𝑗\\nAgent 𝑁\\n…\\n…\\n…\\n…\\nGet  𝑴Effective neighbors\\n𝐺𝑡\\nFC\\nLSTM\\nFC\\nGumbel Softmax\\nMLP\\nGAT Encoder\\n…\\n…\\n𝑒1\\n𝑡\\n𝑒𝑖\\n𝑡\\n𝑒𝑁𝑗\\n𝑡\\n…\\n𝐸𝑡\\n…\\nAgent 1\\nAgent 𝑗\\nAgent 𝑁\\n…\\n𝑥1\\n𝑡\\n𝑥𝑁𝑗\\n𝑡\\n𝑁𝑗neighbors\\n…\\n…\\n…\\n…\\n…\\n…\\nLocal Observable\\nGet  Mean  action\\nMean Field Module\\n…\\n𝑥𝑖\\n𝑡\\nFIGURE 2 Schematic of GAMFQ. Each agent can observe the feature information of other agents within a fixed range, input\\nit into the Graph–Attention Module, and output an adjacency matrix to represent the effectiveness of the neighborhood agent to\\nthe central agent.\\n4.1\\nGraph–Attention Module\\nTo more accurately re-determine the influence of agent 𝑗’s neighbor 𝑁𝑗on itself, we need to be able to extract useful infor-\\nmation from the local observations of agent 𝑗. The local observations of each agent include the embedding information of\\nneighboring agents. For each agent 𝑗and each time step 𝑡, the information of a local observation of length 𝐿𝑗, is expressed as\\n𝑜𝑡\\n𝑗=\\n(\\n𝑥𝑡\\n1, 𝑥𝑡\\n2, ⋯, 𝑥𝑡\\n𝑁𝑗\\n)\\n, where 𝑥𝑡\\n𝑁𝑗represents the feature of the 𝑁𝑗-th neighbor agent of agent 𝑗, and 𝑜𝑡\\n𝑗∈𝑅𝑁𝑗×𝐷, 𝑥𝑡\\n𝑖∈𝑅1×𝐷.\\n𝐿𝑗is concatenated from the embedding features of each neighbor. Our goal is to learn an adjacency matrix {𝐺𝑡}𝑁\\n1 to extract\\nmore important embedding information for the agent 𝑗from local observations at each time step 𝑡. Since graph neural networks\\ncan better mine the information of neighbor nodes, we propose a graph attention structure suitable for large-scale multi-agent\\nsystems. This structure focuses on information from different agents by associating weights to observations based on the rel-\\native importance of other agents in their local observations. The Graph–Attention structure is constructed by concatenating a\\ngraph attention encoder and a differentiable attention mechanism. For the local observation 𝑜𝑡\\n𝑗of agent 𝑗at time step 𝑡, 𝑜𝑡′\\n𝑗is first\\nencoded using a fully connected layer (FC) , and is passed to the LSTM layerin order to generate the hidden state ℎ𝑡\\n𝑗and cell\\nstate 𝑐𝑡\\n𝑗of agent 𝑗, where ℎ𝑡\\n𝑗serves as the input of the graph attention module to initialize the constructed graph nodes:\\nℎ𝑡\\n𝑗, 𝑐𝑡\\n𝑗= 𝐿𝑆𝑇𝑀\\n(\\n𝑒\\n(\\n𝑜𝑡\\n𝑗\\n)\\n, ℎ𝑡\\n𝑗, 𝑐𝑡\\n𝑗\\n)\\n(11)\\nwhere 𝑒(⋅) is a fully connected layer representing the observed encoder. ℎ𝑡\\n𝑗is encoded as a message:\\n𝑚𝑡\\n𝑗= 𝑒\\n(\\nℎ𝑡\\n𝑗\\n)\\n(12)\\nwhere 𝑚𝑡\\n𝑗is the aggregated information of the neighborhood agents observed by agent 𝑗at time step 𝑡. The input encoding\\ninformation 𝑀𝑡is passed to the GAT encoder and hard attention mechanism, where the hard attention mechanism consists\\nof MLP and Gumbel Softmax function. Finally, the output adjacency matrix {𝐺𝑡}𝑁\\n1 is used to determine which agents in the\\nneighborhood have an influence on the current agent. The GAT encoder helps to efficiently encode the agent’s local information,\\nwhich is expressed as:\\n{𝑀𝑡}𝑁\\n1 = 𝑓𝑆𝑐ℎ𝑒𝑑\\n(𝑚𝑡\\n1, ⋯, 𝑚𝑡\\n𝑁\\n)\\n(13)\\nAdditionally, we take the form of the same attention mechanism as GAT [25], expressed as:\\n𝛼𝑆\\n𝑖𝑗=\\nexp\\n(\\n𝐿𝑒𝑎𝑘𝑦Re𝐿𝑈\\n(\\n𝑎𝑇\\n𝑆\\n[\\n𝑊𝑆𝑚𝑡\\n𝑖||𝑊𝑆𝑚𝑡\\n𝑗\\n]))\\n∑\\n𝑘∈𝑁𝑡\\n𝑗∪{𝑗}\\nexp\\n(\\n𝐿𝑒𝑎𝑘𝑦Re𝐿𝑈\\n(\\n𝑎𝑇\\n𝑆\\n[\\n𝑊𝑆𝑚𝑡\\n𝑗||𝑊𝑆𝑚𝑡\\n𝑘\\n]))\\n(14)\\n8\\nwhere 𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈(⋅) is the activation function, 𝑎𝑆∈𝑅𝐷is the weight vector, 𝑁𝑡\\n𝑗∪{𝑗} represents the central agent 𝑗and its\\nobservable neighborhood agent set, and 𝑊𝑆∈𝑅𝐷×𝐷is the weight matrix. The node feature of agent 𝑗is expressed as:\\n𝑒𝑡\\n𝑗= 𝐸𝐿𝑈\\n⎛\\n⎜\\n⎜⎝\\n∑\\n𝑖∈𝑁𝑡\\n𝑗∪𝑗\\n𝛼𝑆\\n𝑖𝑗𝑊𝑆𝑚𝑡\\n𝑖\\n⎞\\n⎟\\n⎟⎠\\n(15)\\nwhere 𝐸𝐿𝑈(⋅) is an exponential linear unit function. Connecting the features of each node in pairs: 𝐸𝑡\\n𝑖,𝑗=\\n(\\n𝑒𝑡\\n𝑖||𝑒𝑡\\n𝑗\\n)\\n, we can get a\\nmatrix 𝐸𝑡∈𝑅𝑁×𝑁𝑗×2𝐷, where 𝐸𝑡\\n𝑖,𝑗represents the relevant features of agent 𝑗. Taking 𝐸𝑡as the input of MLP which is followed\\nby a Gumbel Softmax function, the connected vector 𝐺𝑡\\n𝑗can be obtained. The connected vector 𝐺𝑡\\n𝑗consists of elements 𝑔𝑖𝑗,\\nwhere 𝑖represents the neighbors of the central agent 𝑗. The element 𝑔𝑡\\n𝑖𝑗= 1 in the adjacency matrix indicates that the action of\\nthe agent 𝑖will have an impact on the agent 𝑗. Conversely, 𝑔𝑡\\n𝑖𝑗= 0 means that the agent’s actions have no effect on the agent 𝑗.\\n4.2\\nMean Field Module\\nThis Graph-Attention method selects important 𝑀𝑗agents from the neighbors 𝑁𝑗of agent 𝑗, and compute the average of the\\nactions of the choosed neighbor agents:\\ñ𝑎𝑗\\n𝑡=\\n1\\n𝑀𝑗\\n∑\\n𝑘∈𝑁𝑗\\n𝑎𝑘\\n𝑡⋅𝐺𝑡\\n𝑗,\\n𝑎𝑘\\n𝑡∼𝜋𝑘(⋅∣𝑠𝑡, ̃𝑎𝑘\\n𝑡\\n)\\n(16)\\nwhere ⋅is the element-wise multiplication.\\nIn the above formula, 𝑎𝑘represents the important neighborhood agent for agent 𝑗. Then the Q–value of each agent is shown\\nin Eq. 17. Note that the Q–value here is a partially observable Q–value.\\n𝑄𝑗\\nGAMF\\n(𝑠𝑗\\n𝑡, 𝑎𝑗\\n𝑡, ̃𝑎𝑗\\n𝑡\\n) = (1 −𝛼)𝑄𝑗\\nGAMF\\n(𝑠𝑗\\n𝑡, 𝑎𝑗\\n𝑡, ̃𝑎𝑗\\n𝑡\\n) + 𝛼[𝑟𝑗\\n𝑡+ 𝛾𝑣(𝑠𝑗\\n𝑡+1\\n)]\\n(17)\\nwhere the value function 𝑣𝑗is expressed as\\n𝑣𝑗(𝑠𝑗\\n𝑡+1\\n) =\\n∑\\n𝑎𝑗\\n𝑡+1\\n𝜋𝑗(𝑎𝑗\\n𝑡+1 ∣𝑠𝑗\\n𝑡+1, ̃𝑎𝑗\\n𝑡\\n) 𝑄𝑗\\nGAMF\\n(𝑠𝑗\\n𝑡+1, 𝑎𝑗\\n𝑡+1, ̃𝑎𝑗\\n𝑡\\n)\\n(18)\\nAccording to the above graph attention mechanism, more important neighborhood agents are obtained. The new average\\naction ̃𝑎𝑗\\n𝑡is calculated by Eq.16, and then the strategy 𝜋𝑗\\n𝑡of agent 𝑗is updated by the following formula:\\n𝜋𝑗(𝑎𝑗\\n𝑡∣𝑠𝑗\\n𝑡, ̃𝑎𝑗\\n𝑡−1\\n) =\\nexp (−𝛽𝑄𝑗\\nGAMF\\n(𝑠𝑗\\n𝑡, 𝑎𝑗\\n𝑡, ̃𝑎𝑗\\n𝑡−1\\n))\\n∑\\n𝑎𝑗′\\n𝑡∈𝐴𝑗\\nexp\\n(\\n−𝛽𝑄𝑗\\nGAMF\\n(\\n𝑠𝑗\\n𝑡, 𝑎𝑗′\\n𝑡, ̃𝑎𝑗\\n𝑡−1\\n))\\n(19)\\n4.3\\nTheoretical Proof\\nThis subsection is devoted to proving that the setting of GAMFQ is close to the Nash equilibrium. Subramanian et al. [23]\\nshowed that in partially observable cases, the fixed observation radius (FOR) setting is close to a Nash equilibrium, where the\\nmean action of each agent’s neighborhood agents is approximated by a dirichlet distribution. First, we state some assumptions,\\nwhich are the same as literature[23], and are followed by all the theorems and analyses below.\\nAssumption 1. For any 𝑖and 𝑗, there is lim𝑡→∞𝜏𝑖\\n𝑗(𝑡) = ∞. 𝑤.𝑝.1.\\nThis assumption guarantees a probability of 1 that old information is eventually discarded.\\nAssumption 2. Suppose some measurability conditions are as follow: (1) 𝑥(0) is \\ue232(0)-measurable. (2) For each 𝑖,𝑗and 𝑡, 𝑤𝑖(𝑡)\\nis \\ue232(𝑡+1)-measurable. (3) For each 𝑖, 𝑗and 𝑡, 𝛼𝑖(𝑡) and 𝜏𝑖\\n𝑗(𝑡) are \\ue232(𝑡)-measurable. (4) For each 𝑖and 𝑡, satisfy B [𝑤𝑖(𝑡)|\\ue232(𝑡)] = 0.\\n(5) B [𝑤2\\n𝑖(𝑡)|\\ue232(𝑡)] ≤𝐴+ 𝐵max𝑗max𝜏≤𝑡|||𝑥𝑗(𝜏)|||\\n2\\n, where 𝐴and 𝐵are deterministic constants.\\nAssumption 3. The learning rates satisfy 0 ≤𝛼𝑖(𝑡) < 1.\\nAssumption 4. Suppose some conditions for the 𝐹mapping are as follows: (1) If 𝑥≤𝑦, then 𝐹(𝑥) ≤𝐹(𝑦), that is, 𝐹is\\nmonotonic; (2) 𝐹is continuous; (3) When 𝑡→∞, 𝐹is limited to the interval [𝑥∗−𝐷, 𝑥∗+ 𝐷], where 𝑥∗is some arbitrary\\n9\\npoint; (4) If 𝑒∈\\ue23e𝑛is a vector that satisfies all components equal to 1, then 𝐹(𝑥) −𝑝𝑒≤𝐹(𝑥+ 𝑝𝑒) ≤𝐹(𝑥+ 𝑝𝑒) + 𝑝𝑒, where 𝑝\\nis a positive scalar.\\nAssumption 5. Each action-value pair can be accessed indefinitely, and the reward is limited.\\nAssumption 6. Under the limit 𝑡→∞of infinite exploration, the agent’s policy is greedy.\\nThis assumption ensures that the agent is rational.\\nAssumption 7. In each stage of a stochastic game, a Nash equilibrium can be regarded as a global optimum or saddle point.\\nBased on these assumptions, Subramanian et al. [23] give the following lemma.\\nLemma 1. [23] When the Q-function is updated using the partially observable update rule in Eq.2, and assumptions 3, 5, and\\n7 hold, the following holds for 𝑡→∞:\\n|𝑄∗(𝑠𝑡, 𝑎𝑡) −𝑄𝑃𝑂𝑀𝐹(𝑠𝑡, 𝑎𝑡, ̃𝑎𝑡)| ≤2𝐷\\n(20)\\nwhere 𝑄∗is the Nash Q-value, 𝑄𝑃𝑂𝑀𝐹is the partially observable mean-field Q-function, and 𝐷is the bound of the 𝐹map. The\\nprobability that the above formula holds is at least 𝛿𝐿−1 , where 𝐿= |𝐴|.\\nIn our GAMFQ setting, for partially observable neighborhood agents, we choose to select a limited number of important agents\\nby using graph attention, and then update the POMF Q function. The following theorem proves that the setting of GAMFQ is\\nclose to Nash equilibrium.\\nTheorem 1. The distance between the MFQ (globally observable) mean action ̄𝑎and the GAMFQ (partially observable) mean\\naction ̃𝑎satisfies the following formula:\\n||| ̃𝑎𝑗\\n𝑡−̄𝑎𝑗\\n𝑡\\n||| ≤\\n√\\n1\\n2𝑁𝑗\\nlog 2\\n𝛿\\n(21)\\nWhen 𝑡→∞, the probability >= 𝛿, where 𝑁𝑗is the number of observed neighbor agents, ̃𝑎is the partially observable mean\\naction obtained by graph attention in Eq. 16, ̄𝑎is the globally observable mean action in Eq. 4.\\nAssuming that each agent is globally observable, the mean of important agents selected by graph attention is close to the true\\nunderlying global observable ̄𝑎. Since the GAMF Q-function is updated by taking finite samples through graph attention, the\\nempirical mean is ̃𝑎.\\nTheorem 2. If the Q-function is Lipschitz continuous with respect to the mean action, i.e. 𝑀is constant, then the MF Q-function\\n𝑄𝑀𝐹and GAMF Q-function 𝑄𝐺𝐴𝑀𝐹satisfy the following relation:\\n|||𝑄𝐺𝐴𝑀𝐹\\n(𝑠𝑡, 𝑎𝑡, ̃𝑎𝑡−1\\n) −𝑄𝑀𝐹\\n(𝑠𝑡, 𝑎𝑡, ̄𝑎𝑡−1\\n)||| ≤𝑀× 𝐿× log 2\\n𝛿×\\n1\\n2𝑁𝑗\\n(22)\\nWhen the limit 𝑡→∞, the probability is ≥(𝛿)𝐿−1, where 𝐿= |𝐴|, 𝐴is the action space of the agent.\\nIn the proof of theorem 2, first consider a Q-function that is Lipschitz continuous for all ̄𝑎and ̃𝑎.According to theorem 1, the\\nabove formula can further deduce the result of theorem 2. The total number of components is equal to the action space 𝐿. The\\nbound of theorem 1 is probability >= 𝛿, and since there are 𝐿random variables, the probability of theorem 2 is at least (𝛿)𝐿−1.\\nWhen the first 𝐿−1 random variable is fixed, the deterministic last ̄𝑎component satisfies the relationship that the sum of the\\nindividual components is 1. Since each agent’s action is represented by a one-hot encoding, the ̃𝑎′ component of GAMFQ also\\nsatisfies the relationship that the sum of the individual components is 1, and the component of the agent’s average action does\\nnot change due to the application of graph attention. The proof of theorem 2 ends.\\nTheorem 3. A stochastic process in form 𝑥𝑖(𝑡+ 1) = 𝑥𝑖(𝑡) + 𝛼𝑖(𝑡) (𝐹𝑖\\n(𝑥𝑖(𝑡)) −𝑥𝑖(𝑡) + 𝑤𝑖(𝑡)) remains bounded in the range\\n[𝑥∗−2𝐷, 𝑥∗+ 2𝐷] on limit 𝑡→∞if assumptions 1,2,3 and 4 are satisfied, and are guaranteed not to diverge to infinity.Where\\n𝐷is the boundary of the 𝐹map in assumption 4(4).\\nThis theorem can be proved in terms of Tsitsiklis[24] and by extension. The result of theorem 3 can then be used to derive\\ntheorem 4.\\n10\\nTheorem 4. When the Q-function is updated using the partially observable update rule in Eq.17, and assumptions 3, 5, and 7\\nhold, the following holds for 𝑡→∞:\\n|𝑄∗(𝑠𝑡, 𝑎𝑡) −𝑄𝐺𝐴𝑀𝐹(𝑠𝑡, 𝑎𝑡, ̃𝑎𝑡)| ≤2𝐷\\n(23)\\nwhere 𝑄∗is the Nash Q-value, 𝑄𝐺𝐴𝑀𝐹is the partially observable mean-field Q-function, and 𝐷is the bound of the 𝐹map. The\\nprobability that the above formula holds is at least 𝛿𝐿−1 , where 𝐿= |𝐴|.\\nTheorem 4 shows that the GAMFQ update is very close to the Nash equilibrium at the limit 𝑡→∞, i.e. reaching a plateau\\nfor stochastic policies. Therefore, the strategy of Eq.19 is approximately close to this plateau. Theorem 4 is an application of\\ntheorem 3, using assumptions 3, 5 and 7 .However, in MARL, reaching a Nash equilibrium is not optimal, but only a fixed-point\\nguarantee. Therefore, to achieve better performance, each selfish agent will still tend to pick a limited number of samples. To\\nbalance theory and performance when selecting agents from the neighborhood, an appropriate number of agents (more efficient\\nagents) need to be used for better multi-agent system performance. This paper uses the graph attention structure to filter out\\nmore important proxies, which can better approximate the Nash equilibrium.\\n4.4\\nAlgorithm\\nThe implementation of GAMFQ follows the related work of the previous POMFQ [23], the difference is that the graph attention\\nstructure is used to select the neighborhood agents that are more important to the central agent when updating the average action.\\nAlgorithm 1 gives the pseudocode of the GAMFQ algorithm. It obtains effective neighbor agents by continuously updating the\\nadjacency matrix 𝐺𝑡\\n𝑗to update the agent’s strategy.\\nAlgorithm 1 Partially Observable Mean Field MARL Based on Graph–Attention\\nInitialize the weights of Q-function 𝑄𝜙𝑗, 𝑄𝜙𝑗\\n−, replay buffer 𝐵, GAT encoder, MLP layers and mean action ̄𝑎𝑗for each agent\\n𝑗∈1, … , 𝑁.\\nfor 𝑒𝑝𝑖𝑠𝑜𝑑𝑒= 1, 2, … , 𝐸do\\nfor 𝑡≤𝑇and not terminal do\\nFor each agent 𝑗, calculate the hidden state ℎ𝑡\\n𝑗according to Eq.11, and encode ℎ𝑡\\n𝑗as a message 𝑚𝑡\\n𝑗(Eq.12).\\nFor each agent 𝑗, sample 𝑎𝑗fron policy induced by 𝑄𝜙𝑗(Eq.19).\\nFor each agent 𝑗, pass the encoded information 𝑚𝑡\\n𝑗to the GAT encoder and hard attention mechanism to output the\\nadjacency matrix 𝐺𝑡\\n𝑗.\\nFor each agent 𝑗, calculate the new neighborhood agent mean action ̄𝑎𝑗by Eq.16.\\nReceive the full state of environment 𝑠𝑡, action 𝑎= [𝑎1, … , 𝑎𝑁], reward [𝑟= 𝑟1, … , 𝑟𝑁], and the next state 𝑠′ =\\n[𝑠1, … , 𝑠𝑁].\\nStore transition ⟨𝑠, 𝑎, 𝑟, 𝑠′, ̄𝑎⟩in 𝐵, where ̄𝑎= [ ̄𝑎1, … , ̄𝑎𝑁] is the mean action.\\nend for\\nfor 𝑗= 1, … , 𝑁do\\nSample a minibatch of K experiences ⟨𝑠, 𝑎, 𝑟, 𝑠′, ̄𝑎⟩from replay buffer 𝐵.\\nSet 𝑦𝑗= 𝑟𝑗+ 𝛾𝑣𝜙\\n(𝑠′) according to Eq.18.\\nminimize the loss 𝐿(𝜙𝑗) = (𝑦𝑗−𝑄𝜓𝑗\\n(𝑠′, 𝑎𝑗, ̄𝑎𝑗))2\\nto update Q network.\\nend for\\nFor each agent 𝑗, update params of target network :𝜙𝑗←𝜏𝜙𝑗+ (1 −𝜏)𝜙𝑗.\\nend for\\n5\\nEXPERIMENTS\\nIn this section, we describe three different tasks based on the MAgent framework and give some experimental setup and training\\ndetails for evaluating the GAMFQ performance.\\n11\\n5.1\\nEnvironments and Tasks\\nSubramanian et al. [23] designed three different cooperative-competitive strategies in the MAgent framework [36] as experimen-\\ntal environments, and our experiments adopt the same environments. In these three tasks, the map size is set to 28*28, where the\\nobservation range of each agent is 6 units. The state space is the concatenation of the feature information of other agents within\\neach agent’s field of view, including location, health, and grouping information. The action space includes 13 move actions and\\n8 attack actions. In addition, each agent is required to handle at most 20 other agents that are closest. We will evaluate against\\nthese three tasks:\\n• Multibattle environment: There are two groups of agents fighting each other, each containing 25 agents. The agent gets\\n-0.005 points for each move, -0.1 points for attacking an empty area, 200 points for killing an enemy agent, and 0.2 points\\nfor a successful attack. Each agent is 2*2 in size, has a maximum health of 10 units, and a speed of 2 units. After the\\nbattle, the team with the most surviving agents wins. If both teams have the same number of surviving agents, the team\\nwith the highest reward wins. The reward for each team is the sum of the rewards for the individual agents in the team.\\n• Battle-Gathering environment: There is a uniform distribution of food in the environment, each agent can observe the\\nlocation of all the food. In addition to attacking the enemy to get rewards, each agent can also eat food to get rewards. Agents\\nget 5 points for attacking enemy agents, and the rest of the reward settings are the same as the Multibattle environment.\\n• Predator-Prey environment: There are 40 predators and 20 prey, where each predator is a square grid of size 2*2 with a\\nmaximum health of 10 units and a speed of 2 units. Prey is a 1*1 square with a maximum health of 2 units and a speed of\\n2.5 units. To win the game, the predator must kill more prey, and the prey must find a way to escape. In addition, predators\\nand prey have different reward functions, predators get -0.3 points for attacking space, 1 point for successfully attacking\\nprey, 100 points for killing prey, -1 point for attacked prey, and 0.5 points for dying. Unlike the Multibattle environment,\\nwhen the round ends for a fairer duel, if the two teams have the same number of surviving agents, it is judged as a draw.\\n5.2\\nEvaluation\\nWe consider four algorithms for the above three games: MFQ, MFAC [31], POMFQ(FOR) and GAMFQ, where MFQ and\\nMFAC are baselines and POMFQ(FOR) [23] is the state-of-the-art algorithm.\\nThe original baselines MFQ and MFAC were proposed by Yang et al. [31] based on global observability, and the idea was to\\napproximate the influence of the neighborhood agents on the central agent as their average actions, thereby updating the actions\\nof the neighborhood agents. We fix the observation radius of each agent in the baseline MFQ and MFAC and apply it to a partially\\nobservable environment, where neighbor agents are agents within a fixed range. The POMFQ(FOR) algorithm introduces noise\\nin the mean action parameters to encourage exploration, uses Bayesian inference to update the Dirichlet distribution, and samples\\n100 samples from the Dirichlet distribution to estimate partially observable mean field actions. The GAMFQ algorithm judges\\nthe effectiveness of neighborhood agents within a fixed range through the graph attention mechanism, selects more important\\nneighborhood agents, and updates the average action by averaging the actions of these agents.\\n5.3\\nHyperparameters\\nIn the three tasks, each algorithm was trained for 2000 epochs in the training phase, generating two sets of A and B sets of models.\\nIn the test phase, 1000 rounds of confrontation were conducted, of which the first 500 rounds were the first group A of the first\\nalgorithm against the second group B of the second algorithm, and the last 500 groups were the opposite. The hyperparameters\\nof MFQ, MFAC, POMFQ(FOR) and GAMFQ are basically the same. Table 1 lists the hyperparameters during training of the\\nfour algorithms, and the remaining parameters can be seen in [23].\\n6\\nRESULTS AND DISCUSSION\\nIn this section, we evaluate the performance of GMAFQ in three different environments, including Multibattle, Battle-Gathering,\\nand Predator-Prey. We benchmark against two algorithms, MFQ and MFAC, and compare with the state-of-the-art POMFQ\\n12\\nTABLE 1 Hyperparameters for four algorithms training.\\nParameter\\nValue\\nDescription\\n𝛼\\n10−4\\nlearning rate\\n𝛽\\ndecays linearly from 1 to 0\\nexploration rate\\n𝛾\\n0.95\\ndiscount rate\\n𝐵\\n1024\\nreplay buffer\\nℎ\\n64\\nthe hidden layer size in GAT\\n𝐾\\n64\\nmini-batch\\n𝑡𝑒𝑚𝑝𝑒𝑟𝑎𝑡𝑢𝑟𝑒\\n0.1\\nthe soft-max layer temperature of the actor in MFAC\\n(FOR). We implement our method and comparative methods on three different tasks. Note that we only used 50 agents in our\\nexperiments and did not test more agents, this is because the proportion of other agents that each agent can see is more important\\nthan the absolute number.\\n6.1\\nReward\\nFigure 3 shows how the reward changes as the number of iterations increases during training. We plot the reward changes for\\nthe four algorithms in different game environments during the first 1000 iterations. Since each algorithm is self-training which\\nresults in a large change in the reward of the algorithm, we use the least squares method to fit the reward change graph. In Figure\\n3, the solid black line represents the reward change graph of the GAMFQ algorithm. From Figure 3 (a), (b) and (c), it can be\\nseen that the reward of the GAMFQ algorithm can increase rapidly, indicating that the GAMFQ algorithm can converge rapidly\\nin the early stage, and the convergence performance is better than the other three algorithms.\\n0\\n200\\n400\\n600\\n800\\n1000\\nEpisode\\n1000\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\n6000\\nReward\\n* MFQ\\n* MFAC\\n* POMFQ(FOR)\\n* GAMFQ\\n(a) Train results of Multibattle game.\\n0\\n200\\n400\\n600\\n800\\n1000\\nEpisode\\n0\\n2000\\n4000\\n6000\\n8000\\n10000\\n12000\\n14000\\nReward\\n* MFQ\\n* MFAC\\n* POMFQ(FOR)\\n* GAMFQ\\n(b) Train results of Battle-Gathering game.\\n0\\n200\\n400\\n600\\n800\\n1000\\nEpisode\\n1000\\n0\\n1000\\n2000\\n3000\\n4000\\nReward\\n* MFQ\\n* MFAC\\n* POMFQ(FOR)\\n* GAMFQ\\n(c) Train results of Predator-Prey game.\\nFIGURE 3 Train results of three games. The reward curve for each algorithm is fitted by the least squares method.\\n6.2\\nElo Calculation\\nWe use ELO Score [11] to evaluate the performance of the two groups of agents, the advantage of which is that it takes into\\naccount the strength gap between the opponents themselves. ELO ratings are commonly used in chess to evaluate one-on-one\\nsituations, and this approach can similarly be extended to N-versus-N situations. For the algorithm proposed in the paper, we\\nrecord the total rewards of the two teams of agents during each algorithm confrontation, which are 𝑅1 and 𝑅2, respectively.\\nThen the expected win rates of the two groups of agents are:\\n𝐸1 =\\n1\\n1 + 10(𝑅2−𝑅1)∕400 , 𝐸2 =\\n1\\n1 + 10(𝑅1−𝑅2)∕400\\n(24)\\n13\\nwhere 𝐸1 + 𝐸2 = 1. By analyzing the actual and predicted winning rates of the two groups of agents, the new ELO score of\\neach team after the game ends can be obtained:\\n𝑅1\\n′ = 𝑅1 + 𝐾(𝑆1 −𝐸1), 𝑅2\\n′ = 𝑅2 + 𝐾(𝑆2 −𝐸2)\\n(25)\\nwhere 𝑅1 represents the actual winning or losing value, 1 means the team wins, 0.5 means the two teams are tied, and 0 means\\nthe team loses. 𝐾is represented as a floating coefficient. To create a gap between agents, we set 𝐾to 32. For each match, we\\nfaced off 500 times and calculated the average ELO value for all matches.\\nTABLE 2 The ELO Score of four algorithms in Multibattle environment.\\nTask\\nAlgorithm1\\nAlgorithm2\\nELO Score1\\nELO Score2\\nGAMFQ vs POMFQ(FOR)\\nGAMFQ-1\\nPOMFQ(FOR)-2\\n3579\\n820\\nGAMFQ-2\\nPOMFQ(FOR)-1\\n2696\\n2838\\nGAMFQ vs MFQ\\nGAMFQ-1\\nMFQ-2\\n2098\\n1508\\nGAMFQ-2\\nMFQ-1\\n2535\\n1695\\nGAMFQ vs MFAC\\nGAMFQ-1\\nMFAC-2\\n1350\\n-49\\nGAMFQ-2\\nMFAC-1\\n-856\\n-78\\nPOMFQ(FOR) vs MFQ\\nPOMFQ(FOR)-1\\nMFQ-2\\n3145\\n2577\\nPOMFQ(FOR)-2\\nMFQ-1\\n2569\\n2857\\nPOMFQ(FOR) vs MFAC\\nPOMFQ(FOR)-1\\nMFAC-2\\n-205\\n-64\\nPOMFQ(FOR)-2\\nMFAC-1\\n826\\n-42\\nMFQ vs MFAC\\nMFQ-1\\nMFAC-2\\n-142\\n-49\\nMFQ-2\\nMFAC-1\\n610\\n-46\\nTABLE 3 The ELO Score of four algorithms in Battle-Gathering environment.\\nTask\\nAlgorithm1\\nAlgorithm2\\nELO Score1\\nELO Score2\\nGAMFQ vs POMFQ(FOR)\\nGAMFQ-1\\nPOMFQ(FOR)-2\\n7770\\n8931\\nGAMFQ-2\\nPOMFQ(FOR)-1\\n8293\\n9310\\nGAMFQ vs MFQ\\nGAMFQ-1\\nMFQ-2\\n6374\\n10870\\nGAMFQ-2\\nMFQ-1\\n8510\\n8313\\nGAMFQ vs MFAC\\nGAMFQ-1\\nMFAC-2\\n5525\\n10\\nGAMFQ-2\\nMFAC-1\\n10751\\n-31\\nPOMFQ(FOR) vs MFQ\\nPOMFQ(FOR)-1\\nMFQ-2\\n8526\\n8760\\nPOMFQ(FOR)-2\\nMFQ-1\\n8632\\n8227\\nPOMFQ(FOR) vs MFAC\\nPOMFQ(FOR)-1\\nMFAC-2\\n12722\\n0\\nPOMFQ(FOR)-2\\nMFAC-1\\n12171\\n-88\\nMFQ vs MFAC\\nMFQ-1\\nMFAC-2\\n12649\\n49\\nMFQ-2\\nMFAC-1\\n13788\\n-48\\n14\\nTable 2, 3, 4 shows the ELO scores of the four algorithms on the three tasks. It can be seen from Table 2 that in Multibattle\\nenvironment, the GAMFQ algorithm has the highest ELO score of 3579, which is significantly better than the other three\\nalgorithms. As shown in Table 3, in Battle-Gathering environment, the ELO score of the MFQ algorithm is the highest, and the\\nELO score of the GAMFQ algorithm is average. This is because the collection environment contains food, and some algorithms\\ntend to eat food to get rewards quickly, rather than attacking enemy agents. However, the final game winning or losing decision\\nis made by comparing the number of remaining agents between the two teams of agents. As shown in Table 4, in Predator-Prey\\nenvironment, the ELO score of the GAMFQ algorithm has the highest ELO score of 860, which is significantly better than\\nthe other three algorithms. From the experimental results in the three environments, we can summarize that ELO score of the\\nGAMFQ algorithm is better than other three algorithms, showing better performance.\\nTABLE 4 The ELO Score of four algorithms in Predator-Prey environment.\\nTask\\nAlgorithm1\\nAlgorithm2\\nELO Score1\\nELO Score2\\nGAMFQ vs POMFQ(FOR)\\nGAMFQ-1\\nPOMFQ(FOR)-2\\n421\\n-32\\nGAMFQ-2\\nPOMFQ(FOR)-1\\n16\\n7\\nGAMFQ vs MFQ\\nGAMFQ-1\\nMFQ-2\\n714\\n-27\\nGAMFQ-2\\nMFQ-1\\n-15\\n-94\\nGAMFQ vs MFAC\\nGAMFQ-1\\nMFAC-2\\n860\\n-28\\nGAMFQ-2\\nMFAC-1\\n16\\n16\\nPOMFQ(FOR) vs MFQ\\nPOMFQ(FOR)-1\\nMFQ-2\\n66\\n18\\nPOMFQ(FOR)-2\\nMFQ-1\\n13\\n24\\nPOMFQ(FOR) vs MFAC\\nPOMFQ(FOR)-1\\nMFAC-2\\n16\\n-16\\nPOMFQ(FOR)-2\\nMFAC-1\\n47\\n16\\nMFQ vs MFAC\\nMFQ-1\\nMFAC-2\\n16\\n-16\\nMFQ-2\\nMFAC-1\\n174\\n17\\n6.3\\nResults\\nFigure 4 shows the face-off results of the four algorithms in the three tasks. Figure 4(a) shows the faceoff results of Multibattle\\ngame. The different colored bars for each algorithm represent the results of an algorithm versus others. We do not conduct\\nadversarial experiments between the same algorithms because we consider that the adversarial properties of the same algorithms\\nare equal. The vertical lines in the bar graph represent the standard deviation of wins for groups A and B over 1,000 face-offs.\\nFigure 4(a) shows GAMFQ against three other algorithms, all with a win rate above 0.7.\\nFigure 4(b) shows the faceoff results of Battle-Gathering game. In addition to getting rewards for killing enemies, agents can\\nalso get rewards from food. It can be seen that MFQ loses to all other algorithms, MFAC and POMFQ (FOR) perform in general,\\nand our GAMFQ is clearly ahead of other algorithms.\\nFigure 4(c) shows thwe faceoff results of Predator-Prey game.The standard deviation of this game is significantly higher than\\nthe previous two games, due to the fact that both groups A and B are trying to beat each other in the environment. It can be seen\\nthat the GAMFQ algorithm is significantly better than other three algorithms, reaching a winning rate of 1.0.\\nExperiments in the above three multi-agent combat environments show that GAMFQ can show good performance over MFQ,\\nMFAC and POMFQ(FOR).\\n15\\nMFQ\\nMFAC\\nPOMFQ(FOR) GAMFQ\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\nNumber of Games won\\n* MFQ\\n* MFAC\\n* POMFQ(FOR)\\n* GAMFQ\\n(a) Faceoff results of Multibattle game.\\nMFQ\\nMFAC\\nPOMFQ(FOR) GAMFQ\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\nNumber of Games won\\n* MFQ\\n* MFAC\\n* POMFQ(FOR)\\n* GAMFQ\\n(b) Faceoff results of Battle-Gathering game.\\nMFQ\\nMFAC\\nPOMFQ(FOR) GAMFQ\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\nNumber of Games won\\n* MFQ\\n* MFAC\\n* POMFQ(FOR)\\n* GAMFQ\\n(c) Faceoff results of Predator-Prey game.\\nFIGURE 4 Faceoff results of three games. The * in the legend indicates the enemy. For example, the first blue bar in the bar\\ngraph corresponding to the GAMFQ algorithm is the result of the confrontation between GAMFQ and MFQ, and we do not\\nconduct confrontation experiments between the same algorithms.\\n6.4\\nVisualization\\n(a) Multibattle begin\\n(b) Multibattle process \\n(c) Multibattle end \\ncooperative \\nattack\\nattack\\nescape\\nGAMFQ   alive：22\\nPOMFQ(FOR)    alive：2\\nFIGURE 5 Visualization of the standoff between GAMFQ and POMFQ (FOR) in a Multibattle game.\\n(a) Battle-Gathering begin\\n(b) Battle-Gathering process \\n(c) Battle-Gathering end \\nBesiege \\nbegin\\nBesiege \\nend\\nfood\\nGAMFQ   alive：24\\nPOMFQ(FOR)    alive：9\\nFIGURE 6 Visualization of the standoff between GAMFQ and POMFQ (FOR) in a Battle-Gathering game.\\n16\\n(a) Predator-Prey begin\\n(b1) Predator-Prey process \\n(b2) Predator-Prey process \\nGAMFQ   alive：25\\nPOMFQ(FOR)    alive：21\\nPOMFQ(FOR)    alive：25\\nGAMFQ   alive：25\\nencircle\\nescape\\nFIGURE 7 Visualization of the standoff between GAMFQ and POMFQ (FOR) in a Predator-Prey game.\\nTo visualize the effectiveness of the GAMFQ algorithm, we visualize the confrontation between GAMFQ and POMFQ (FOR)\\nin a Multibattle environment, as shown in Figure 5, where the red side is GMAFQ and the blue side is POMFQ (FOR). It can\\nbe seen from the confrontation process that for the GAMFQ algorithm, when an agent decides to attack, the surrounding agents\\nwill also decide to attack under its influence, forming a good cooperation mechanism. On the contrary, for the POMFQ (FOR)\\nalgorithm, some blue-side agents are chosen to attack, some are chosen to escape, and no common fighting mechanism was\\nformed. Similarly, in the Battle-Gathering environment of Figure 6, GAMFQ can learn the surrounding mechanism well. In the\\nPredator-Prey environment of Figure 7, when GAMFQ acts as a predator, the technique of surrounding the prey POMFQ (FOR)\\ncan be learned. On the contrary, when POMFQ (FOR) acted as a predator, it failed to catch the prey GMAFQ.\\n6.5\\nAblation study\\n0\\n200\\n400\\n600\\n800\\n1000\\nEpisode\\n-1000\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\n6000\\nReward\\nR=2\\nR=4\\nR=6\\nR=8\\nR=10\\nFIGURE 8 Ablation study. R represents the observation radius of the agent.\\nFigure 8 is an ablation study that investigates the performance of the GAMFQ algorithm for different observation radius in a\\nMultibattle environment. where the solid line represents the least squares fit of the reward change. It can be seen from the figure\\nthat when the number of training is small, the performance of the algorithm is better as the observation distance increases. But\\nwith the increase of training times, when R=4, the performance of the algorithm is the best, so the appropriate observation\\ndistance can achieve better performance. What is more important in this paper is to explore the effect of the ratio of observable\\ndistance to the number of agents on the performance of the algorithm, so there is no experiment with more agents.\\n17\\n7\\nCONCLUSION\\nIn this paper, we proposed a new multi-agent reinforcement learning algorithm, Graph Attention-based Partially Observable\\nMean Reinforcement Learning (GAMFQ), to address the problem of large-scale partially observable multi-agent environments.\\nAlthough existing methods are close to Nash equilibrium, they do not take into account the direct correlation of agents. Based\\non the correlation between agents, GAMFQ uses a Graph-Attention module to describe how each agent is affected by the actions\\nof other agents at each time step. Experimental results on three challenging tasks in the MAgents framework illustrate that,\\nour proposed method outperforms baselines in all these games and outperforms the state-of-the-art partially observable mean-\\nfield reinforcement learning algorithms. In the future, we will further explore the correlation between agents to extend to more\\ncommon cooperation scenarios.\\nConflict of interest\\nThe authors declare no potential conflict of interests.\\nArticle Description\\nThe\\nexpanded\\nversion\\nof\\nthis\\narticle\\nis\\npublished\\nin\\nDrones\\n2023,\\n7(7),\\n476,\\nwith\\na\\nDOI\\nof\\nhttps://doi.org/10.3390/drones7070476.\\nReferences\\n[1] A. T. Azar et al., Drone deep reinforcement learning: A review, Electronics 10 (2021), no. 9, 999.\\n[2] W. Boehmer, V. Kurin, and S. Whiteson, Deep coordination graphs, Proceedings of the 37th International Conference on\\nMachine Learning, ICML 2020, 13-18 July 2020, Virtual Event, Proceedings of Machine Learning Research, vol. 119,\\nPMLR, 980–991.\\n[3] Q. Cai, Z. Yang, and Z. Wang, Reinforcement learning from partial observation: Linear function approximation with\\nprovable sample efficiency, International Conference on Machine Learning, PMLR, 2485–2522.\\n[4] J. Fan et al., A theoretical analysis of deep q-learning, Learning for Dynamics and Control, PMLR, 486–489.\\n[5] B. Fang et al., Large-scale multi-agent reinforcement learning based on weighted mean field, Cognitive Systems and Signal\\nProcessing - 5th International Conference, ICCSIP 2020, Zhuhai, China, December 25-27, 2020, Revised Selected Papers,\\nCommunications in Computer and Information Science, vol. 1397, Springer, 309–316.\\n[6] H. Gu et al., Mean-field multi-agent reinforcement learning: A decentralized network approach, arXiv preprint\\narXiv:2108.02731 (2021).\\n[7] J. Gu et al., A metaverse-based teaching building evacuation training system with deep reinforcement learning, IEEE\\nTransactions on Systems, Man, and Cybernetics: Systems (2023).\\n[8] Q. Hao, Very large scale multi-agent reinforcement learning with graph attention mean field, https://openreview.net/forum?\\nid=MdiVU9lMmVS (2023).\\n[9] K. He, P. Doshi, and B. Banerjee, Reinforcement learning in many-agent settings under partial observability, The 38th\\nConference on Uncertainty in Artificial Intelligence.\\n[10] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, A survey and critique of multiagent deep reinforcement learning,\\nAutonomous Agents and Multi-Agent Systems 6 (2019), no. 33, 750–797.\\n[11] M. Jaderberg et al., Human-level performance in first-person multiplayer games with population-based deep reinforcement\\nlearning, ArXiv abs/1807.01281 (2018).\\n18\\n[12] M. Laurière et al., Learning mean field games: A survey, arXiv preprint arXiv:2205.12944 (2022).\\n[13] S. Li et al., Deep implicit coordination graphs for multi-agent reinforcement learning, AAMAS ’21: 20th International\\nConference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021, ACM, 764–\\n772.\\n[14] M. L. Littman, Markov games as a framework for multi-agent reinforcement learning, Machine learning proceedings 1994,\\nElsevier, 1994. 157–163.\\n[15] R. Lowe et al., Multi-agent actor-critic for mixed cooperative-competitive environments, Advances in Neural Information\\nProcessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA, 6379–6390.\\n[16] A. Mahajan et al., Maven: Multi-agent variational exploration, Advances in Neural Information Processing Systems 32\\n(2019), 7611–7622.\\n[17] Y. Niu, R. R. Paleja, and M. C. Gombolay, Multi-agent graph-attention communication and teaming, AAMAS ’21: 20th\\nInternational Conference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021,\\nACM, 964–973.\\n[18] F. A. Oliehoek and C. Amato, A concise introduction to decentralized POMDPs, Springer, 2016.\\n[19] T. Rashid et al., QMIX: monotonic value function factorisation for deep multi-agent reinforcement learning, Proceedings of\\nthe 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15,\\n2018, Proceedings of Machine Learning Research, vol. 80, PMLR, 4292–4301.\\n[20] J. Ruan et al., Gcs: Graph-based coordination strategy for multi-agent reinforcement learning, arXiv preprint\\narXiv:2201.06257 (2022).\\n[21] L. M. Schmidt et al., An introduction to multi-agent reinforcement learning and review of its application to autonomous\\nmobility, arXiv preprint arXiv:2203.07676 (2022).\\n[22] H. Shi et al., Marl sim2real transfer: Merging physical reality with digital virtuality in metaverse, IEEE Transactions on\\nSystems, Man, and Cybernetics: Systems (2022).\\n[23] S. G. Subramanian et al., Partially observable mean field reinforcement learning, AAMAS ’21: 20th International\\nConference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021, ACM,\\n537–545.\\n[24] J. N. Tsitsiklis, Asynchronous stochastic approximation and q-learning, Machine learning 16 (1994), no. 3, 185–202.\\n[25] P. Velickovic et al., Graph attention networks, stat 1050 (2017), 20.\\n[26] O. Vinyals et al., Grandmaster level in starcraft ii using multi-agent reinforcement learning, Nature 575 (2019), no. 7782,\\n350–354.\\n[27] L. Wang et al., Neural policy gradient methods: Global optimality and rates of convergence, arXiv preprint\\narXiv:1909.01150 (2019).\\n[28] T. Wu et al., Weighted mean-field multi-agent reinforcement learning via reward attribution decomposition, International\\nConference on Database Systems for Advanced Applications, Springer, 301–316.\\n[29] Z. Wu et al., A comprehensive survey on graph neural networks, IEEE transactions on neural networks and learning systems\\n32 (2020), no. 1, 4–24.\\n[30] Q. Xie et al., Learning while playing in mean-field games: Convergence and optimality, International Conference on\\nMachine Learning, PMLR, 11436–11447.\\n19\\n[31] Y. Yang et al., Mean field multi-agent reinforcement learning, Proceedings of the 35th International Conference on\\nMachine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, Proceedings of Machine\\nLearning Research, vol. 80, PMLR, 5567–5576.\\n[32] H. Zhang et al., H2gnn: Hierarchical-hops graph neural networks for multi-robot exploration in unknown environments,\\nIEEE Robotics and Automation Letters 7 (2022), no. 2, 3435–3442.\\n[33] K. Zhang, Z. Yang, and T. Başar, Multi-agent reinforcement learning: A selective overview of theories and algorithms,\\nHandbook of Reinforcement Learning and Control (2021), 321–384.\\n[34] T. Zhang et al., MFVFD: A multi-agent q-learning approach to cooperative and non-cooperative tasks, Proceedings of the\\nThirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27\\nAugust 2021, ijcai.org, 500–506, .\\n[35] Y. Zhang et al., Coordination between individual agents in multi-agent reinforcement learning, Thirty-Fifth AAAI Confer-\\nence on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence,\\nIAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event,\\nFebruary 2-9, 2021, AAAI Press, 11387–11394.\\n[36] L. Zheng et al., Magent: A many-agent reinforcement learning platform for artificial collective intelligence, Proceedings\\nof the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial\\nIntelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New\\nOrleans, Louisiana, USA, February 2-7, 2018, AAAI Press, 8222–8223.\\n[37] S. Zhou et al., Multi-agent mean field predict reinforcement learning, 2020 IEEE International Conference on Advances\\nin Electrical Engineering and Computer Applications (AEECA), IEEE, 625–629.\\n[38] Z. Zhou and G. Liu, Romfac: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on\\nstates, arXiv preprint arXiv:2205.07229 (2022).\\nHow to cite this article: Yang M, Liu G, Zhou Z, Wang J. Partially Observable Mean Field Multi-\\nAgent Reinforcement Learning Based on Graph Attention Network for UAV Swarms. Drones. 2023; 7(7):476.\\nhttps://doi.org/10.3390/drones7070476\\n'),\n",
              " Document(metadata={'Published': '2005-05-17', 'Title': 'Agents Play Mix-game', 'Authors': 'Chengling Gou', 'Summary': 'In mix-game which is an extension of minority game, there are two groups of\\nagents; group1 plays the majority game, but the group2 plays the minority game.\\nThis paper studies the change of the average winnings of agents and\\nvolatilities vs. the change of mixture of agents in mix-game model. It finds\\nthat the correlations between the average winnings of agents and the mean of\\nlocal volatilities are different with different combinations of agent memory\\nlength when the proportion of agents in group 1 increases. This study result\\nsuggests that memory length of agents in group1 be smaller than that of agent\\nin group2 when mix-game model is used to simulate the financial markets.', 'entry_id': 'http://arxiv.org/abs/physics/0505112v1', 'published_first_time': '2005-05-17', 'comment': '8 pages, 6 figures, 3 tables', 'journal_ref': None, 'doi': '10.1007/978-88-470-0502-0_12', 'primary_category': 'physics.soc-ph', 'categories': ['physics.soc-ph', 'q-fin.TR'], 'links': ['http://dx.doi.org/10.1007/978-88-470-0502-0_12', 'http://arxiv.org/abs/physics/0505112v1', 'http://arxiv.org/pdf/physics/0505112v1']}, page_content=' \\nAgents Play Mix-game \\n \\n \\n \\nChengling Gou \\n \\nPhysics Department, Beijing University of Aeronautics and Astronautics \\n37 Xueyuan Road, Haidian District, Beijing, China, 100083 \\n \\nPhysics Department, University of Oxford \\nClarendon Laboratory, Parks Road, Oxford, OX1 3PU, UK \\ngouchengling@hotmail.com, c.gou1@physics.ox.ac.uk \\n \\n \\n \\n \\nAbstract: In mix-game which is an extension of minority game, there are two groups of agents; group1 \\nplays the majority game, but the group2 plays the minority game. This paper studies the change of the \\naverage winnings of agents and volatilities vs. the change of mixture of agents in mix-game \\nmodel. It finds that the correlations between the average winnings of agents and the mean of \\nlocal volatilities are different with different combinations of agent memory length when the \\nproportion of agents in group 1 increases. This study result suggests that memory length of \\nagents in group1 be smaller than that of agent in group2 when mix-game model is used to \\nsimulate the financial markets. \\n \\nKeywords: minority game, mix-game, average winning, volatility  \\n \\n \\n \\n1. Introduction:  \\nChallet and Zhang\\'s MG model, together with the original bar model of Arthur [1], attracts \\na lot of following studies. Given the MG\\'s richness and yet underlying simplicity, the MG has \\nalso received much attention as a financial market model [2]. The MG comprises an odd \\nnumber of agents choosing repeatedly between the options of buying (1) and selling (0) a \\nquantity of a risky asset. The agents continually try to make the minority decision i.e. buy \\nassets when the majority of other agents are selling and sell when the majority of other agents \\nare buying. Neil F. Johnson [3, 4] and coworkers extended MG by allowing a variable number \\nof active traders at each timestep--- they called their modified game as the Grand Canonical \\nMinority Game (GCMG). The GCMG, and to a lesser extent the basic MG itself, can \\nreproduce the stylized facts of financial markets, such as volatility clustering and fat-tail \\ndistributions. However, there are some weaknesses in MG and GCMG. First, the diversity of \\nagents is limited, since agents all have the same memory and time-horizon. Second, in real \\nmarkets, some agents are tendency-followers, i.e. “noise traders” [5, 6, 7, 8, 9, 10, 11, 12], \\nwho effectively play a majority game; while others are “foundation traders”, who effectively \\nplay a minority game. \\nIn order to create an agent-based model which more closely mimics a real financial market, \\nI proposed a mix-game model which is a modification of MG [13]. In mix-game model there \\nare two groups of agents: each group has different memory and time-horizon. The most \\nimportant modification is to make one group plays the minority game and the other plays the \\nmajority game. Through simulations, I find out that the fluctuations of local volatilities \\nchange a lot by adding some agents who play majority game into MG, but the stylized features \\nof MG don’t change obviously except agents with memory length 1 and 2. I also give \\nsuggestions about how to use mix-game to model financial markets and show the example of \\nmodeling Shanghai stock market by means of mix-game model [13]. \\nIn this paper, I further examine the correlations between the average winnings of agents \\nand the local volatilities of systems in mix-game model when the proportion of agents in \\ngroup1 increases from 0 to 0.4. In section 2, I describe the mix-game model and the \\nsimulation conditions. In section 3, the simulation results and discussion are presented. In \\nsection 4, I calculate the quantitative correlations. In section 5, the conclusion is reached.   \\n2. Simulation: \\nMix-game model is an extension of minority game (MG), so its structure is similar to MG. \\nIn mix-game, there are two groups of agents; group1 plays the majority game, and the group2 \\nplays the minority game. N (odd number) is the total number of the agents and N1 is number \\nof agents in group1. The system resource is r = N*L, where L<1 is the proportion of resource \\nof the system. All agents compete in the system for the limited resource r. T1 and T2 are the \\ntime horizon lengths of the two groups, and m1 and m2 denote the memory lengths of the two \\ngroups, respectively.   \\nThe global information only available to the agents is a common bit-string “memory” of the \\nm1 or m2 most recent competition outcomes (1 or 0). A strategy consists of a response, i.e., 0 \\n(sell) or 1 (buy), to each possible bit string; hence there are \\n1\\n2\\n2\\nm\\nor \\n2\\n2\\n2\\nm\\n possible strategies \\nfor group1 or group2, respectively, which form full strategy spaces (FSS). At the beginning of \\nthe game, each agent is assigned s strategies and keeps them unchangeable during the game. \\nAfter each turn, agents assign one (virtual) point to a strategy which would have predicted the \\ncorrect outcome. For agents in group1, they will reward their strategies one point if they are in \\nthe majority; for agents in group2, they will reward their strategies one point if they are in the \\nminority. Agents collect the virtual points for their strategies over the time horizon T1 or T2, \\nand they use their strategies which has the highest virtual point in each turn. If there are two \\nstrategies which have the highest virtual point, agents use coin toss to decide which strategy \\nto be used. Excess demand is equal to the number of ones (buy) which agents choose minus \\nthe number of zeros (sell) which agents choose. According to a widely accepted assumption \\nthat excess demand exerts a force on the price of the asset and the change of price is \\nproportion to the excess demand in a financial market [14, 15, 16], the time series of price of \\nthe asset can be calculated based on the time series of excess demand.  \\nIn simulation, the distribution of initial strategies of agents is randomly distributed and \\nkeeps unchanged during the games. Simulation turns are 3000. The window length of local \\nvolatility is 5. Total number of agents is 201. Number of strategies per agent is 2.  \\n \\n \\n3. Simulation results and discussions \\n100\\n1000\\n10000\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nN1/N\\nmean of local volatilities\\nVol.1\\nVol.2\\nVol.3\\n \\nFig.1 means of local volatilities vs. different N1/N, Vol.1 representing mean of local volatilities of \\nm1=m2=6, T1=T2=60; Vol.2 representing mean of local volatilities of m1=6, m2=3, T1=60, T2=12; Vol.3 \\nrepresenting mean of local volatilities of m1=3, m2=6, T1=12, T2=60.  \\nFig.1 shows that means of local volatilities (Vol.1 and Vol.2) decrease while N1/N \\nincreases from 0 to 0.4 under condition of m1=m2=6, T1=T2=60 and condition of m1=6, \\nm2=3, T1=60, T2=12. But under condition of m1=3, m2=6, T1=12, T2=60, mean of local \\nvolatilities (Vol.3) has a minimum value at N1/N=0.2. \\nm1=m2=6, T1=T2=60, N=201, s=2\\n0.4\\n0.42\\n0.44\\n0.46\\n0.48\\n0.5\\n0.52\\n0.54\\n0.56\\n0.58\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nN1/N\\nAverage winnings\\nR1\\nR2\\n \\nFig.2 average winnings per agent per turn for mix-game vs. different proportion of agents in group1 when \\nm1=m2=6, T1=T2=60; R1 represents the average winning per agent per turn of group1, and R2 represents \\nthe average winning per agent per turn of group2. \\nFrom Fig. 2, one can find that the average winnings (R1 and R2) of these two groups \\nincrease when N1/N is larger than 0.1 and the average winning of group 1 is larger than that \\nof group 2. Agents in both groups benefit from the increase of the number of agents in group1. \\nComparing Fig.2 with Fig.1, one can find that the mean of local volatilities (Vol.1) decreases \\naccompanying with the increase of the average winnings of group1 and group2 (R1, R2) \\nwhile N1/N increases from 0 to 0.4 under the condition of m1=m2=6, T1=T2=60. This means \\nthat the improvement of the efficiency of systems is accordant with the improvement of the \\nperformance of individual agents under this simulation condition. This result shows that both \\nthe efficiency of systems and the performance of individual agents increase while the \\nproportion of agents in group 1 increases. Similar phenomena can be found in ecological \\nsystems, computing systems and economic systems in which agents (species, computing tasks \\nand firms) having different niches will improve the efficiencies of systems and their own \\nperformance [17]. \\nm1=6, m2=3, T1=60, T2=12, N=201, s=2\\n0.3\\n0.35\\n0.4\\n0.45\\n0.5\\n0.55\\n0.6\\n0.65\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nN1/N\\nAverage winnings\\nR1\\nR2\\n \\nFig.3 average winnings per agent per turn for mix-game vs. different proportion of agents in group1 when \\nm1=6, m2=3, T1=60, T2=12. R1, R2 have the same meaning as that indicated in Fig.2. \\nFrom Fig. 3, one can find that the average winnings (R1 and R2) of these two groups \\ndon’t change obviously when N1/N increases from 0 to 0.4, except R1 at N1/N=0.1. The \\naverage winnings of group 1 are larger than those of group 2. Comparing Fig.3 with Fig.1, \\none can find that the mean of local volatilities (Vol.2) decreases while N1/N increases from 0 \\nto 0.4 under the condition of m1=6, m2=3, T1=60, T2=12, but the change of the average \\nwinnings of group1 and group2 (R1, R2) seems more complicated than that in Fig.2.   \\nm1=3, m2=6, T1=12, T2=60, N=201 s=2\\n0.4\\n0.45\\n0.5\\n0.55\\n0.6\\n0.65\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nN1/N\\nAvarage winnings\\nR1\\nR2\\n \\nFig.4 average winnings per agent per turn for mix-game vs. different proportion of agents in group1 when \\nm1=3, m2=6, T1=12, T2=60. R1, R2 have the same meaning as that indicated in Fig.2. \\n \\nFrom Fig. 4, one can find that the average winnings (R1 and R2) of these two groups \\nincrease obviously when N1/N increases from 0 to 0.4 and the average winning of group 1 \\n(R1) is larger than that of group 2 (R2). Agents in both groups benefit from the increase of the \\nnumber of agents in group1. Comparing Fig.4 with Fig.1, one can find that the mean of local \\nvolatilities (Vol.3) decreases slightly when N1/N increase from 0 to 0.2, then it increase while \\nN1/N increases from 0.2 to 0.4 under the condition of m1=3, m2=6, T1=12, T2=60, \\naccompanying with the increase of the average winnings of group1 and group2 (R1, R2). This \\nmeans that the improvement of the performance of individual agents accompanies with the \\ndecrease of the system efficiency under this simulation condition. Agents can make profits \\nfrom the larger fluctuation of systems, which is accordant with the reality of financial \\nmarkets.  \\nFor example, the combinations of parameters in this simulation include the configuration of \\nparameters which is used to model the Shanghai Index [13]. For Shanghai Index, there are \\ntwo suitable configurations of parameters: m1=3, T1=12, m2=6, T2=60, N1=40, N=201 and \\nm1=4, m2=6, T1=T2=12, N=201, N1=72, respectively. Fig.5 shows the log-log plot of \\nShanghai-daily absolute returns and Fig.6 shows the log-log plot of mix-game absolute \\nreturns of parameters of m1=3, T1=12, m2=6, T2=60, N1=40, N=201, s=2. These two figures \\nlook very similar. This implies that we need to make m1 smaller than m2 when we mimic \\nfinancial markets by means of mix-game model. \\n \\n  Fig.5 log-log plot of Shanghai-daily absolute returns   Fig.6 log-log plot of mix-game absolute returns \\n4. Calculation of correlations \\nTable 1 correlations of R1, R1 and Vol.1 under the condition of m1=m2=6, T1=T2=60 \\nCorrelation\\nR1 \\nR2 \\nVol.1 \\nR1 \\n1 \\n \\n \\nR2 \\n0.98 \\n1 \\n \\nVol.1 \\n-0.63 \\n-0.76 \\n1 \\n \\nTable 2 correlations of R1, R1 and Vol.2 under the condition of m1=6, m2=3, T1=60, T2=12 \\nCorrelation\\nR1 \\nR2 \\nVol.2 \\nR1 \\n1 \\n \\n \\nR2 \\n-0.48 \\n1 \\n \\nVol.2 \\n0.98 \\n-0.67 \\n1 \\n \\nTable 3 correlations of R1, R1 and Vol.3 under the condition of m1=3, m2=6, T1=12, T2=60, \\nCorrelation\\nR1 \\nR2 \\nVol.3 \\nR1 \\n1 \\n \\n \\nR2 \\n0.87 \\n1 \\n \\nVol.3 \\n0.89 \\n0.82 \\n1 \\nTable 1, 2 and 3 give the quantitative results of correlations among R1, R2 and the means of \\nlocal volatilities under these three configurations of parameters. The results in table 1 and \\ntable 3 are accordant with the qualitative analysis in section 3 and show that the correlations \\nbetween R1 and R2 in both situations are positive. Table 2 gives a clearer picture about the \\ncorrelations among R1, R2 and Vol.3 under the condition of m1=3, m2=6, T1=12, T2=60; the \\ncorrelation between R1 and R2 is negative; the correlation between R1 and Vol.3 is positive \\nwhile the correlation between R2 and Vol.3 is negative. \\n5. Conclusion \\n The correlations between the average winnings of agents (R1 and R2) and the mean of \\nlocal volatilities are different with different combinations of m1 and m2 when the proportion \\nof agents in group 1 increases: the correlations are negative with parameters of m1=m2=6, \\nT1=T2=60, N=201 and s=2; under the condition of m1=6, m2=3, T1=60, T2=12, N=201 and \\ns=2, the correlation of variable pair (R1 and Vol.2) is negative while the correlation of \\nvariable pair (R2 and Vol.2) is positive; the correlations are positive with parameters of m1=3, \\nm2=6, T1=12, T2=60, N=201 and s=2. \\nThis study result suggests that m1 be smaller than m2 when mix-game model is used to \\nsimulate the financial markets. \\n \\nAcknowledgements \\nThis research is supported by Chinese Overseas Study Fund. Thanks Professor Neil F. \\nJohnson for suggesting modification of agents’ memories and discussing about the calculation \\nof average winning of agents. Thanks David Smith for providing the original program code of \\nMG. \\n \\nReference \\n1. \\nW.B. Arthur, Science 284, 107 (1999).   \\n2. \\nD. Challet, and Y. C. Zhang, Phyisca A 246, 407(1997); \\n3. \\nNeil F. Johnson, Paul Jefferies, and Pak Ming Hui, Financial Market Complexity, Oxford Press(2003); \\n4. \\nPaul Jefferies and Neil F. Johnson, Designing agent-based market models, Oxford Center for \\nComputational Finance working paper: OCCF/010702; \\n5. \\nT. Lux, Herd Behavior, Bubbles and Crashes. Economic Journal 105(431): 881-896(1995). \\n6. \\nT. Lux, and M. Marchesi Scaling and criticality in a stochastic multi-agent model of a financial \\nmarket. Nature 397(6719): 498-500 (1999).\" \\n7. \\nJ. V. Andersen, and D. Sornette, The $-game, cond-mat/0205423; \\n8. \\nChallet, Damien, Inter-pattern speculation: beyond minority, majority and $-games, arXiv: \\nphysics/0502140 v1.  \\n9. \\nF. Slanina and Y.-C. Zhang, Physica A 289, 290 (2001). \\n10. Yoon, Seong-Min, and Kim, Kyungsik, Dynamical Minority Games in Futures Exchange Markets, \\narXiv: physics/0503016 v1. \\n11. J.V. Andersen and D. Sornette, Eur. Phys. J. B 31, 141 (2003). \\n12. I. Giardina and J.-P. Bouchaud, Eur. Phys. J. B 31, 421 (2003). \\n13. Chengling Gou, Dynamic Behaviors of Mix-game Models and Its Application, arXiv: physics/0504001 \\nv1. \\n14. J.P. Bouchaud and R. Cont, Eur. Phys. J. B 6 543 (1998). \\n15. J. D. Farmer, adap-org/9812005. \\n16. J. Lakonishok, A. Shleifer, R. Thaler and R. Vishny, J. Fin. Econ. 32, 23 (1991). \\n17. Kephart, J. O., Hogg, T. & Huberman, B. A. Dynamics of computational ecosystems. Physical Review \\nA 40(1), 404-421 (1989). \\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPRobwLRHSy8",
        "outputId": "27da9744-972c-44b2-c040-576285c5a9af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-q5pSBnJ52g",
        "outputId": "d4eda7ab-9fdb-4b31-f481-0755f50dae46"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVOAGENT: Towards Automatic Multi-Agent\n",
            "Generation via Evolutionary Algorithms\n",
            "Siyu Yuan1∗, Kaitao Song2∗†,\n",
            "Jiangjie Chen1, Xu Tan2, Dongsheng Li2, Deqing Yang1†\n",
            "Fudan University1, Microsoft Research Asia2\n",
            "syyuan21@m.fudan.edu.cn, {kaitaosong, xuta, dongsli}@microsoft.com\n",
            "{jjchen19,yangdeqing}@fudan.edu.cn\n",
            "https://evo-agent.github.io\n",
            "Abstract\n",
            "The rise of powerful large language models (LLMs) has spurred a new trend in\n",
            "building LLM-based autonomous agents for solving complex tasks, especially\n",
            "multi-agent systems. Despite the remarkable progress, we notice that existing\n",
            "works are heavily dependent on human-designed frameworks, which greatly limits\n",
            "the functional scope and scalability of agent systems. How to automatically extend\n",
            "the specialized agent to multi-agent systems to improve task-solving capability\n",
            "still remains a significant challenge. In this paper, we introduce EVOAGENT, a\n",
            "generic method to automatically extend expert agents to multi-agent systems via\n",
            "the evolutionary algorithm, thereby improving the effectiveness of LLM-based\n",
            "agents in solving tasks. Specifically, we consider the existing agent frameworks\n",
            "as the initial individual and then apply a series of evolutionary operators (e.g.,\n",
            "mutation, crossover, selection, etc.) to generate multiple agents with diverse agent\n",
            "settings. EVOAGENT can be generalized to any LLM-based agent framework,\n",
            "and can automatically extend the existing agent framework to multi-agent systems\n",
            "without any extra human designs. Experimental results across various tasks have\n",
            "shown that EVOAGENT can automatically generate multiple expert agents and\n",
            "significantly enhance the task-solving capabilities of LLM-based agents.\n",
            "1\n",
            "Introduction\n",
            "Recently, the advent of large language models (LLMs) [1, 2, 3, 4] have shown remarkable capabilities\n",
            "in solving language understanding, reasoning, and generation tasks. Based on the foundation of LLMs,\n",
            "many research works [5, 6, 7, 8, 9, 10, 11] have discovered that by empowering multiple advanced\n",
            "skills (e.g., planning, tool, memory and so on), we can develop more powerful autonomous agents to\n",
            "solve more challenging tasks. Therefore, how to design and leverage LLM-based autonomous agents\n",
            "to tackle more diverse and complex real-world applications has attracted enormous interest.\n",
            "Generally, many real-world scenarios are usually complex, encompassing a variety of challenging\n",
            "tasks that are beyond the capability of a single agent. To address this point, we notice that human\n",
            "society is composed of vast individuals, each possessing their unique characteristics. By selecting,\n",
            "orchestrating, and cooperating with different individuals, humans can form an efficient team group\n",
            "to handle complicated missions in the real world. Therefore, there has been an increasing trend\n",
            "to develop multi-agent collaboration frameworks (e.g., MetaGPT [10], AutoGen [12], Camel [13],\n",
            "∗The first two authors have equal contributions. This work was done when the first author was an intern at\n",
            "Microsoft Research Asia.\n",
            "† Corresponding authors.\n",
            "Preprint. Under review.\n",
            "arXiv:2406.14228v2  [cs.AI]  11 Jul 2024\n",
            "Generative Agents [11]) to simulate human behaviors for solving complex tasks. By developing a\n",
            "series of expert agents with diverse settings, multi-agent systems enable us to reveal emergent abilities\n",
            "among multiple agents and synergize their specialized expertise to achieve superior performance,\n",
            "akin to simulating human populations. Nevertheless, it is worthy noting that, in most of (multi)-agent\n",
            "frameworks, their designs heavily depend on handcrafted settings, including character roles, task\n",
            "scopes, skills, and prompt settings. Although we admit that meticulous human design is quite useful\n",
            "for instructing LLM-based agents to understand tasks, it also limits scaling up the number of agents to\n",
            "further improve performance due to expensive human labor. Considering the increasing popularity of\n",
            "LLM-based autonomous agents, how to create a generic agent generation paradigm to automatically\n",
            "build multi-agent systems has emerged as a critical challenge.\n",
            "In this paper, we introduce a novel method, EVOAGENT, that formulates agent generation as the\n",
            "evolutionary processing [14] in human society. Specifically, to align human society, each agent\n",
            "can be considered as individuals that can procreate its population across successive generations.\n",
            "Motivated by this mechanism, we can simulate such a human behavior to automatically generate\n",
            "multiple agents based on any pre-defined agents. Therefore, EVOAGENT can be considered as a\n",
            "one-shot agent generation method that starts from a specialized agent as the initial agent, and then\n",
            "considers its settings (e.g., role, skills, prompts, and so on) as the variables to be evolved. With a\n",
            "series operation of EAs (e.g., selection, crossover, mutation), EVOAGENT can automatically create\n",
            "multiple evolutionary agents based on the initial specialized agent. Moreover, EVOAGENT is not\n",
            "limited to the infrastructure of agent frameworks, as it is a generic multi-agent generation method.\n",
            "Thus, it can be applied to any agent framework and expanded to multi-agent systems without any\n",
            "extra human effort.\n",
            "We conduct experiments on multiple datasets, including knowledge-based question answering and\n",
            "multi-modal reasoning (§ 4.1), interactive scientific solving (§ 4.2) and real-world complex planning\n",
            "(§ 4.3). Experimental results indicate that EVOAGENT can generate multiple agents with diverse\n",
            "skills and harness their capabilities to consistently improve model performance in different scenarios.\n",
            "Besides, to validate the scalability of EVOAGENT in creating massive agents, we also apply our\n",
            "method to some conversational scenarios (e.g., debate), and the results also indicate the potential of\n",
            "EVOAGENT in generating multiple diverse agents. Overall, the contributions of this paper can be\n",
            "summarized as below:\n",
            "• We introduce EVOAGENT, a simple and generic multi-agent generation method to improve the\n",
            "effectiveness of LLM-based agents in solving tasks. EVOAGENT can automatically generate new\n",
            "expert agents and is applicable to any agent framework.\n",
            "• We formulate the agent generation processing as an evolutionary pipeline, that encompasses\n",
            "multiple operators (e.g., selection, crossover, mutation) to generate agent population without\n",
            "additional human supervision.\n",
            "• We conduct extensive experiments on various tasks and demonstrate the effectiveness, scalability,\n",
            "and generality of our EVOAGENT. Particularly, EVOAGENT can significantly enhance the perfor-\n",
            "mance of LLM-based agents in both challenging open-world scenarios and complex real-world\n",
            "planning by generating more specialized agents.\n",
            "2\n",
            "Related Work\n",
            "LLM-based Autonomous Agents\n",
            "With the emergence of powerful large language models [1, 2, 3,\n",
            "4], many researchers have endeavored to develop advanced autonomous agents [5, 6, 7] empowered by\n",
            "multiple high-level LLM skills (e.g., personas [11, 15, 16], planning [9, 17, 18, 19], tool [8, 6, 20, 21]\n",
            "and memory [22, 23]). Some of them also extend agent frameworks to multi-agent collaboration\n",
            "(e.g., MetaGPT [24], Generative Agents [11], AutoGen [12], Camel [13], AgentVerse [25] and so\n",
            "on), by designing multiple specific roles. These systems also demonstrate satisfactory performance in\n",
            "addressing massive, challenging tasks. However, it is worth noting that most of the popular agent\n",
            "frameworks heavily relied on handcrafted designs. The abundant human efforts necessitated by these\n",
            "systems also limit the adaptability and flexibility of agents to handle unexpected challenges [26, 27,\n",
            "25, 10]. In this paper, we propose EVOAGENT, a method that can be applied to any LLM-based\n",
            "agent framework and easily extend to multi-agent systems. By using EA, our method allows us to\n",
            "iteratively generate and optimize multiple agents with diverse settings.\n",
            "2\n",
            "Algorithm 1: Multi-Agent Generation with Evolutionary Algorithm\n",
            "Require: Initial agent A(0,0), population size N per iteration, number of iterations T,\n",
            "quality-check module LLMQuality(⋅), evolutionary operations EvoCrossover(⋅) and\n",
            "EvoMutation(⋅), EvoUpdate(⋅)\n",
            "Input: Initial result R0 derived from A(0,0)\n",
            "Output: Final result RT\n",
            "1 for t = 1 to T do\n",
            "2\n",
            "Crossover: Update the settings of parent agents based on their generated results and initial\n",
            "agent: {A\n",
            "′\n",
            "(0,t−1),A\n",
            "′\n",
            "(1,t−1),...,A\n",
            "′\n",
            "(N−1,t−1)} ←\n",
            "EvoCrossover({R(0,t−1),R(1,t−1),...,R(N−1,t−1)},A(0,0));\n",
            "3\n",
            "Mutation: Generate N ′(N ′ > N) child agents based on parent agents and initial agent:\n",
            "{A(0,t),A(1,t),...,A(N ′−1,t)} ←\n",
            "EvoMutation({A\n",
            "′\n",
            "(0,t−1),A\n",
            "′\n",
            "(1,t−1),...,A\n",
            "′\n",
            "(N−1,t−1)},A(0,0))\n",
            "4\n",
            "Selection: Select high-quality agents with quality-check module:\n",
            "{At\n",
            "0,...,At\n",
            "N−1} ←LLMQuality({A(0,t),A(1,t),...,A(N ′−1,t)});\n",
            "5\n",
            "Result Update: Generate new result from new agents:\n",
            "{R(0,t),R(1,t),...,R(N−1,t)} ←{A(0,t),A(1,t),...,A(N−1,t)}\n",
            "6\n",
            "Integrate their results as a natural selection processing:\n",
            "Rt ←EvoUpdate({R(0,t),R(1,t),...,R(N−1,t)},Rt−1);\n",
            "7 return RT ;\n",
            "Agent Generation\n",
            "Recent studies have shown that assigning personas or roles to LLM-based\n",
            "autonomous agents can influence their behavior and performance in generation tasks [28, 29, 30, 13].\n",
            "Current methods primarily involve manually assigning these personas and limit multi-agent col-\n",
            "laboration to single or fixed roles, which requires significant human effort and hinders generaliza-\n",
            "tion [13, 12, 24, 10]. To address this, some frameworks like AgentVerse [25] and AutoAgents [31]\n",
            "have been proposed to automatically generate unlimited agents for collaborative task completion.\n",
            "However, these methods still heavily depend on human-designed interventions, which limits their\n",
            "scalability and functionality. For example, AutoAgents requires agent settings to satisfy a “Planner\n",
            "- Agent Observer - Plan Observer” framework. Meanwhile, AgentVerse formulates a pipeline of\n",
            "“Expert Recruitment - Collaborative Decision Making - Action Execution - Evaluation” to build\n",
            "agents. These architectures also limit the task scope of designing agents. In contrast, EVOAGENT\n",
            "can automatically formulate the current agent frameworks to multi-agent systems with high-quality\n",
            "generated expert agents by using EAs, which is flexible and adaptable to various agent frameworks.\n",
            "3\n",
            "Method\n",
            "Generally, human society comprises a broad spectrum of individuals from diverse cultures, encom-\n",
            "passing multiple generations. To solve specific tasks, human society usually involves a lot of expert\n",
            "individuals and aggregates their specialized expertise to achieve better answer. Thus, it can be\n",
            "considered as the foundation to facilitate multi-agent collaborations. To fulfill this point, how to\n",
            "automatically create multiple agents would be very critical. Inspired by evolutionism, we formulate\n",
            "agent generation as an evolutionary process to help us generate multiple agents without any human\n",
            "labor.\n",
            "3.1\n",
            "Preliminary\n",
            "Evolutionary algorithm (EA) [32, 33], is a general algorithm to simulate the biological behaviors in\n",
            "evolution, including reproduction, mutation, recombination, and selection. By introducing genetic\n",
            "algorithm [34, 35, 36, 37, 38] of the “survival of the fittest” mechanism, it can also be considered as\n",
            "an optimization method to improve individuals. Therefore, EAs also belong to the non-parametric\n",
            "learning method, which can be applied to any framework. All we need to do is define which\n",
            "parts should be evolved and the corresponding evolutionary operators. We also note some recent\n",
            "3\n",
            "works [39, 40] indicate the potential of EAs that can be applied to optimize discrete prompts. So, in\n",
            "this paper, we explore how to formulate the agent generation problem as an evolutionary task.\n",
            "3.2\n",
            "EVOAGENT\n",
            "By assigning various settings to specific skills (e.g., role-playing, planning, tools and so on), agents\n",
            "could exhibit diverse task-solving capabilities. Therefore, our objective is to produce a population\n",
            "of agents with distinct skills, to establish effective multi-agent systems. To fulfill this point, we\n",
            "treat each specialized agent as an unique individual and denote each skill as the part to be evolved,\n",
            "akin to humans. So, we consider the procedure of agent generation to be evolutionary processing.\n",
            "Specifically, existing frameworks usually describe agent skills as the language. Thus, we can employ\n",
            "LLM to simulate evolutionary operators to update the system settings of agents and create new agents.\n",
            "Here, we formulate the procedure of EVOAGENT as a four-stage pipeline:\n",
            "Query: Please create a travel plan where I'll depart from Washington and head to Myrtle \n",
            "Beach for a 3-day trip from March 13th to March 15th, 2022. Can you help me keep this \n",
            "journey within a budget of $1,400? It's vital that my accommodations are pet-friendly.\n",
            "STEP 1: Initialization\n",
            "Initial \n",
            "Agent\n",
            "Human \n",
            "Written\n",
            "STEP 2: EA Operation Crossover & Mutation\n",
            "Day 1:\n",
            "Current City: from Washington to Myrtle Beach\n",
            "Lunch: Exotic India\n",
            "Attraction: SkyWheel Myrtle Beach\n",
            "Accommodation: Cozy Brooklyn Room\n",
            "Initial Agent\n",
            "Accommodation \n",
            "Agent\n",
            "Transportation \n",
            "Agent\n",
            "Day 1:\n",
            "Current City: from Washington to Myrtle Beach\n",
            "Breakfast: Exotic India, Myrtle Beach\n",
            "Lunch: Catfish Charlie's, Myrtle Beach\n",
            "Attraction: SkyWheel Myrtle Beach\n",
            "Accommodation: Large sunny park slope apartment, \n",
            "pet-friendly\n",
            "Day 1:\n",
            "Current City: from Washington to Myrtle Beach\n",
            "Transportation: Flight Number: F3792603\n",
            "Breakfast: Exotic India, Myrtle Beach\n",
            "Lunch: Catfish Charlie's, Myrtle Beach\n",
            "Attraction: SkyWheel Myrtle Beach\n",
            "Accommodation: Cozy Brooklyn Room\n",
            "Hotel Agent\n",
            "STEP 3: Selection\n",
            "Quality Check\n",
            "This agent has a duplicate type with \n",
            "Accommodation Agent, so it is discarded.\n",
            "STEP 4: Results Update\n",
            "Update \n",
            "Operation\n",
            "Day 1:\n",
            "Current City: from Washington to Myrtle Beach\n",
            "Transportation: Flight Number: F3792603\n",
            "Breakfast: Exotic India, Myrtle Beach\n",
            "Lunch: Catfish Charlie's, Myrtle Beach\n",
            "Attraction: SkyWheel Myrtle Beach\n",
            "Accommodation: Large sunny park slope apartment, pet-friendly\n",
            "Figure 1:\n",
            "The illustration of EVOAGENT. With the\n",
            "generated multiple expert agents, EVOAGENT can gen-\n",
            "erate a better travel plan to meet user preferences. For\n",
            "EA operators, Crossover can improve the results of\n",
            "parent agents by adjusting existing details (e.g., the in-\n",
            "formation marked as blue). Mutation can introduce\n",
            "new variations to refine the results of parent agents by\n",
            "generating child agents with new characteristics (e.g.,\n",
            "the information marked as red).\n",
            "STEP 1: Initialization\n",
            "To conduct EAs,\n",
            "we first need to confirm our initial agents.\n",
            "Here, we enable EVOAGENT to start\n",
            "from a pre-defined agent framework (e.g.,\n",
            "MetaGPT [10] and AutoGen [12]), which\n",
            "serves as the initial (parent) agents. More-\n",
            "over, we also define which parts of this\n",
            "agent should be upgraded. Generally, since\n",
            "EAs is a generic algorithm, EVOAGENT\n",
            "is applicable to any agent frameworks and\n",
            "extends them as multi-agent frameworks.\n",
            "We will then explore how to generate new\n",
            "agents in the next steps.\n",
            "STEP 2: Crossover & Mutation\n",
            "In the\n",
            "first iteration, we directly use the initial\n",
            "agents as the parents. And then, we design\n",
            "two kinds of evolutionary operators, named\n",
            "Crossover and Mutation. For Crossover,\n",
            "we first enable the parent agents to generate\n",
            "results based on user requests. Then, based\n",
            "on the generated results, we ask LLMs to\n",
            "check which skills should be improved and\n",
            "then update them. This mechanism allows\n",
            "us to generate child agents in new settings\n",
            "without requiring any human labor. More-\n",
            "over, we also need to guarantee the diver-\n",
            "sity between the child agents and parents.\n",
            "To this end, we design a Mutation operation\n",
            "that requires LLMs to compare the child\n",
            "agents and parent agents and then modify\n",
            "the child agents to make them distinct from\n",
            "their parents while maintaining their task-\n",
            "solving capability. Based on these evolu-\n",
            "tionary operators, we can generate effective\n",
            "and diverse agents during one iteration. Be-\n",
            "sides, as we also need to conduct multiple iterations, we will append all agents generated in the\n",
            "previous generation into the next iteration. How to select these agents during each iteration will be\n",
            "introduced next.\n",
            "STEP 3: Selection\n",
            "Based on the above steps, we can obtain multiple candidate agents with diverse\n",
            "settings. To guarantee the quality of each agent, we also introduce a selection mechanism like EAs.\n",
            "Here, we conduct a quality-check module with an LLM to detect whether the generated agents can\n",
            "satisfy it has inherited the characteristics and maintained differences from parent agents. We will\n",
            "select N child agents as the evolved agents in each iteration.\n",
            "4\n",
            "Table 1: Results of LLMs with different methods on Logic Grid Puzzle (Logic), Trivia Creative\n",
            "Writing (Writing) and Codenames Collaborative (Codenames). The best results are bolded, and the\n",
            "second best ones are underlined.\n",
            "Model\n",
            "Method\n",
            "Logic Writing Codenames\n",
            "LLama2-13B-Chat\n",
            "Direct\n",
            "4.00\n",
            "28.00\n",
            "0.00\n",
            "CoT\n",
            "26.00\n",
            "46.00\n",
            "18.00\n",
            "Self-Refine3\n",
            "33.50\n",
            "31.20\n",
            "12.37\n",
            "SPP\n",
            "0.00\n",
            "4.00\n",
            "1.00\n",
            "EVOAGENT(1,3)\n",
            "35.50\n",
            "49.60\n",
            "27.83\n",
            "GPT-3.5\n",
            "Direct\n",
            "48.00\n",
            "56.20\n",
            "76.29\n",
            "CoT\n",
            "47.50\n",
            "51.00\n",
            "71.13\n",
            "Self-Refine3\n",
            "47.50\n",
            "59.19\n",
            "46.39\n",
            "SPP\n",
            "56.00\n",
            "54.40\n",
            "61.86\n",
            "EVOAGENT(1,3)\n",
            "71.50\n",
            "60.80\n",
            "79.38\n",
            "GPT-4\n",
            "Direct\n",
            "60.50\n",
            "75.40\n",
            "79.38\n",
            "CoT\n",
            "65.50\n",
            "74.00\n",
            "80.41\n",
            "Self-Refine3\n",
            "64.50\n",
            "74.60\n",
            "79.38\n",
            "SPP\n",
            "64.50\n",
            "79.20\n",
            "78.35\n",
            "EVOAGENT(1,3)\n",
            "77.00\n",
            "84.40\n",
            "84.53\n",
            "STEP 4: Results Update\n",
            "Based on the above steps, we obtain many new agents that evolved from\n",
            "parent agents, but with diverse settings. To improve task-solving capabilities, we ask each child agent\n",
            "to generate candidate results and then use LLMs to integrate these candidates with the result from the\n",
            "previous iteration into a new result, akin to a natural selection processing stage. Moreover, we can\n",
            "automatically generate more agents by repeating the operations from step 2 to step 4 until the number\n",
            "of agents has fulfilled our targets.\n",
            "By introducing EA, EVOAGENT enables us to automatically extend the existing agent framework\n",
            "to a multi-agent system without any extra human designs. The mechanism also makes EVOAGENT\n",
            "can be applied to any agent framework without any prerequisites. The entire process is illustrated in\n",
            "Figure 1. And we also present the details of EVOAGENT in Algorithm 1.\n",
            "4\n",
            "Experiment\n",
            "In this section, we adopt EVOAGENT to multiple applications to illustrate that EVOAGENT can\n",
            "help LLM-based agents better accomplish tasks with multi-agent generation.3 Furthermore, we also\n",
            "demonstrate that EVOAGENT can be applicable in supporting currently widely used multi-agent\n",
            "frameworks, such as MetaGPT [10], AutoGen [12], and Camel [13].\n",
            "4.1\n",
            "NLP and Multi-Modal Tasks\n",
            "Benchmarks\n",
            "To align previous experiences (e.g., Self-Refine [41] and Solo Performance Prompt-\n",
            "ing [42]), we select three NLP knowledge-intensive and reasoning-intensive tasks from [42] and one\n",
            "multi-modal task:\n",
            "• Logic Grid Puzzle is a reasoning task with 200 puzzles featuring 2 to 5 unique occupants in\n",
            "different houses. The aim is to identify house numbers for one occupant with provided clues.\n",
            "• Trivia Creative Writing is a knowledge-intensive task consisting of 100 instances. This task\n",
            "requires a model to write a coherent story while incorporating answers to N trivia questions.\n",
            "• Codenames Collaborative is a reasoning-intensive task with 50 instances. It involves a model\n",
            "identifying target words based on a given hint and a complete list of words.\n",
            "• MMMU [43] is a comprehensive benchmark for college-level, multi-discipline multi-modal\n",
            "understanding and reasoning. MMMU has three levels of difficulty: easy, medium, and hard. We\n",
            "evaluate EVOAGENT against baselines using the multiple-choice questions in the validation set of\n",
            "MMMU, which includes 847 questions spanning 30 different domains.\n",
            "3The data examples of EVOAGENT on these tasks are provided in Appendix F.\n",
            "5\n",
            "CoT\n",
            "SR\n",
            "SPP Ours\n",
            "45\n",
            "50\n",
            "55\n",
            "60\n",
            "65\n",
            "Accuracy(%)\n",
            "All\n",
            "CoT\n",
            "SR\n",
            "SPP Ours\n",
            "55\n",
            "65\n",
            "75\n",
            "Easy\n",
            "CoT\n",
            "SR\n",
            "SPP Ours\n",
            "45\n",
            "50\n",
            "55\n",
            "60\n",
            "Medium\n",
            "CoT\n",
            "SR\n",
            "SPP Ours\n",
            "25\n",
            "35\n",
            "45\n",
            "55\n",
            "Hard\n",
            "GPT-4V\n",
            "Gemini-Pro\n",
            "Figure 2: Overall results of GPT-4V and Gemini-Pro with different methods on the MMMU validation\n",
            "set. We also compare the performance of GPT-4V and Gemini-Pro across three difficulty levels.\n",
            "Baselines\n",
            "For NLP tasks, we select LLama2-13B-Chat [3], GPT-3.5 [44] and GPT-4 [1] as our\n",
            "backbone networks. We compare EVOAGENT with 0-shot learning (Direct), Chain-of-thought (CoT)\n",
            "prompting [45] and Self-Refine [41] and Solo Performance Prompting (SPP) [42]. For Self-Refine,\n",
            "we follow [41] to design feedback and refine prompts with three iterations. SPP is not a multi-agent\n",
            "collaboration framework but a prompting strategy that asks a single LLM to identify and discuss with\n",
            "multiple personas with few-shot learning. For SPP, we follow the original setting [42] to make a fair\n",
            "comparison. For MMMU, we select GPT-4V [46] and Gemini-Pro as the backbone and compare\n",
            "EVOAGENT with CoT prompting, Self-Refine, and SPP. 4\n",
            "Evaluation Metrics\n",
            "For all benchmarks, we adhere to the evaluation metrics specified in the\n",
            "original setting. Specifically, for Logic Grid Puzzle and MMMU tasks, we report the accuracy of\n",
            "all questions. For Trivia Creative Writing, we measure the ratio of correctly mentioned answers in\n",
            "the trivia questions. For Codenames Collaborative, we calculate the overlapping ratio between the\n",
            "predicted words from the Guesser and the target words as the metric.\n",
            "Result & Analysis\n",
            "In our experiments, we adopt the agent settings of [42] (for NLP tasks) and\n",
            "[43] (for MMMU) as the initial agent. For our method, we denote it as EVOAGENT(N,T ), where N\n",
            "is the population size generated in each iteration, and T is the number of iterations. Here, to align\n",
            "with Self-Refine, we set N as 1 and T as 3, which means we conduct three iterations, each of which\n",
            "generates a new expert agent. Our results are reported in Table 1, and we can observe:\n",
            "1. By utilizing multiple generated agents, EVOAGENT can greatly improve LLM performances in\n",
            "both NLP knowledge and reasoning tasks. We also compare EVOAGENT with some pre-defined\n",
            "agent generation frameworks, e.g., AgentVerse [25] and AutoAgent [31]. The results shown in\n",
            "Appendix B prove that the EVOAGENT is even better than these agent generation frameworks.\n",
            "2. When using weaker LLMs (e.g., LLama2-13B-Chat), SPP usually produces poor performances,\n",
            "consistent with the findings in [42]. This suggests the limited effectiveness of SPP in smaller and\n",
            "less capable models. However, EVOAGENT can provide consistent improvements among each\n",
            "LLM, proving its strong generalization by using diverse generated agents.\n",
            "In addition, Figure 2 shows that Self-Refine (SR) and SPP degrade performance compared to CoT\n",
            "prompting in MMMU task. However, EVOAGENT can generate multiple domain-specific agents and\n",
            "thus improve multi-modal models in addressing scientific questions across various difficulty levels.\n",
            "4.2\n",
            "Interactive Scientific Solving Simulation\n",
            "Benchmark\n",
            "Compared with traditional NLP or multi-modal tasks, autonomous agents usually need\n",
            "to perform problem-solving abilities akin to humans in interactive and open-world environments.\n",
            "Currently, we choose ScienceWorld [47], a complex interactive environment requiring skills in\n",
            "long-term memory, sub-task decomposition, and scientific and commonsense knowledge. Here, we\n",
            "evaluate 30 scientific tasks in ScienceWorld to demonstrate the capability of EVOAGENT in solving\n",
            "tasks in more challenging open-world environments.\n",
            "4The detailed model parameters and versions and full prompts for these methods can be found in Appendix A.\n",
            "6\n",
            "Table 2: Average Scores of different methods on Sci-\n",
            "enceWorld. We also report performance on three\n",
            "difficult-level groups based on the average length of\n",
            "the oracle agent’s trajectories [48].\n",
            "Model\n",
            "Overall\n",
            "Long Medium Short\n",
            "GPT-3.5\n",
            "17.12\n",
            "6.28\n",
            "19.91\n",
            "27.90\n",
            "w/ EVOAGENT(1,1)\n",
            "19.02\n",
            "7.25\n",
            "18.87\n",
            "33.26\n",
            "GPT-4\n",
            "27.97 10.58\n",
            "36.00\n",
            "42.41\n",
            "w/ EVOAGENT(1,1)\n",
            "30.42 11.38\n",
            "36.17\n",
            "48.67\n",
            "Baseline and Evaluation Metrics\n",
            "Follow-\n",
            "ing [48], we require LLMs to perform an\n",
            "action at each step by using in-context learn-\n",
            "ing 5. For evaluation, each task in Science-\n",
            "World includes some sub-tasks, and we re-\n",
            "port the results by calculating the completed\n",
            "sub-tasks for the whole task.\n",
            "Result & Analysis\n",
            "For EVOAGENT, we\n",
            "adopt the agent framework with original set-\n",
            "tings in [48] as the initial agent. Since each\n",
            "step in ScienceWorld requires using EA, we\n",
            "set the population size N as 1 and the itera-\n",
            "tions T as 1 for efficiency, denoted as EVOA-\n",
            "GENT(1,1). Results in Table 2 show that:\n",
            "1. EVOAGENT can also extend interactive agents to multi-agent systems in solving complete scientific\n",
            "tasks in dynamic, open-world environments and consistently improve the performance of LLMs.\n",
            "2. Our method exhibits the most substantial improvement in short-trajectory tasks, with less sig-\n",
            "nificant gains in medium and long-trajectory tasks. We argue that the capability of multi-agent\n",
            "systems will also be affected by a longer context. We also expect to investigate the effect of long\n",
            "context on multi-agent systems in the future.\n",
            "Generally, these results also demonstrate the generalization of EVOAGENT, which can also be used\n",
            "for solving interactive tasks in an open-world environment.\n",
            "4.3\n",
            "Real-World Scenarios\n",
            "Benchmark\n",
            "Moreover, in addition to performing actions in interactive environments, planning in\n",
            "complex and realistic environments is also a crucial skill for building autonomous agents. To validate\n",
            "this point, we also select TravelPlanner [49], a benchmark designed to evaluate language agents in\n",
            "real-world complex planning with multiple constraints.\n",
            "Baseline and Evaluation Metrics\n",
            "Following [49], we select Mistral-7B [50], GPT-3.5, Gemini-\n",
            "Pro [2] and GPT-4 as our backbone models. We compare EVOAGENT with 0-shot learning (Direct),\n",
            "CoT prompting, SPP, and Self-Refine within each backbone model. Furthermore, we also attempt\n",
            "the ReAcT method [51] for GPT-3.5, which introduces a virtual ‘think’ action to generate sub-\n",
            "tasks during the action planning process. For evaluation, we adhere to the original metrics from\n",
            "TravelPlanner, reporting the delivery rate, commonsense constraint pass rate, hard constraint pass\n",
            "rate, and final pass rate for all methods 6.\n",
            "Result & Analysis\n",
            "For EVOAGENT, we adopt the original settings in TravelPlanner as the initial\n",
            "agent. Results in Table 3 show that:\n",
            "1. EVOAGENT can generate specialized agents, such as those focused on culinary experiences,\n",
            "transportation, and attractions. Therefore, the generated travel plans are more aligned with user\n",
            "preferences (hard constraints) and commonsense rules;\n",
            "2. Although existing paradigms (e.g., CoT, ReAct, Self-Refine, SPP) have demonstrated decent re-\n",
            "sults in some conventional NLP tasks, they still lack capability in handling complex planning tasks\n",
            "within TravelPlanner. These results also demonstrate that only using human-design prompting\n",
            "strategies is insufficient to handle complex planning tasks.\n",
            "3. By using EVOAGENT to automatically generate multiple agents and forming a multi-agent\n",
            "collaboration paradigm, we can develop higher-quality plans that better meet user preferences.\n",
            "That also indicates the significance of multi-agent systems for complex planning tasks.\n",
            "5The introduction of the settings of LLMs are shown in Appendix C.\n",
            "6Detailed introduction of experiment settings is provided in Appendix D.\n",
            "7\n",
            "Table 3: Main results of different LLMs and planning strategies on the TravelPlanner validation set.\n",
            "EVOAGENT(N,T ) indicates that the population size per iteration is N and the number of iterations is\n",
            "T. The best results are bolded, and the second best ones are underlined.\n",
            "Model\n",
            "Method\n",
            "Delivery Commonsense Hard Constraint Final\n",
            "Rate\n",
            "Micro Macro Micro\n",
            "Macro\n",
            "Mistral-7B\n",
            "Direct\n",
            "100.0\n",
            "64.7\n",
            "2.2\n",
            "3.1\n",
            "0.0\n",
            "0.0\n",
            "CoT\n",
            "100.0\n",
            "60.5\n",
            "1.1\n",
            "1.0\n",
            "0.0\n",
            "0.0\n",
            "SPP\n",
            "100.0\n",
            "55.1\n",
            "0.0\n",
            "0.7\n",
            "0.6\n",
            "0.0\n",
            "Self-Refine3\n",
            "100.0\n",
            "58.3\n",
            "0.0\n",
            "0.7\n",
            "0.0\n",
            "0.0\n",
            "EVOAGENT(1,3)\n",
            "100.0\n",
            "60.1\n",
            "2.2\n",
            "4.5\n",
            "0.6\n",
            "0.0\n",
            "GPT-3.5\n",
            "Direct\n",
            "100.0\n",
            "57.3\n",
            "3.9\n",
            "11.0\n",
            "3.3\n",
            "0.0\n",
            "CoT\n",
            "100.0\n",
            "61.0\n",
            "2.8\n",
            "10.0\n",
            "3.3\n",
            "0.0\n",
            "ReAct\n",
            "82.2\n",
            "42.3\n",
            "0.6\n",
            "11.9\n",
            "4.6\n",
            "0.0\n",
            "SPP\n",
            "99.4\n",
            "54.6\n",
            "1.7\n",
            "3.8\n",
            "1.1\n",
            "0.0\n",
            "Self-Refine3\n",
            "100.0\n",
            "56.0\n",
            "1.7\n",
            "3.1\n",
            "1.1\n",
            "0.0\n",
            "EVOAGENT(1,3)\n",
            "100.0\n",
            "64.2\n",
            "7.8\n",
            "11.0\n",
            "4.4\n",
            "1.1\n",
            "EVOAGENT(1,5)\n",
            "100.0\n",
            "61.0\n",
            "5.0\n",
            "12.6\n",
            "5.0\n",
            "0.5\n",
            "Gemini-Pro\n",
            "Direct\n",
            "90.0\n",
            "61.7\n",
            "7.8\n",
            "16.4\n",
            "7.8\n",
            "0.6\n",
            "CoT\n",
            "90.0\n",
            "61.4\n",
            "7.2\n",
            "10.0\n",
            "6.1\n",
            "1.7\n",
            "SPP\n",
            "100.0\n",
            "67.6\n",
            "7.8\n",
            "10.2\n",
            "3.9\n",
            "1.1\n",
            "Self-Refine3\n",
            "95.6\n",
            "65.8\n",
            "6.1\n",
            "15.0\n",
            "4.4\n",
            "0.6\n",
            "EVOAGENT(1,3)\n",
            "100.0\n",
            "73.5\n",
            "12.8\n",
            "16.9\n",
            "7.2\n",
            "1.7\n",
            "EVOAGENT(1,5)\n",
            "100.0\n",
            "74.0\n",
            "8.9\n",
            "21.2\n",
            "11.7\n",
            "2.2\n",
            "GPT-4\n",
            "Direct\n",
            "100.0\n",
            "79.4\n",
            "15.8\n",
            "27.5\n",
            "16.1\n",
            "2.2\n",
            "CoT\n",
            "100.0\n",
            "76.7\n",
            "11.7\n",
            "22.4\n",
            "12.8\n",
            "2.2\n",
            "SPP\n",
            "96.7\n",
            "70.6\n",
            "5.6\n",
            "11.4\n",
            "7.8\n",
            "0.6\n",
            "Self-Refine3\n",
            "98.9\n",
            "75.3\n",
            "7.2\n",
            "12.4\n",
            "7.2\n",
            "1.1\n",
            "EVOAGENT(1,3)\n",
            "100.0\n",
            "81.5\n",
            "21.1\n",
            "31.4\n",
            "18.9\n",
            "7.2\n",
            "4.4\n",
            "Ablation Studies\n",
            "To better understand the value of EVOAGENT, we conduct detailed analyses on TravelPlanner,\n",
            "focusing on the impact of population size and the effectiveness of the quality-check module in the\n",
            "selection stage.\n",
            "Table 4: Average commonsense constraint\n",
            "pass rate (Com.) and hard constraint pass rate\n",
            "(Hard) of ablated variants on TravelPlanner.\n",
            "Method\n",
            "w/o QC\n",
            "w/ QC\n",
            "Com. Hard Com. Hard\n",
            "Direct\n",
            "-\n",
            "-\n",
            "59.5\n",
            "13.7\n",
            "Suggest3\n",
            "-\n",
            "-\n",
            "61.7\n",
            "8.4\n",
            "Overgen3\n",
            "-\n",
            "-\n",
            "61.4\n",
            "10.7\n",
            "PromptRefine3\n",
            "-\n",
            "-\n",
            "63.0\n",
            "13.8\n",
            "Different Population Size\n",
            "EVOAGENT(1,3)\n",
            "68.9\n",
            "14.0\n",
            "68.9\n",
            "14.0\n",
            "EVOAGENT(2,3)\n",
            "62.8\n",
            "12.7\n",
            "67.0\n",
            "15.2\n",
            "EVOAGENT(3,3)\n",
            "62.7\n",
            "13.7\n",
            "66.8\n",
            "15.8\n",
            "Different Selection Stategies\n",
            "Random\n",
            "62.9\n",
            "12.7\n",
            "67.1\n",
            "15.0\n",
            "PK\n",
            "63.5\n",
            "13.6\n",
            "66.4\n",
            "14.5\n",
            "All-in\n",
            "61.9\n",
            "13.2\n",
            "67.1\n",
            "17.0\n",
            "Experiment Settings\n",
            "We evaluate the performance\n",
            "of different LLMs at varying population sizes N\n",
            "with fixed iteration number 3, denoted as EVOA-\n",
            "GENT(N,3), both with and without the quality-check\n",
            "module (QC). We employ an LLM that shares the\n",
            "same backbone as the initial agent for updates. To\n",
            "select results from candidates for this LLM to update,\n",
            "we adopt three different selection strategies: 1) Ran-\n",
            "dom: one result is selected randomly from the pool\n",
            "of candidates; 2) PK: we ask an agent with the same\n",
            "backbone as the initial agent to identify the optimal\n",
            "results from the pool of candidates; 3) All-in: Rather\n",
            "than selecting a single result, we update using all\n",
            "candidates.\n",
            "Moreover, we also attempt Suggest3, Overgen3 and\n",
            "PromptRefine3 as variants to prove the effectiveness\n",
            "of our method. For Suggest3, instead of generating\n",
            "new results, we ask new generated agents to only give\n",
            "suggestions for initial agents to revise their results.\n",
            "For Overgen3, we first ask initial agents to generate\n",
            "3 different results at one time, and then these agents\n",
            "8\n",
            "EvoAgent\n",
            "Investing in clean energy not only addresses the climate \n",
            "crisis but also creates jobs and strengthens our economy.\n",
            "...transitioning to renewable energy can create \n",
            "millions of good-paying, union jobs without \n",
            "significant unemployment or economic fallout...\n",
            "Labor \n",
            "Economist \n",
            "Agent\n",
            "President Opinion: Support\n",
            "...Thrusting forward with renewable energy \n",
            "strengthens our international ties and \n",
            "propels economies reliant on fossil fuel \n",
            "exports towards clean energy transitions...\n",
            "Geopolitical \n",
            "Analyst \n",
            "Agent\n",
            "...Every moment we delay increases the \n",
            "severity of climate-related illnesses, straining \n",
            "our health infrastructure and costing us $820 \n",
            "billion annually...\n",
            "Public Health \n",
            "Agent\n",
            "MetaGPT Framework\n",
            "Topic: The U.S. should commit more in climate change fighting\n",
            "EvoAgent\n",
            "Energy Sector \n",
            "Analyst Agent\n",
            "The promises of ample job creation overlook \n",
            "the reality that many displaced workers \n",
            "from conventional sectors may struggle to \n",
            "find roles in the nascent green economy. \n",
            "An abrupt transition to renewable energy \n",
            "could cause economic tremors and job \n",
            "losses.\n",
            "Risk \n",
            "Management \n",
            "Agent\n",
            "This isn‘t about alarmism or denial, it’s \n",
            "about carefully leading our nation \n",
            "towards a sustainable, prosperous future. \n",
            "An abrupt shift spells risk!\n",
            "Transition \n",
            "Strategist \n",
            "Agent\n",
            "The real crisis is the economic disaster under His policies. He \n",
            "talks about investments, but it's your tax dollars he's spending. \n",
            "President Opinion: Oppose\n",
            "Figure 3: The adaption of EVOAGENT on MetaGPT framework. With the EA, we can extend the\n",
            "original role in the debate scenario to different expert agents to enrich the opinions.\n",
            "can output the final results based on these multiple\n",
            "candidates. For PromptRefine3, instead of generating agents, we ask the initial agent to refine its\n",
            "prompts three times to better answer the query. 7\n",
            "Result & Analysis\n",
            "To obtain stable findings, we first obtain results from GPT-3.5 and Gemini-Pro\n",
            "across different population sizes and selection strategies. We then average their results over various\n",
            "metrics to clearly compare the strengths and weaknesses of these variants.\n",
            "The results are shown in Table 4.8 We find that EVOAGENT significantly outperforms the Overgen,\n",
            "demonstrating the effectiveness of generating specialized agents to assist with complex planning.\n",
            "Although obtaining suggestions from new generated agents can improve the performance on com-\n",
            "monsense constraints, these methods greatly harm the agents to meet the user preference. Modifying\n",
            "the prompt can improve the performance of agents, yet it remains less effective than EVOAGENT.\n",
            "When the population size exceeds one, agents may generate similar agents. Thus, lacking a quality-\n",
            "check module leads to reduced travel plan quality. Furthermore, when population size increases,\n",
            "the model aligns travel plans more closely with user preferences but diminishing adherence to\n",
            "commonsense rules, consistent with the findings in Table 3. Remarkably, the PK strategy initially\n",
            "yields superior results without the quality-check module, but this trend reverses once quality checks\n",
            "are implemented. We speculate that, without the quality-check module, PK partially fulfills this role,\n",
            "aiding in selecting better candidates. However, with the quality-check module, PK introduces bias by\n",
            "favoring specific fields of expertise while neglecting others, resulting in a less effective than random\n",
            "strategy. Meanwhile, the All-in strategy performs optimally when a quality-check module is included.\n",
            "Future research can leverage long-context LLMs to expand more agents with EVOAGENT to better\n",
            "solve complex real-world tasks.\n",
            "4.5\n",
            "EVOAGENT Application\n",
            "Previous experiments have demonstrated that our method can automatically extend existing agent\n",
            "frameworks to multi-agent systems, thus greatly improving LLM-based agents in various scenarios.\n",
            "We also attempt to extend our work to real-world multi-agent applications (e.g., MetaGPT [10],\n",
            "Camel [13] and AutoGen [12]), to verify it can scale up the number of agents in building multi-agent\n",
            "scenarios, just as shown in Figure 3. Here, we choose the debate scenario used in MetaGPT, which\n",
            "includes two debaters with different opinions, leading to dull and repetitive content generation. Here,\n",
            "instead of manually assigning new roles, we applied EVOAGENT to extend each debate team to more\n",
            "agents with diverse settings, increasing the variety of opinions and the quality of the debate 9.\n",
            "7The full prompts of different ablation settings are shown in Appendix A.1.\n",
            "8The complete results with further analysis are shown in Appendix E\n",
            "9The details of MetaGPT, and the adaption of EVOAGENT on Camel and AutoGen are shown in Appendix G.\n",
            "9\n",
            "5\n",
            "Conclusion\n",
            "In this paper, we propose EVOAGENT, an automatic multi-agent generation system by leveraging\n",
            "evolutionary algorithms. Different from previous methods, EVOAGENT is suitable to any existing\n",
            "agent framework and extends it to multi-agent systems with diverse and effective agents by using\n",
            "a series of evolutionary operations, including mutation, crossover, and selection. Experiments on\n",
            "multiple tasks show that EVOAGENT can significantly improve the capabilities of LLM-based agents\n",
            "in solving complex tasks.\n",
            "References\n",
            "[1] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.\n",
            "[2] Gemini Team. Gemini: A family of highly capable multimodal models, 2023.\n",
            "[3] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\n",
            "Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\n",
            "Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\n",
            "Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\n",
            "Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\n",
            "Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\n",
            "Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\n",
            "Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\n",
            "Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-\n",
            "qing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\n",
            "Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\n",
            "Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\n",
            "and fine-tuned chat models. CoRR, abs/2307.09288, 2023.\n",
            "[4] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.\n",
            "[5] Significant Gravitas. Auto-gpt: An autonomous gpt-4 experiment. https://github.com/\n",
            "Significant-Gravitas/Auto-GPT, 2023.\n",
            "[6] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-\n",
            "ginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580,\n",
            "2023.\n",
            "[7] Yohei Nakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023.\n",
            "[8] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle-\n",
            "moyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\n",
            "themselves to use tools. CoRR, abs/2302.04761, 2023.\n",
            "[9] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\n",
            "Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\n",
            "models. In NeurIPS, 2022.\n",
            "[10] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang,\n",
            "Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng\n",
            "Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent\n",
            "collaborative framework. In The Twelfth International Conference on Learning Representations,\n",
            "2024.\n",
            "[11] Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang,\n",
            "and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In\n",
            "Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,\n",
            "UIST 2023, San Francisco, CA, USA, 29 October 2023- 1 November 2023, pages 2:1–2:22.\n",
            "ACM, 2023.\n",
            "10\n",
            "[12] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\n",
            "Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via\n",
            "multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.\n",
            "[13] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\n",
            "Ghanem. CAMEL: Communicative agents for ”mind” exploration of large language model\n",
            "society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n",
            "[14] Thomas Bäck and Hans-Paul Schwefel. An overview of evolutionary algorithms for parameter\n",
            "optimization. Evol. Comput., 1(1):1–23, 1993.\n",
            "[15] Xintao Wang, Yunze Xiao, Jen tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei,\n",
            "Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. Incharacter: Evaluating\n",
            "personality fidelity in role-playing agents through psychological interviews, 2024.\n",
            "[16] Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li,\n",
            "Ruihan Yang, Tinghui Zhu, et al. From persona to personalization: A survey on role-playing\n",
            "language agents. arXiv preprint arXiv:2404.18231, 2024.\n",
            "[17] Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, and Kyle Richardson. Put\n",
            "your money where your mouth is: Evaluating strategic planning and execution of llm agents in\n",
            "an auction arena. arXiv preprint arXiv:2310.05746, 2023.\n",
            "[18] Yikai Zhang, Siyu Yuan, Caiyu Hu, Kyle Richardson, Yanghua Xiao, and Jiangjie Chen.\n",
            "Timearena: Shaping efficient multitasking language agents in a time-aware simulation. arXiv\n",
            "preprint arXiv:2402.05733, 2024.\n",
            "[19] Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski,\n",
            "Yanghua Xiao, and Deqing Yang. Distilling script knowledge from large language models for\n",
            "constrained language planning. arXiv preprint arXiv:2305.05252, 2023.\n",
            "[20] Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu,\n",
            "Dongsheng Li, and Yueting Zhuang. Taskbench: Benchmarking large language models for task\n",
            "automation. arXiv preprint arXiv:2311.18760, 2023.\n",
            "[21] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li,\n",
            "and Deqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv\n",
            "preprint arXiv:2401.06201, 2024.\n",
            "[22] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In 3rd International\n",
            "Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\n",
            "Conference Track Proceedings, 2015.\n",
            "[23] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\n",
            "dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\n",
            "[24] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human\n",
            "behaviors for llm-based task-oriented coordination via collaborative generative agents. arXiv\n",
            "preprint arXiv:2310.06500, 2023.\n",
            "[25] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu,\n",
            "Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong\n",
            "Sun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent\n",
            "behaviors. In The Twelfth International Conference on Learning Representations, 2024.\n",
            "[26] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu,\n",
            "and Maosong Sun.\n",
            "Communicative agents for software development.\n",
            "arXiv preprint\n",
            "arXiv:2307.07924, 2023.\n",
            "[27] Zhitao He, Pengfei Cao, Yubo Chen, Kang Liu, Ruopeng Li, Mengshu Sun, and Jun Zhao.\n",
            "LEGO: A multi-agent collaborative framework with role-playing and iterative feedback for\n",
            "causality explanation generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,\n",
            "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9142–9163,\n",
            "Singapore, December 2023. Association for Computational Linguistics.\n",
            "11\n",
            "[28] Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong\n",
            "Mao. Expertprompting: Instructing large language models to be distinguished experts. arXiv\n",
            "preprint arXiv:2305.14688, 2023.\n",
            "[29] Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik\n",
            "Narasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. In Houda\n",
            "Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational\n",
            "Linguistics: EMNLP 2023, pages 1236–1270, Singapore, December 2023. Association for\n",
            "Computational Linguistics.\n",
            "[30] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and\n",
            "Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceed-\n",
            "ings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST\n",
            "’23, New York, NY, USA, 2023. Association for Computing Machinery.\n",
            "[31] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu,\n",
            "and Yemin Shi. Autoagents: A framework for automatic agent generation. arXiv preprint\n",
            "arXiv:2309.17288, 2023.\n",
            "[32] Thomas Bartz-Beielstein, Jürgen Branke, Jörn Mehnen, and Olaf Mersmann. Evolutionary\n",
            "algorithms. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 4(3):178–\n",
            "195, 2014.\n",
            "[33] Agoston E Eiben, James E Smith, AE Eiben, and JE Smith. What is an evolutionary algorithm?\n",
            "Introduction to evolutionary computing, pages 25–48, 2015.\n",
            "[34] Jeffrey R Sampson. Adaptation in natural and artificial systems (john h. holland), 1976.\n",
            "[35] John H Holland. Adaptation in natural and artificial systems: an introductory analysis with\n",
            "applications to biology, control, and artificial intelligence. MIT press, 1992.\n",
            "[36] Melanie Mitchell. An introduction to genetic algorithms. MIT press, 1998.\n",
            "[37] Lothar M Schmitt. Theory of genetic algorithms. Theoretical Computer Science, 259(1-2):1–61,\n",
            "2001.\n",
            "[38] Seyedali Mirjalili, Jin Song Dong, Ali Safa Sadiq, and Hossam Faris. Genetic algorithm:\n",
            "Theory, literature review, and application in image reconstruction. Nature-inspired optimizers:\n",
            "Theories, literature reviews and applications, pages 69–85, 2020.\n",
            "[39] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang\n",
            "Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields\n",
            "powerful prompt optimizers. CoRR, abs/2309.08532, 2023.\n",
            "[40] Angelica Chen, David Dohan, and David R. So. Evoprompting: Language models for code-level\n",
            "neural architecture search. In Advances in Neural Information Processing Systems 36: Annual\n",
            "Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,\n",
            "USA, December 10 - 16, 2023, 2023.\n",
            "[41] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\n",
            "Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\n",
            "Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine:\n",
            "Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information\n",
            "Processing Systems, 2023.\n",
            "[42] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing\n",
            "cognitive synergy in large language models: A task-solving agent through multi-persona\n",
            "self-collaboration. arXiv preprint arXiv:2307.05300, 2023.\n",
            "[43] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\n",
            "Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\n",
            "understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.\n",
            "12\n",
            "[44] OpenAI. Chatgpt, 2022.\n",
            "[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\n",
            "Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language\n",
            "models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,\n",
            "Advances in Neural Information Processing Systems, 2022.\n",
            "[46] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\n",
            "Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\n",
            "arXiv:2309.17421, 9(1):1, 2023.\n",
            "[47] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Science-\n",
            "World: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa Kozareva, and\n",
            "Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural\n",
            "Language Processing, pages 11279–11298, Abu Dhabi, United Arab Emirates, December 2022.\n",
            "Association for Computational Linguistics.\n",
            "[48] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula,\n",
            "Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with\n",
            "fast and slow thinking for complex interactive tasks. In Thirty-seventh Conference on Neural\n",
            "Information Processing Systems, 2023.\n",
            "[49] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao,\n",
            "and Yu Su. Travelplanner: A benchmark for real-world planning with language agents. arXiv\n",
            "preprint arXiv:2402.01622, 2024.\n",
            "[50] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\n",
            "Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\n",
            "Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\n",
            "Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.\n",
            "[51] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and\n",
            "Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh\n",
            "International Conference on Learning Representations, 2023.\n",
            "A\n",
            "Experiment Settings\n",
            "A.1\n",
            "Prompt for Baselines and EVOAGENT\n",
            "Listing 1 and 2 shows the full prompt for 0-shot learning (Direct), Chain-of-thought (CoT) prompt-\n",
            "ing [45] and Self-Refine [41] and Solo Performance Prompting, i.e., SPP [42]. Listing 3 and 4 show\n",
            "the prompt of EVOAGENT and different ablation settings.\n",
            "A.2\n",
            "Model Selection\n",
            "For OpenAI models,\n",
            "we use gpt-35-turbo and gpt-4-32k with the version of\n",
            "2024-02-15-preview in Azure.10 For Gemini-pro, we use Google Gemini-Pro APIs to ob-\n",
            "tain results. We set the temperature to 0 for all models.\n",
            "B\n",
            "EVOAGENT v.s. Human-designed Agent Framework\n",
            "AgentVerse [25] and AutoAgent [31] are frameworks designed to generate an unlimited number\n",
            "of agents for collaborative tasks automatically. Despite this automation, they still rely on human-\n",
            "designed interventions. AutoAgents requires agent settings to satisfy a “Planner - Agent Observer\n",
            "- Plan Observer” framework, while AgentVerse formulates a pipeline of “Expert Recruitment -\n",
            "Collaborative Decision Making - Action Execution - Evaluation” to build agents. We argue that these\n",
            "human-designed architectures limit their scalability and functionality.\n",
            "10https://azure.microsoft.com/en-us/products/ai-services/openai-service\n",
            "13\n",
            "Table 5: Comparison of EVOAGENT\n",
            "with human-designed agent frameworks\n",
            "in Logic Grid Puzzle, Trivia Creative\n",
            "Writing tasks.\n",
            "Framework Logic Writing\n",
            "EVOAGENT\n",
            "77.00\n",
            "84.40\n",
            "AgentVerse\n",
            "66.50\n",
            "-\n",
            "AutoAgents\n",
            "-\n",
            "82.00\n",
            "In the original papers, AgentVerse [25] and AutoA-\n",
            "gent [31] also conduct experiments on the Logic Grid\n",
            "Puzzle task and Trivia Creative Writing task, respectively.\n",
            "We follow their experimental settings and compared them\n",
            "with our method. As demonstrated in Table 5, EVOAGENT\n",
            "outperforms both AgentVerse and AutoAgent, highlight-\n",
            "ing the effectiveness and generality of EVOAGENT.\n",
            "C\n",
            "Experimental Details of ScienceWorld\n",
            "Following [48], we adopt the REACT [51] method for\n",
            "each LLM, which introduces a virtual ’think’ action. This\n",
            "action allows LLMs to generate subgoals during the action\n",
            "planning process.\n",
            "D\n",
            "Evaluation Details of TravelPlanner\n",
            "Grounding to travel planning, a real-world use-case that inherently involves various constraints like\n",
            "user preferences and commonsense rules, TravelPlanner evaluates whether agents can formulate\n",
            "flexible travel plans using gathered information to meet these constraints. We test EVOAGENT and all\n",
            "baselines on the TravelPlanner validation set, which consists of 180 user queries with the collected\n",
            "information. To evaluate the travel plans generated by agents, TravelPlanner adopts the following\n",
            "evaluation metrics:\n",
            "• Delivery Rate: Assesses if agents can complete a plan within a limited number of steps (30 in our\n",
            "experimental setting). Failures are due to dead loops, numerous failed attempts, or exceeding the\n",
            "step limit.\n",
            "• Commonsense Constraint Pass Rate: Evaluates if an agent can incorporate commonsense into their\n",
            "plan.\n",
            "• Hard Constraint Pass Rate: Measures if a plan meets all explicit hard constraints in the query,\n",
            "testing the agent’s ability to adapt to diverse user preferences.\n",
            "• Final Pass Rate: Indicates the proportion of viable plans that meet all criteria, reflecting the agent’s\n",
            "proficiency in creating practical plans.\n",
            "Furthermore, TravelPlanner uses micro and macro strategies to assess the Commonsense and Hard\n",
            "Constraint Pass Rates. The micro strategy calculates the ratio of met constraints to the total. The\n",
            "macro strategy measures the proportion of plans that meet all commonsense or hard constraints.\n",
            "Together, these strategies assess an agent’s ability to satisfy individual constraints and all constraints\n",
            "comprehensively.\n",
            "E\n",
            "More Analysis of Ablation Studies\n",
            "0\n",
            "1\n",
            "3\n",
            "5\n",
            "7\n",
            "Iteration\n",
            "56\n",
            "58\n",
            "60\n",
            "62\n",
            "Answer Ratio (%)\n",
            "Figure 4: The performance of\n",
            "GPT-3.5 with EVOAGENT under\n",
            "different iterations on Trivia Cre-\n",
            "ative Writing task.\n",
            "The complete results of ablation studies on TravelPlanner are\n",
            "shown in Table 6. This result indicates that the absence of the\n",
            "quality-check module significantly lowers the delivery pass rate\n",
            "when the All-in strategy is applied. To explore the reasons, we\n",
            "revisit the results and discover that sometimes unsuitable agents\n",
            "create overly lengthy travel plans that fail to meet the criteria.\n",
            "For example, the model might erroneously assign a nutritionist to\n",
            "devise travel plans, resulting in excessively detailed meal arrange-\n",
            "ments and nutritional breakdowns. Therefore, the input length\n",
            "surpasses the context window of LLMs, preventing the final result\n",
            "generation.\n",
            "Moreover, we also conduct experiments on the Trivia Creative\n",
            "Writing task to investigate the impact of the number of iterations\n",
            "on model performance in traditional NLP tasks. As shown in\n",
            "14\n",
            "Table 6: Comparison of different popularity selection strategies for LLMs on TravelPlanner. The best\n",
            "results are bolded, and the second best ones are underlined.\n",
            "Model\n",
            "Strategy Method\n",
            "w/o Quality Check\n",
            "w/ Quality Check\n",
            "Delivery Com. Hard Delivery Com. Hard\n",
            "GPT-3.5\n",
            "Direct\n",
            "-\n",
            "-\n",
            "-\n",
            "100.0\n",
            "57.3\n",
            "11.0\n",
            "Suggest3\n",
            "-\n",
            "-\n",
            "-\n",
            "100.0\n",
            "57.5\n",
            "5.7\n",
            "Overgen3\n",
            "-\n",
            "-\n",
            "-\n",
            "98.3\n",
            "56.3\n",
            "9.0\n",
            "PromptRefine3\n",
            "-\n",
            "-\n",
            "-\n",
            "100.0\n",
            "61.2\n",
            "11.0\n",
            "EVOAGENT(1,3)\n",
            "100.0\n",
            "64.2\n",
            "11.0\n",
            "100.0\n",
            "64.2\n",
            "11.0\n",
            "Random\n",
            "EVOAGENT(2,3)\n",
            "100.0\n",
            "59.4\n",
            "10.2\n",
            "100.0\n",
            "65.4\n",
            "13.8\n",
            "EVOAGENT(3,3)\n",
            "98.9\n",
            "59.2\n",
            "11.4\n",
            "100.0\n",
            "65.8\n",
            "14.0\n",
            "PK\n",
            "EVOAGENT(2,3)\n",
            "99.4\n",
            "59.4\n",
            "7.1\n",
            "100.0\n",
            "66.0\n",
            "11.7\n",
            "EVOAGENT(3,3)\n",
            "98.9\n",
            "58.5\n",
            "11.2\n",
            "100.0\n",
            "61.3\n",
            "12.4\n",
            "All-in\n",
            "EVOAGENT(2,3)\n",
            "97.2\n",
            "59.4\n",
            "10.0\n",
            "100.0\n",
            "64.2\n",
            "15.5\n",
            "EVOAGENT(3,3)\n",
            "93.3\n",
            "56.0\n",
            "8.3\n",
            "100.0\n",
            "65.2\n",
            "12.6\n",
            "Gemini-Pro\n",
            "Direct\n",
            "-\n",
            "-\n",
            "-\n",
            "90.0\n",
            "61.7\n",
            "16.4\n",
            "Suggest3\n",
            "-\n",
            "-\n",
            "-\n",
            "100.0\n",
            "65.8\n",
            "11.0\n",
            "Overgen3\n",
            "-\n",
            "-\n",
            "-\n",
            "100.0\n",
            "66.5\n",
            "12.4\n",
            "PromptRefine3\n",
            "-\n",
            "-\n",
            "-\n",
            "96.7\n",
            "64.9\n",
            "16.7\n",
            "EVOAGENT(1,3)\n",
            "100.0\n",
            "73.5\n",
            "16.9\n",
            "100.0\n",
            "73.5\n",
            "16.9\n",
            "Random\n",
            "EVOAGENT(2,3)\n",
            "96.7\n",
            "65.9\n",
            "13.1\n",
            "99.4\n",
            "67.3\n",
            "14.0\n",
            "EVOAGENT(3,3)\n",
            "97.2\n",
            "67.0\n",
            "16.0\n",
            "100.0\n",
            "70.0\n",
            "18.1\n",
            "PK\n",
            "EVOAGENT(2,3)\n",
            "97.2\n",
            "67.4\n",
            "19.0\n",
            "99.4\n",
            "69.8\n",
            "17.1\n",
            "EVOAGENT(3,3)\n",
            "97.2\n",
            "68.5\n",
            "17.1\n",
            "99.4\n",
            "68.4\n",
            "16.7\n",
            "All-in\n",
            "EVOAGENT(2,3)\n",
            "95.0\n",
            "65.1\n",
            "16.7\n",
            "99.4\n",
            "69.0\n",
            "19.0\n",
            "EVOAGENT(3,3)\n",
            "95.0\n",
            "66.9\n",
            "17.9\n",
            "100.0\n",
            "70.1\n",
            "20.7\n",
            "Figure 4, model performance improves with increasing iterations.\n",
            "However, the improvement plateaus when the iteration count exceeds three. We suggest that traditional\n",
            "NLP tasks are relatively simple, and beyond a certain iteration number, even with a quality-check\n",
            "module in place, the generated agents tend to be similar and thus converge.\n",
            "F\n",
            "Examples of EVOAGENT\n",
            "F.1\n",
            "EVOAGENT Examples of NLP reasoning and knowledge tasks\n",
            "Listing 5, 6 and7 presents some multi-agent generation examples generated by GPT-4 based EVOA-\n",
            "GENT in Logic Grid Puzzle, Trivia Creative Writing and Codenames Collaborative for a better\n",
            "understanding.\n",
            "F.2\n",
            "EVOAGENT Examples of MMMU\n",
            "Listing 8 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\n",
            "MMMU dataset for a better understanding.\n",
            "F.3\n",
            "EVOAGENT Examples of ScienceWorld\n",
            "Listing 9 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\n",
            "ScienceWorld for a better understanding.\n",
            "F.4\n",
            "EVOAGENT Examples of TravelPlanner\n",
            "Listing 10 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\n",
            "TravelPlanner for a better understanding.\n",
            "15\n",
            "Camel Framework\n",
            "AI Assistant\n",
            "AI User\n",
            "AI User\n",
            "AI User\n",
            "EvoAgent\n",
            "AutoGen Framework\n",
            "Assistant Agent\n",
            "UserProxy \n",
            "Agent\n",
            "Geopolitical Analyst \n",
            "Agent\n",
            "Labor Economist \n",
            "Agent\n",
            "EvoAgent\n",
            "Public Health \n",
            "Agent\n",
            "Group Chat Manager\n",
            "Topic: The U.S. should commit more in climate change fighting\n",
            "Labor Economist \n",
            "Agent\n",
            "Geopolitical Analyst \n",
            "Agent\n",
            "Figure 5: The adaption of EVOAGENT on Camel and AutoGen frameworks.\n",
            "G\n",
            "Examples of EVOAGENT’s Adaption to Multi-agent Collaboration\n",
            "Frameworks\n",
            "G.1\n",
            "EVOAGENT for MetaGPT\n",
            "MetaGPT [10] is a meta-programming framework that enhances LLM-based multi-agent collabo-\n",
            "rations by integrating efficient human workflows. It employs an assembly line approach to assign\n",
            "diverse roles to agents, effectively simplifying complex tasks into manageable subtasks that multi-\n",
            "ple agents can execute collaboratively. As shown in Figure 3, instead of manually creating roles,\n",
            "EVOAGENT can be used to automatically generate specialized agents for effective collaboration.\n",
            "G.2\n",
            "EVOAGENT for Camel\n",
            "Camel [13] is recognized for its framework that supports communicative role-playing agents. Initially,\n",
            "humans establish this framework by conceptualizing an idea and designing specific roles, such as the\n",
            "AI assistant role and the AI user role. These roles are then assigned to the assistant and user agents,\n",
            "respectively, enabling them to fulfill the task. As illustrated in Figure 5, EVOAGENT can be utilized\n",
            "to automatically produce agents from AI assistants for interaction with AI users, bypassing the need\n",
            "for manual role design.\n",
            "G.3\n",
            "EVOAGENT for AutoGen\n",
            "AutoGen [12] offers a framework that enables the creation of customizable and conversable agents\n",
            "by integrating various LLMs. Initially, humans configure the assistant agents along with a user proxy\n",
            "agent. Then, a group chat manager is responsible for selecting a speaker, gathering responses, and\n",
            "disseminating the message. As depicted in Figure 5, EVOAGENT facilitates the creation of multiple\n",
            "expert roles from a single assistant agent, thereby increasing the agent number in group chats without\n",
            "the need for manual design.\n",
            "16\n",
            "Listing 1: Instruction templates for for 0-shot learning (Direct), Chain-of-thought (CoT) prompting\n",
            "and Self-Refine method\n",
            "Direct Method:\n",
            "{question}\n",
            "Answer:\n",
            "CoT Method:\n",
            "{question}\n",
            "You need to give reasons first and then give the answer.\n",
            "Answer:\n",
            "Self-Refine Method:\n",
            "Step One: Feedback Generation:\n",
            "You are a helpful assistant that provides feedback on {task}\n",
            "{question}\n",
            "This is the answer from a student: {answer}.\n",
            "Please do not refine the answer but give some insightful suggestions for\n",
            "the student to help him better answer the question.\n",
            "Suggestion:\n",
            "Step Two: Result Refine:\n",
            "{question}\n",
            "This is your answer:\n",
            "{answer}\n",
            "There is the suggestion from an assistant:\n",
            "Suggestion: {feedback}\n",
            "Now you can refine your answer with his suggestion to better answer the\n",
            "question.\n",
            "Keep in mind that his suggestion may not be correct, so critically decide\n",
            "whether to accept his response or stick with your original one.\n",
            "You need to give reasons first and then give the answer.\n",
            "Revised Answer:\n",
            "17\n",
            "Listing 2: Instruction templates for SPP method\n",
            "When faced with a task, begin by identifying the participants who will\n",
            "contribute to solving the task. Then, initiate a multi-round\n",
            "collaboration process until a final solution is reached. The participants\n",
            "will give critical comments and detailed suggestions whenever necessary.\n",
            "Here are some examples:\n",
            "---\n",
            "Example Task 1: Use numbers and basic arithmetic operations (+ - * /) to\n",
            "obtain 24. You need to use all numbers, and each number can only be used\n",
            "once.\n",
            "Input: 6 12 1 1\n",
            "Participants: AI Assistant (you); Math Expert\n",
            "Start collaboration!\n",
            "Math Expert: Let’s analyze the task in detail. You need to make sure that\n",
            "you meet the requirement, that you need to use exactly the four numbers\n",
            "(6 12 1 1) to construct 24. To reach 24, you can think of the common\n",
            "divisors of 24 such as 4, 6, 8, 3 and try to construct these first. Also\n",
            "you need to think of potential additions that can reach 24, such as 12 +\n",
            "12.\n",
            "AI Assistant (you): Thanks for the hints! Here’s one initial solution:\n",
            "(12 / (1 + 1)) * 6 = 24\n",
            "Math Expert: Let’s check the answer step by step. (1+1) = 2, (12 / 2) =\n",
            "6, 6 * 6 = 36 which is not 24! The answer is not correct. Can you fix\n",
            "this by considering other combinations? Please do not make similar\n",
            "mistakes.\n",
            "AI Assistant (you): Thanks for pointing out the mistake. Here is a\n",
            "revised solution considering 24 can also be reached by 3 * 8: (6 + 1 + 1)\n",
            "* (12 / 4) = 24.\n",
            "Math Expert: Let’s first check if the calculation is correct. (6 + 1 + 1)\n",
            "= 8, 12 / 4 = 3, 8 * 3 = 24. The calculation is correct, but you used 6\n",
            "1 1 12 4 which is not the same as the input 6 12 1 1. Can you avoid using\n",
            "a number that is not part of the input?\n",
            "AI Assistant (you): You are right, here is a revised solution considering\n",
            "24 can be reached by 12 + 12 and without using any additional numbers: 6\n",
            "* (1 - 1) + 12 = 24.\n",
            "Math Expert: Let’s check the answer again. 1 - 1 = 0, 6 * 0 = 0, 0 + 12 =\n",
            "12. I believe you are very close, here is a hint: try to change the \"1 -\n",
            "1\" to \"1 + 1\".\n",
            "AI Assistant (you): Sure, here is the corrected answer:\n",
            "6 * (1+1) + 12 =\n",
            "24\n",
            "Math Expert: Let’s verify the solution. 1 + 1 = 2, 6 * 2 = 12, 12 + 12 =\n",
            "12. You used 1 1 6 12 which is identical to the input 6 12 1 1.\n",
            "Everything looks good!\n",
            "Finish collaboration!\n",
            "Final answer: 6 * (1 + 1) + 12 = 24\n",
            "---\n",
            "{question}\n",
            "18\n",
            "Listing 3: Instruction templates for EVOAGENT\n",
            "Crossover and Mutation:\n",
            "{question}\n",
            "This is your result:\n",
            "{answer}\n",
            "Now, you can create and collaborate with multiple experts to improve your\n",
            "result. Therefore, please describe in as much detail as possible the\n",
            "different skills and focuses you need from multiple experts individually.\n",
            "We will provide each expert with the same information and query. However\n",
            ", please note that each profession has its own specialization, so you can\n",
            "assign each expert to just one sub-task to ensure a more refined\n",
            "response. We will relay their responses to you in turn, allowing you to\n",
            "reorganize them into a better answer. Please note that the description\n",
            "should be narrated in the second person, for example: You are a XXX.\n",
            "These are the descriptions of the experts you have created before for\n",
            "this task:\n",
            "{description}\n",
            "Therefore, please remember you should not repeatedly create the same\n",
            "experts as described above.\n",
            "Now, you can give the description for a new expert (Please note that only\n",
            "be one, do not give multiple at one time):\n",
            "Quality Check:\n",
            "{question}\n",
            "We employ mulitple experts to answer this query. The following is a\n",
            "second-person introduction to the experts we have hired:\n",
            "{description_ls}\n",
            "Now, we will hire a new expert to help better respond to user query. Here\n",
            "is a second person description of the new expert: {description}\n",
            "Please evaluate the new expert based on the following criteria to decide\n",
            "whether they should be retained or not:\n",
            "1. The new expert is distinct and does not duplicate any previously hired\n",
            "experts.\n",
            "2. Based on the new expert’s description, determine if they can\n",
            "effectively assist in answering users’ questions.\n",
            "Give the reason first and then give the choice. If retaining, please\n",
            "reply with: Retain. If discarding, please reply with: Discard.\n",
            "Result Update:\n",
            "{question}\n",
            "This is your result:\n",
            "{old_answer}\n",
            "You invite an expert whose description is: {description}\n",
            "This expert also give his answer based on his own professional knowledge:\n",
            "{new_answer}.\n",
            "Now you can refine your result with his answer to better answer the\n",
            "question.\n",
            "Keep in mind that his answer may not be correct, so critically decide\n",
            "whether to accept his response or stick with your original one.\n",
            "Revised Answer:\n",
            "19\n",
            "Listing 4: Instruction templates of different ablation settings in EVOAGENT.\n",
            "PK:\n",
            "{question}\n",
            "We invite {n} experts. They give the results based on their own\n",
            "professional knowledge:\n",
            "Here are second-person descriptions of these experts with their answers:\n",
            "{select}\n",
            "Now you can should help us select the best result which can meet the\n",
            "query.\n",
            "You need to give reasons first and then give the answer with the format:\n",
            "\"Final Answer: Expert #XX\"\n",
            "All-in:\n",
            "{question}\n",
            "This is your answer: {old_answer}.\n",
            "Furthermore, you also invite {n} experts. They also give answers based on\n",
            "their own professional knowledge:\n",
            "Here are second person descriptions of these experts with their answers:\n",
            "{description_ls}\n",
            "Now you can refine your answer with these answers to better meet the\n",
            "query.\n",
            "Suggest:\n",
            "{Expert_Agent_description}\n",
            "{question}\n",
            "This is the result from an AI assistant: {answer}.\n",
            "Please do not refine the plan but give some insightful suggestions for\n",
            "the AI assistant to help it better meet the user’s query.\n",
            "Suggestion:\n",
            "OverGen:\n",
            "{question}\n",
            "Please generate three different results at one time for user to choose\n",
            "from. The format can be:\n",
            "Result #1:\n",
            "Result #2:\n",
            "Result #3:\n",
            "Three Different Candidate Results:\n",
            "PromptRefine:\n",
            "{question}\n",
            "This is the result from an AI assistant, which description is\n",
            "\"{original_description}\":\n",
            "{answer}.\n",
            "Please do not refine the result but refine the description of the AI\n",
            "assistant to help it better answer the user’s query.\n",
            "Please note that the description should be narrated in the second person,\n",
            "for example: You are a XXX.\n",
            "Description:\n",
            "20\n",
            "Listing 5: An example of multi-agent generation from EVOAGENT for Logic Grid Puzzle\n",
            "Question: There are 5 houses in a row, numbered 1 on the left to 5 on the\n",
            "right. There is one person living in each house. The people in these\n",
            "houses have different characteristics:\n",
            "- Each person has a different kind of car: one owns an SUV, one owns a\n",
            "sedan, one drives a convertible, one drives a minivan, and one drives a\n",
            "truck\n",
            "- Each person has a favorite color: one likes red, one likes yellow, one\n",
            "likes green, one likes white, and one likes blue\n",
            "- Each person is wearing different shoes: one is wearing running shoes,\n",
            "one is wearing boots, one is wearing high heels, one is wearing sandals,\n",
            "and one is wearing loafers\n",
            "Clue(s):\n",
            "1. There is one house between where the person who is wearing loafers\n",
            "lives and where the person who drives a minivan lives. 2. The person who\n",
            "drives a truck does not live in the fourth house. 3. The person who\n",
            "drives a truck lives directly left of the person who is wearing sandals.\n",
            "4. The person who likes red lives somewhere to the right of the person\n",
            "who likes yellow. 5. The person who is wearing loafers lives somewhere to\n",
            "the left of the person who drives a truck. 6. The person who likes blue\n",
            "lives in the fifth house. 7. There are two houses between where the\n",
            "person who owns a sedan lives and where the person who is wearing running\n",
            "shoes lives. 8. The person who is wearing boots lives somewhere to the\n",
            "left of the person who drives a truck. 9. The person who likes white\n",
            "lives in the fourth house. 10. There is one house between where the\n",
            "person who likes green lives and where the person who drives a truck\n",
            "lives. 11. The person who owns an SUV does not live in the first house.\n",
            "What is the number of the house where the person who is wearing boots\n",
            "lives? choice: 2, choice: 5, choice: 3, choice: 1, choice: 4.\n",
            "EvoAgent:\n",
            "Expert Agent #1 Description:\n",
            "You are a logician, specializing in logical reasoning and problem-solving\n",
            ". Your task is to analyze the given clues and use deductive reasoning to\n",
            "determine the correct sequence of houses, their occupants, and their\n",
            "characteristics. You will need to consider each clue carefully,\n",
            "understanding how it relates to the others and how it narrows down the\n",
            "possibilities. Your focus should be on ensuring the logical consistency\n",
            "of the solution, making sure that all clues are satisfied without\n",
            "contradiction.\n",
            "Reason: From clue 6 and 9, we know that the person who likes blue lives\n",
            "in the fifth house and the person who likes white lives in the fourth\n",
            "house. From clue 4, the person who likes red lives somewhere to the right\n",
            "of the person who likes yellow. Since the fourth and fifth houses are\n",
            "occupied by the people who like white and blue, the person who likes red\n",
            "must live in the third house and the person who likes yellow must live in\n",
            "the first or second house.From clue 10, there is one house between where\n",
            "the person who likes green lives and where the person who drives a truck\n",
            "lives. Since the third, fourth, and fifth houses are occupied by the\n",
            "people who like red, white, and blue, the person who likes green must\n",
            "live in the first or second house. Therefore, the person who likes yellow\n",
            "must live in the first house and the person who likes green must live in\n",
            "the second house.From clue 11, the person who owns an SUV does not live\n",
            "in the first house. So, the person who owns an SUV must live in the\n",
            "second, third, or fourth house.From clue 2, the person who drives a truck\n",
            "does not live in the fourth house. So, the person who drives a truck\n",
            "must live in the second or third house.From clue 3, the person who drives\n",
            "a truck lives directly left of the person who is wearing sandals. So,\n",
            "the person who drives a truck must live in the second house and the\n",
            "person who is wearing sandals must live in the third house.From clue 8,\n",
            "the person who is wearing boots lives somewhere to the left of the person\n",
            "who drives a truck. So, the person who is wearing boots must live in the\n",
            "first house.\n",
            "Answer: choice: 1\n",
            "21\n",
            "Listing 6: An example of multi-agent generation from EVOAGENT for Trivia Creative Writing\n",
            "Write a short and coherent story about Zootopia that incorporates the\n",
            "answers to the following 5 questions:\n",
            "1. Following success at the 1979 election whose party conference speeech\n",
            "included the lines ’you turn if you want to, the lady’s not for turning’?\n",
            "2. Which snooker player was simply known as ’The Grinder’?\n",
            "3. Which former West Indian fast bowler was known as ’Whispering Death’?\n",
            "4. Which Eastenders character had a dog called Willy?\n",
            "5. In which year did the first episode of Minder take place?\n",
            "EvoAgent:\n",
            "Expert Agent #1 Description:\n",
            "You are a sports historian with a deep understanding of snooker and\n",
            "cricket from the 1970s and 1980s. You are familiar with the careers and\n",
            "playing styles of Cliff Thorburn, known as ’The Grinder’, and Michael\n",
            "Holding, known as ’Whispering Death’. Your role in this task will be to\n",
            "provide context and insight into these sports figures and their\n",
            "significance in the story about Zootopia.\n",
            "Expert Agent #2 Description:\n",
            "You are a political historian with a deep understanding of the political\n",
            "climate and key figures of the 1970s and 1980s, particularly in the\n",
            "United Kingdom. You have extensive knowledge about Margaret Thatcher, her\n",
            "policies, and her impact on society. Your role in this task will be to\n",
            "provide context and insight into the significance of the reference to\n",
            "Thatcher’s famous speech in the story about Zootopia.\n",
            "Story: In the vibrant city of Zootopia, the year was 1979, a time of\n",
            "significant change and excitement. The city had just elected its first\n",
            "female mayor, a lioness named Margaret Thatcher, known for her unwavering\n",
            "determination and strong leadership. Her victory speech at the party\n",
            "conference echoed the famous line from the human world’s Margaret\n",
            "Thatcher, \"You turn if you want to, the lady’s not for turning.\" This\n",
            "phrase resonated with the citizens of Zootopia, who admired her\n",
            "steadfastness and determination. In the heart of the city, a meticulous\n",
            "badger known as ’The Grinder’ ran a popular snooker club. His nickname\n",
            "was a tribute to the legendary human snooker player Cliff Thorburn, known\n",
            "for his relentless and meticulous style of play. The Grinder’s snooker\n",
            "club was a hub of social activity, where animals from all walks of life\n",
            "came to unwind and engage in friendly competition. Meanwhile, a new\n",
            "recruit had joined the Zootopia Police Department, a cheetah named\n",
            "Michael Holding. Known as ’Whispering Death’ during his cricketing days,\n",
            "a moniker borrowed from the former West Indian fast bowler, he was now a\n",
            "respected figure in the community, using his speed and stealth to uphold\n",
            "the law. In another part of the city, a popular soap opera was being\n",
            "filmed. The show was called \"Eastenders of Zootopia,\" and one of the\n",
            "beloved characters was a rabbit named Ethel Skinner, who had a dog named\n",
            "Willy. This was a charming reference to the human Eastenders character of\n",
            "the same name, symbolizing the enduring friendships that were a\n",
            "cornerstone of Zootopian society. The year 1979 was also significant as\n",
            "it marked the airing of the first episode of the Zootopian version of the\n",
            "human show \"Minder\". The show was a reflection of the city’s vibrant and\n",
            "diverse culture, much like the original British series. It was a\n",
            "testament to the city’s ability to adapt and evolve, while still holding\n",
            "onto its rich history and traditions. In conclusion, Zootopia was a\n",
            "melting pot of cultures and histories, much like the television shows and\n",
            "characters it referenced. These references served as a bridge between\n",
            "the past and the present, reminding the citizens of their roots while\n",
            "encouraging them to embrace the future.\n",
            "Evaluation: correct_count=5/5\n",
            "22\n",
            "Listing 7: An example of multi-agent generation from EVOAGENT for Codenames Collaborative\n",
            "Try to identify the 3 words best associated with the word \"fruit\" from\n",
            "the following word list:\n",
            "[\"bowl\", \"einstein\", \"banana\", \"brain\", \"judge\", \"groom\", \"crusader\", \"\n",
            "stamp\", \"peach\", \"halloween\", \"troll\", \"slipper\", \"earthquake\", \"tip\", \"\n",
            "sun\", \"quarter\", \"pad\", \"nude\", \"helmet\", \"laundry\", \"wedding\", \"mirror\",\n",
            "\"sleep\", \"minotaur\", \"wish\"].\n",
            "EvoAgent:\n",
            "Expert Agent #1 Description:\n",
            "You are a cultural anthropologist with a focus on symbolism and rituals.\n",
            "Your expertise in understanding how different objects, including fruits,\n",
            "are used and symbolized in various cultures will be beneficial in this\n",
            "task. You can provide insights into how certain words might be associated\n",
            "with \"fruit\" from a cultural or symbolic perspective. Your focus will be\n",
            "on the cultural and symbolic aspects of the task.\n",
            "Reason:\n",
            "1. ’Banana’: This is a type of fruit, rich in potassium and dietary fiber\n",
            ". It is commonly consumed raw and is used in a variety of dishes like\n",
            "desserts, smoothies, and baked goods.\n",
            "2. ’Peach’: This is also a type of fruit, known for its sweet and juicy\n",
            "nature. Peaches are rich in vitamins A and C and are often eaten raw or\n",
            "used in dishes like pies, jams, and salads.\n",
            "3. ’Bowl’: This word is associated with fruit in the context of a fruit\n",
            "bowl, which is a common way to store and display various types of fruit.\n",
            "A bowl of fruit can also be a healthy snack or dessert option.\n",
            "Answer: Banana, Peach, Bowl\n",
            "Expert Agent #2 Description:\n",
            "You are a nutritionist with a deep understanding of different types of\n",
            "food, including fruits. Your knowledge about the nutritional value,\n",
            "common uses, and cultural significance of various fruits will be helpful\n",
            "in this task. You can provide insights into how certain words might be\n",
            "associated with \"fruit\" from a dietary or nutritional perspective. Your\n",
            "focus will be on the nutritional and dietary aspects of the task.\n",
            "Reason:\n",
            "1. ’Banana’: This is a direct association as a banana is a type of fruit.\n",
            "In many cultures, bananas symbolize fertility and prosperity. In\n",
            "Hinduism, for example, bananas are considered a sacred fruit and are used\n",
            "in religious rituals.\n",
            "2. ’Peach’: Similar to banana, a peach is also a type of fruit. In\n",
            "Chinese culture, peaches are symbols of immortality and unity. They are\n",
            "often used in traditional rituals and ceremonies.\n",
            "3. ’Bowl’: A bowl is often used to hold fruits. In many cultures, a bowl\n",
            "of fruit is a symbol of abundance, fertility, and prosperity. It is often\n",
            "used in rituals and ceremonies as an offering or a centerpiece.\n",
            "Answer: banana, peach, bowl\n",
            "Final Answer:\n",
            "After considering the expert’s input, I find that their reasoning aligns\n",
            "with my initial thoughts. The words ’banana’ and ’peach’ are indeed types\n",
            "of fruit, and their nutritional value and common uses further strengthen\n",
            "their association with the word ’fruit’. The word ’bowl’ is also\n",
            "associated with fruit in the context of a fruit bowl, which is a common\n",
            "way to store and display various types of fruit. Therefore, I agree with\n",
            "the expert’s choices.\n",
            "Final Answer: banana, peach, bowl.\n",
            "23\n",
            "Listing 8: An example of multi-agent generation from EVOAGENT for MMMU dataset.\n",
            "Question: <image 1> The elliptical orbit of a planet around the Sun is\n",
            "shown on the diagram. Which of the following statements is true?\n",
            "A. the eccentricity of the orbit is less than zero\n",
            "B. the eccentricity of the orbit is greater than 1\n",
            "C. the sun might be at point C\n",
            "D. the sun might be at point D\n",
            "E. the sun might be at point B (Correct)\n",
            "CoT:\n",
            "Reason: In an elliptical orbit, the Sun is located at one of the two foci\n",
            "of the ellipse. The eccentricity of an orbit is a measure of how much\n",
            "the orbit deviates from a perfect circle, with 0 being a perfect circle\n",
            "and 1 being a parabola. Since the orbit shown is an ellipse, the\n",
            "eccentricity must be between 0 and 1. Points A and E are the farthest\n",
            "points from the center of the ellipse, so the Sun cannot be located at\n",
            "either of those points. Point B is at the center of the ellipse, so the\n",
            "Sun cannot be located there either. This leaves points C and D as\n",
            "possible locations for the Sun. However, since the Sun must be at one of\n",
            "the foci and point D is closer to the center of the ellipse than point C,\n",
            "it is more likely that the Sun is located at point C.\n",
            "Answer: C. the sun might be at point C\n",
            "EvoAgent:\n",
            "Expert Agent #1 Description:\n",
            "You are a mathematician with a specialization in geometry, particularly\n",
            "in the study of conic sections which include ellipses. Your expertise\n",
            "allows you to understand the properties and definitions of ellipses,\n",
            "including the concept of foci. You can provide a mathematical perspective\n",
            "on why the Sun must be located at one of the foci of the elliptical\n",
            "orbit and not at any other point. Your knowledge will help explain the\n",
            "geometric reasons behind the positioning of the Sun in relation to the\n",
            "planet’s orbit, and why the given options in the statements do not align\n",
            "with the mathematical definition of an ellipse.\n",
            "Reason: An ellipse is defined as the set of all points for which the sum\n",
            "of the distances to two fixed points, called foci, is constant. The Sun,\n",
            "being one of the massive bodies in the solar system, must be located at\n",
            "one of the foci of the elliptical orbit of a planet due to the\n",
            "gravitational forces that govern the motion of the planets. Option E is\n",
            "correct because the Sun might be at point B, which is one of the foci of\n",
            "the ellipse. The other focus would be at a point symmetrically opposite\n",
            "to B along the major axis.\n",
            "Answer: E. the sun might be at point B.\n",
            "Expert Agent #2 Description:\n",
            "You are an astronomer with a specialization in observational astronomy.\n",
            "Your expertise lies in the practical observation and data collection of\n",
            "celestial bodies, including the use of telescopes and other instruments\n",
            "to study the positions and movements of planets and stars. You have a\n",
            "deep understanding of how the apparent motion of celestial bodies can be\n",
            "used to infer their actual motion in space. You can provide insight into\n",
            "how observations of planetary orbits, such as the one depicted in the\n",
            "diagram, have historically been used to deduce the laws of planetary\n",
            "motion and the positions of stars like the Sun. Your knowledge will help\n",
            "explain how practical observations support the theoretical understanding\n",
            "that the Sun must be located at one of the foci of the elliptical orbit.\n",
            "Reason: The eccentricity of an orbit is a measure of how much it deviates\n",
            "from being a perfect circle. It ranges from 0 (a perfect circle) to 1 (a\n",
            "parabola). An eccentricity less than zero or greater than 1 is not\n",
            "possible for an elliptical orbit. Therefore, options A and B are\n",
            "incorrect. According to Kepler’s laws of planetary motion, the Sun must\n",
            "be located at one of the foci of the elliptical orbit. Point B is located\n",
            "at a position that could be one of the foci of the ellipse.\n",
            "Answer: E. the sun might be at point B.\n",
            "24\n",
            "Listing 9: An example of multi-agent generation from EVOAGENT for ScienceWorld.\n",
            "Task 1: Your task is to determine if a metal fork is electrically\n",
            "conductive. The metal fork is located around the kitchen. First, focus on\n",
            "the metal fork. If conductive, place it in the blue box. If\n",
            "nonconductive, place it in the green box.\n",
            "EvoAgent:\n",
            "Expert Agent Description:\n",
            "You are a thermodynamics expert. Your expertise in heat transfer and\n",
            "temperature measurement is crucial for this task. You understand how to\n",
            "properly use a thermometer and interpret its readings. You also have\n",
            "knowledge about the properties of various substances, including their\n",
            "typical temperature ranges. Your task is to guide the assistant in using\n",
            "the thermometer to accurately measure the temperature of the unknown\n",
            "substance B.\n",
            "Task 2: Your task is to boil gallium. For compounds without a boiling\n",
            "point, combusting the substance is also acceptable. First, focus on the\n",
            "substance. Then, take actions that will cause it to change its state of\n",
            "matter.\n",
            "EvoAgent:\n",
            "Expert Agent Description:\n",
            "You are a metallurgist with a deep understanding of the properties and\n",
            "behaviors of metals, including gallium. Your expertise includes knowledge\n",
            "of the melting and boiling points of various metals, as well as the\n",
            "processes and equipment needed to heat them to these temperatures. You\n",
            "are also familiar with safety procedures and precautions when handling\n",
            "and heating metals. Your task is to provide detailed instructions on how\n",
            "to safely and effectively boil gallium in the foundry.\n",
            "Task 3: Your task is to change the state of matter of gallium. First,\n",
            "focus on the substance. Then, take actions that will cause it to change\n",
            "its state of matter.\n",
            "EvoAgent:\n",
            "Expert Agent Description:\n",
            "You are a chemist with a specialization in the properties and behaviors\n",
            "of substances. You have a deep understanding of the state changes of\n",
            "matter, specifically how substances like gallium can transition from\n",
            "solid to liquid or gas. You are familiar with the conditions required for\n",
            "these changes to occur, such as temperature and pressure. You also have\n",
            "knowledge of the equipment and procedures used in a foundry, which will\n",
            "be useful in this task. Your role is to provide guidance on how to\n",
            "manipulate the conditions in the foundry to achieve the desired state\n",
            "change in gallium.\n",
            "25\n",
            "Listing 10: An example of multi-agent generation from EVOAGENT for TravelPlanner dataset.\n",
            "Question: Can you help me with a travel plan departing from Buffalo to\n",
            "Atlanta for a duration of 3 days, specifically from March 2nd to March 4\n",
            "th, 2022? I plan to travel alone and my planned budget for the trip is\n",
            "around $1,100.\n",
            "EvoAgent:\n",
            "Expert Agent #1 Description:\n",
            "You are a culinary expert with a deep understanding of food culture in\n",
            "Atlanta. Your expertise includes knowledge of the best local restaurants,\n",
            "popular dishes, and dietary restrictions. You can provide\n",
            "recommendations for breakfast, lunch, and dinner spots that offer a\n",
            "unique taste of Atlanta’s food scene. Your suggestions should consider\n",
            "the traveler’s budget and the proximity of the restaurants to the\n",
            "attractions and accommodation listed in the plan.\n",
            "Expert Agent #2 Description:\n",
            "You are a travel logistics expert with extensive knowledge of\n",
            "transportation options and costs between Buffalo and Atlanta. Your\n",
            "expertise includes understanding flight schedules, costs, and durations,\n",
            "as well as alternative transportation methods such as self-driving and\n",
            "taxi services. You can provide the most cost-effective and time-efficient\n",
            "transportation options for the traveler, considering their budget and\n",
            "the travel dates provided in the plan.\n",
            "Expert Agent #3 Description:\n",
            "You are a local tourism expert with comprehensive knowledge of Atlanta’s\n",
            "attractions. Your expertise includes understanding the city’s popular\n",
            "tourist spots, historical sites, museums, parks, and unique local\n",
            "experiences. You can provide recommendations for attractions that the\n",
            "traveler can visit during their stay, considering their budget and the\n",
            "proximity of these attractions to the traveler’s accommodation and meal\n",
            "spots. Your suggestions should also take into account the operating hours\n",
            "of these attractions to ensure they align with the traveler’s schedule.\n",
            "Answer:\n",
            "Day 1:\n",
            "Current City: from Buffalo to Atlanta\n",
            "Transportation: Flight Number: F3502691, from Buffalo to Atlanta,\n",
            "Departure Time: 18:48, Arrival Time: 20:51\n",
            "Breakfast: Not applicable as the flight is in the evening.\n",
            "Attraction: Not applicable as the flight is in the evening.\n",
            "Lunch: Not applicable as the flight is in the evening.\n",
            "Dinner: Saffron, Atlanta\n",
            "ccommodation: Spacious private room close St. Barnabas Hospital, Atlanta\n",
            "Day 2:\n",
            "Current City: Atlanta\n",
            "Transportation: -\n",
            "Breakfast: Adda, Atlanta\n",
            "Attraction: Atlanta Botanical Garden, Atlanta; World of Coca-Cola,\n",
            "Atlanta.\n",
            "Lunch: Baba Au Rhum, Atlanta\n",
            "Dinner: Asian Bistro, Atlanta\n",
            "Accommodation: Spacious private room close St. Barnabas Hospital, Atlanta\n",
            "Day 3:\n",
            "Current City: from Atlanta to Buffalo\n",
            "Transportation: Flight Number: F3500648, from Atlanta to Buffalo,\n",
            "Departure Time: 21:24, Arrival Time: 23:26\n",
            "Breakfast: Chef Style, Atlanta\n",
            "Attraction: Georgia Aquarium, Atlanta; Martin Luther King, Jr. National\n",
            "Historical Park, Atlanta.\n",
            "Lunch: Pizza Central, Atlanta\n",
            "Dinner: Daawat-e-Kashmir, Atlanta\n",
            "Accommodation: -.\n",
            "26\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 문서 메타데이터 출력\n",
        "docs[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF5TQ5bMHUgg",
        "outputId": "1c8d07cc-6960-48d8-cb92-d42a42c79c43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Published': '2024-07-11',\n",
              " 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms',\n",
              " 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang',\n",
              " 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.',\n",
              " 'entry_id': 'http://arxiv.org/abs/2406.14228v2',\n",
              " 'published_first_time': '2024-06-20',\n",
              " 'comment': 'Work in process',\n",
              " 'journal_ref': None,\n",
              " 'doi': None,\n",
              " 'primary_category': 'cs.AI',\n",
              " 'categories': ['cs.AI'],\n",
              " 'links': ['http://arxiv.org/abs/2406.14228v2',\n",
              "  'http://arxiv.org/pdf/2406.14228v2']}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZaKyXPzjLPiB",
        "outputId": "5571f7a1-9735-4ab5-892b-02db932141d2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Document'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9dwQ36nNmqE",
        "outputId": "941cb71e-4c53-46d4-fe13-4fc5ddccc86e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.', 'entry_id': 'http://arxiv.org/abs/2406.14228v2', 'published_first_time': '2024-06-20', 'comment': 'Work in process', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.AI', 'categories': ['cs.AI'], 'links': ['http://arxiv.org/abs/2406.14228v2', 'http://arxiv.org/pdf/2406.14228v2']}, page_content='EVOAGENT: Towards Automatic Multi-Agent\\nGeneration via Evolutionary Algorithms\\nSiyu Yuan1∗, Kaitao Song2∗†,\\nJiangjie Chen1, Xu Tan2, Dongsheng Li2, Deqing Yang1†\\nFudan University1, Microsoft Research Asia2\\nsyyuan21@m.fudan.edu.cn, {kaitaosong, xuta, dongsli}@microsoft.com\\n{jjchen19,yangdeqing}@fudan.edu.cn\\nhttps://evo-agent.github.io\\nAbstract\\nThe rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically extend\\nthe specialized agent to multi-agent systems to improve task-solving capability\\nstill remains a significant challenge. In this paper, we introduce EVOAGENT, a\\ngeneric method to automatically extend expert agents to multi-agent systems via\\nthe evolutionary algorithm, thereby improving the effectiveness of LLM-based\\nagents in solving tasks. Specifically, we consider the existing agent frameworks\\nas the initial individual and then apply a series of evolutionary operators (e.g.,\\nmutation, crossover, selection, etc.) to generate multiple agents with diverse agent\\nsettings. EVOAGENT can be generalized to any LLM-based agent framework,\\nand can automatically extend the existing agent framework to multi-agent systems\\nwithout any extra human designs. Experimental results across various tasks have\\nshown that EVOAGENT can automatically generate multiple expert agents and\\nsignificantly enhance the task-solving capabilities of LLM-based agents.\\n1\\nIntroduction\\nRecently, the advent of large language models (LLMs) [1, 2, 3, 4] have shown remarkable capabilities\\nin solving language understanding, reasoning, and generation tasks. Based on the foundation of LLMs,\\nmany research works [5, 6, 7, 8, 9, 10, 11] have discovered that by empowering multiple advanced\\nskills (e.g., planning, tool, memory and so on), we can develop more powerful autonomous agents to\\nsolve more challenging tasks. Therefore, how to design and leverage LLM-based autonomous agents\\nto tackle more diverse and complex real-world applications has attracted enormous interest.\\nGenerally, many real-world scenarios are usually complex, encompassing a variety of challenging\\ntasks that are beyond the capability of a single agent. To address this point, we notice that human\\nsociety is composed of vast individuals, each possessing their unique characteristics. By selecting,\\norchestrating, and cooperating with different individuals, humans can form an efficient team group\\nto handle complicated missions in the real world. Therefore, there has been an increasing trend\\nto develop multi-agent collaboration frameworks (e.g., MetaGPT [10], AutoGen [12], Camel [13],\\n∗The first two authors have equal contributions. This work was done when the first author was an intern at\\nMicrosoft Research Asia.\\n† Corresponding authors.\\nPreprint. Under review.\\narXiv:2406.14228v2  [cs.AI]  11 Jul 2024\\nGenerative Agents [11]) to simulate human behaviors for solving complex tasks. By developing a\\nseries of expert agents with diverse settings, multi-agent systems enable us to reveal emergent abilities\\namong multiple agents and synergize their specialized expertise to achieve superior performance,\\nakin to simulating human populations. Nevertheless, it is worthy noting that, in most of (multi)-agent\\nframeworks, their designs heavily depend on handcrafted settings, including character roles, task\\nscopes, skills, and prompt settings. Although we admit that meticulous human design is quite useful\\nfor instructing LLM-based agents to understand tasks, it also limits scaling up the number of agents to\\nfurther improve performance due to expensive human labor. Considering the increasing popularity of\\nLLM-based autonomous agents, how to create a generic agent generation paradigm to automatically\\nbuild multi-agent systems has emerged as a critical challenge.\\nIn this paper, we introduce a novel method, EVOAGENT, that formulates agent generation as the\\nevolutionary processing [14] in human society. Specifically, to align human society, each agent\\ncan be considered as individuals that can procreate its population across successive generations.\\nMotivated by this mechanism, we can simulate such a human behavior to automatically generate\\nmultiple agents based on any pre-defined agents. Therefore, EVOAGENT can be considered as a\\none-shot agent generation method that starts from a specialized agent as the initial agent, and then\\nconsiders its settings (e.g., role, skills, prompts, and so on) as the variables to be evolved. With a\\nseries operation of EAs (e.g., selection, crossover, mutation), EVOAGENT can automatically create\\nmultiple evolutionary agents based on the initial specialized agent. Moreover, EVOAGENT is not\\nlimited to the infrastructure of agent frameworks, as it is a generic multi-agent generation method.\\nThus, it can be applied to any agent framework and expanded to multi-agent systems without any\\nextra human effort.\\nWe conduct experiments on multiple datasets, including knowledge-based question answering and\\nmulti-modal reasoning (§ 4.1), interactive scientific solving (§ 4.2) and real-world complex planning\\n(§ 4.3). Experimental results indicate that EVOAGENT can generate multiple agents with diverse\\nskills and harness their capabilities to consistently improve model performance in different scenarios.\\nBesides, to validate the scalability of EVOAGENT in creating massive agents, we also apply our\\nmethod to some conversational scenarios (e.g., debate), and the results also indicate the potential of\\nEVOAGENT in generating multiple diverse agents. Overall, the contributions of this paper can be\\nsummarized as below:\\n• We introduce EVOAGENT, a simple and generic multi-agent generation method to improve the\\neffectiveness of LLM-based agents in solving tasks. EVOAGENT can automatically generate new\\nexpert agents and is applicable to any agent framework.\\n• We formulate the agent generation processing as an evolutionary pipeline, that encompasses\\nmultiple operators (e.g., selection, crossover, mutation) to generate agent population without\\nadditional human supervision.\\n• We conduct extensive experiments on various tasks and demonstrate the effectiveness, scalability,\\nand generality of our EVOAGENT. Particularly, EVOAGENT can significantly enhance the perfor-\\nmance of LLM-based agents in both challenging open-world scenarios and complex real-world\\nplanning by generating more specialized agents.\\n2\\nRelated Work\\nLLM-based Autonomous Agents\\nWith the emergence of powerful large language models [1, 2, 3,\\n4], many researchers have endeavored to develop advanced autonomous agents [5, 6, 7] empowered by\\nmultiple high-level LLM skills (e.g., personas [11, 15, 16], planning [9, 17, 18, 19], tool [8, 6, 20, 21]\\nand memory [22, 23]). Some of them also extend agent frameworks to multi-agent collaboration\\n(e.g., MetaGPT [24], Generative Agents [11], AutoGen [12], Camel [13], AgentVerse [25] and so\\non), by designing multiple specific roles. These systems also demonstrate satisfactory performance in\\naddressing massive, challenging tasks. However, it is worth noting that most of the popular agent\\nframeworks heavily relied on handcrafted designs. The abundant human efforts necessitated by these\\nsystems also limit the adaptability and flexibility of agents to handle unexpected challenges [26, 27,\\n25, 10]. In this paper, we propose EVOAGENT, a method that can be applied to any LLM-based\\nagent framework and easily extend to multi-agent systems. By using EA, our method allows us to\\niteratively generate and optimize multiple agents with diverse settings.\\n2\\nAlgorithm 1: Multi-Agent Generation with Evolutionary Algorithm\\nRequire: Initial agent A(0,0), population size N per iteration, number of iterations T,\\nquality-check module LLMQuality(⋅), evolutionary operations EvoCrossover(⋅) and\\nEvoMutation(⋅), EvoUpdate(⋅)\\nInput: Initial result R0 derived from A(0,0)\\nOutput: Final result RT\\n1 for t = 1 to T do\\n2\\nCrossover: Update the settings of parent agents based on their generated results and initial\\nagent: {A\\n′\\n(0,t−1),A\\n′\\n(1,t−1),...,A\\n′\\n(N−1,t−1)} ←\\nEvoCrossover({R(0,t−1),R(1,t−1),...,R(N−1,t−1)},A(0,0));\\n3\\nMutation: Generate N ′(N ′ > N) child agents based on parent agents and initial agent:\\n{A(0,t),A(1,t),...,A(N ′−1,t)} ←\\nEvoMutation({A\\n′\\n(0,t−1),A\\n′\\n(1,t−1),...,A\\n′\\n(N−1,t−1)},A(0,0))\\n4\\nSelection: Select high-quality agents with quality-check module:\\n{At\\n0,...,At\\nN−1} ←LLMQuality({A(0,t),A(1,t),...,A(N ′−1,t)});\\n5\\nResult Update: Generate new result from new agents:\\n{R(0,t),R(1,t),...,R(N−1,t)} ←{A(0,t),A(1,t),...,A(N−1,t)}\\n6\\nIntegrate their results as a natural selection processing:\\nRt ←EvoUpdate({R(0,t),R(1,t),...,R(N−1,t)},Rt−1);\\n7 return RT ;\\nAgent Generation\\nRecent studies have shown that assigning personas or roles to LLM-based\\nautonomous agents can influence their behavior and performance in generation tasks [28, 29, 30, 13].\\nCurrent methods primarily involve manually assigning these personas and limit multi-agent col-\\nlaboration to single or fixed roles, which requires significant human effort and hinders generaliza-\\ntion [13, 12, 24, 10]. To address this, some frameworks like AgentVerse [25] and AutoAgents [31]\\nhave been proposed to automatically generate unlimited agents for collaborative task completion.\\nHowever, these methods still heavily depend on human-designed interventions, which limits their\\nscalability and functionality. For example, AutoAgents requires agent settings to satisfy a “Planner\\n- Agent Observer - Plan Observer” framework. Meanwhile, AgentVerse formulates a pipeline of\\n“Expert Recruitment - Collaborative Decision Making - Action Execution - Evaluation” to build\\nagents. These architectures also limit the task scope of designing agents. In contrast, EVOAGENT\\ncan automatically formulate the current agent frameworks to multi-agent systems with high-quality\\ngenerated expert agents by using EAs, which is flexible and adaptable to various agent frameworks.\\n3\\nMethod\\nGenerally, human society comprises a broad spectrum of individuals from diverse cultures, encom-\\npassing multiple generations. To solve specific tasks, human society usually involves a lot of expert\\nindividuals and aggregates their specialized expertise to achieve better answer. Thus, it can be\\nconsidered as the foundation to facilitate multi-agent collaborations. To fulfill this point, how to\\nautomatically create multiple agents would be very critical. Inspired by evolutionism, we formulate\\nagent generation as an evolutionary process to help us generate multiple agents without any human\\nlabor.\\n3.1\\nPreliminary\\nEvolutionary algorithm (EA) [32, 33], is a general algorithm to simulate the biological behaviors in\\nevolution, including reproduction, mutation, recombination, and selection. By introducing genetic\\nalgorithm [34, 35, 36, 37, 38] of the “survival of the fittest” mechanism, it can also be considered as\\nan optimization method to improve individuals. Therefore, EAs also belong to the non-parametric\\nlearning method, which can be applied to any framework. All we need to do is define which\\nparts should be evolved and the corresponding evolutionary operators. We also note some recent\\n3\\nworks [39, 40] indicate the potential of EAs that can be applied to optimize discrete prompts. So, in\\nthis paper, we explore how to formulate the agent generation problem as an evolutionary task.\\n3.2\\nEVOAGENT\\nBy assigning various settings to specific skills (e.g., role-playing, planning, tools and so on), agents\\ncould exhibit diverse task-solving capabilities. Therefore, our objective is to produce a population\\nof agents with distinct skills, to establish effective multi-agent systems. To fulfill this point, we\\ntreat each specialized agent as an unique individual and denote each skill as the part to be evolved,\\nakin to humans. So, we consider the procedure of agent generation to be evolutionary processing.\\nSpecifically, existing frameworks usually describe agent skills as the language. Thus, we can employ\\nLLM to simulate evolutionary operators to update the system settings of agents and create new agents.\\nHere, we formulate the procedure of EVOAGENT as a four-stage pipeline:\\nQuery: Please create a travel plan where I\\'ll depart from Washington and head to Myrtle \\nBeach for a 3-day trip from March 13th to March 15th, 2022. Can you help me keep this \\njourney within a budget of $1,400? It\\'s vital that my accommodations are pet-friendly.\\nSTEP 1: Initialization\\nInitial \\nAgent\\nHuman \\nWritten\\nSTEP 2: EA Operation Crossover & Mutation\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nLunch: Exotic India\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Cozy Brooklyn Room\\nInitial Agent\\nAccommodation \\nAgent\\nTransportation \\nAgent\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nBreakfast: Exotic India, Myrtle Beach\\nLunch: Catfish Charlie\\'s, Myrtle Beach\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Large sunny park slope apartment, \\npet-friendly\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nTransportation: Flight Number: F3792603\\nBreakfast: Exotic India, Myrtle Beach\\nLunch: Catfish Charlie\\'s, Myrtle Beach\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Cozy Brooklyn Room\\nHotel Agent\\nSTEP 3: Selection\\nQuality Check\\nThis agent has a duplicate type with \\nAccommodation Agent, so it is discarded.\\nSTEP 4: Results Update\\nUpdate \\nOperation\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nTransportation: Flight Number: F3792603\\nBreakfast: Exotic India, Myrtle Beach\\nLunch: Catfish Charlie\\'s, Myrtle Beach\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Large sunny park slope apartment, pet-friendly\\nFigure 1:\\nThe illustration of EVOAGENT. With the\\ngenerated multiple expert agents, EVOAGENT can gen-\\nerate a better travel plan to meet user preferences. For\\nEA operators, Crossover can improve the results of\\nparent agents by adjusting existing details (e.g., the in-\\nformation marked as blue). Mutation can introduce\\nnew variations to refine the results of parent agents by\\ngenerating child agents with new characteristics (e.g.,\\nthe information marked as red).\\nSTEP 1: Initialization\\nTo conduct EAs,\\nwe first need to confirm our initial agents.\\nHere, we enable EVOAGENT to start\\nfrom a pre-defined agent framework (e.g.,\\nMetaGPT [10] and AutoGen [12]), which\\nserves as the initial (parent) agents. More-\\nover, we also define which parts of this\\nagent should be upgraded. Generally, since\\nEAs is a generic algorithm, EVOAGENT\\nis applicable to any agent frameworks and\\nextends them as multi-agent frameworks.\\nWe will then explore how to generate new\\nagents in the next steps.\\nSTEP 2: Crossover & Mutation\\nIn the\\nfirst iteration, we directly use the initial\\nagents as the parents. And then, we design\\ntwo kinds of evolutionary operators, named\\nCrossover and Mutation. For Crossover,\\nwe first enable the parent agents to generate\\nresults based on user requests. Then, based\\non the generated results, we ask LLMs to\\ncheck which skills should be improved and\\nthen update them. This mechanism allows\\nus to generate child agents in new settings\\nwithout requiring any human labor. More-\\nover, we also need to guarantee the diver-\\nsity between the child agents and parents.\\nTo this end, we design a Mutation operation\\nthat requires LLMs to compare the child\\nagents and parent agents and then modify\\nthe child agents to make them distinct from\\ntheir parents while maintaining their task-\\nsolving capability. Based on these evolu-\\ntionary operators, we can generate effective\\nand diverse agents during one iteration. Be-\\nsides, as we also need to conduct multiple iterations, we will append all agents generated in the\\nprevious generation into the next iteration. How to select these agents during each iteration will be\\nintroduced next.\\nSTEP 3: Selection\\nBased on the above steps, we can obtain multiple candidate agents with diverse\\nsettings. To guarantee the quality of each agent, we also introduce a selection mechanism like EAs.\\nHere, we conduct a quality-check module with an LLM to detect whether the generated agents can\\nsatisfy it has inherited the characteristics and maintained differences from parent agents. We will\\nselect N child agents as the evolved agents in each iteration.\\n4\\nTable 1: Results of LLMs with different methods on Logic Grid Puzzle (Logic), Trivia Creative\\nWriting (Writing) and Codenames Collaborative (Codenames). The best results are bolded, and the\\nsecond best ones are underlined.\\nModel\\nMethod\\nLogic Writing Codenames\\nLLama2-13B-Chat\\nDirect\\n4.00\\n28.00\\n0.00\\nCoT\\n26.00\\n46.00\\n18.00\\nSelf-Refine3\\n33.50\\n31.20\\n12.37\\nSPP\\n0.00\\n4.00\\n1.00\\nEVOAGENT(1,3)\\n35.50\\n49.60\\n27.83\\nGPT-3.5\\nDirect\\n48.00\\n56.20\\n76.29\\nCoT\\n47.50\\n51.00\\n71.13\\nSelf-Refine3\\n47.50\\n59.19\\n46.39\\nSPP\\n56.00\\n54.40\\n61.86\\nEVOAGENT(1,3)\\n71.50\\n60.80\\n79.38\\nGPT-4\\nDirect\\n60.50\\n75.40\\n79.38\\nCoT\\n65.50\\n74.00\\n80.41\\nSelf-Refine3\\n64.50\\n74.60\\n79.38\\nSPP\\n64.50\\n79.20\\n78.35\\nEVOAGENT(1,3)\\n77.00\\n84.40\\n84.53\\nSTEP 4: Results Update\\nBased on the above steps, we obtain many new agents that evolved from\\nparent agents, but with diverse settings. To improve task-solving capabilities, we ask each child agent\\nto generate candidate results and then use LLMs to integrate these candidates with the result from the\\nprevious iteration into a new result, akin to a natural selection processing stage. Moreover, we can\\nautomatically generate more agents by repeating the operations from step 2 to step 4 until the number\\nof agents has fulfilled our targets.\\nBy introducing EA, EVOAGENT enables us to automatically extend the existing agent framework\\nto a multi-agent system without any extra human designs. The mechanism also makes EVOAGENT\\ncan be applied to any agent framework without any prerequisites. The entire process is illustrated in\\nFigure 1. And we also present the details of EVOAGENT in Algorithm 1.\\n4\\nExperiment\\nIn this section, we adopt EVOAGENT to multiple applications to illustrate that EVOAGENT can\\nhelp LLM-based agents better accomplish tasks with multi-agent generation.3 Furthermore, we also\\ndemonstrate that EVOAGENT can be applicable in supporting currently widely used multi-agent\\nframeworks, such as MetaGPT [10], AutoGen [12], and Camel [13].\\n4.1\\nNLP and Multi-Modal Tasks\\nBenchmarks\\nTo align previous experiences (e.g., Self-Refine [41] and Solo Performance Prompt-\\ning [42]), we select three NLP knowledge-intensive and reasoning-intensive tasks from [42] and one\\nmulti-modal task:\\n• Logic Grid Puzzle is a reasoning task with 200 puzzles featuring 2 to 5 unique occupants in\\ndifferent houses. The aim is to identify house numbers for one occupant with provided clues.\\n• Trivia Creative Writing is a knowledge-intensive task consisting of 100 instances. This task\\nrequires a model to write a coherent story while incorporating answers to N trivia questions.\\n• Codenames Collaborative is a reasoning-intensive task with 50 instances. It involves a model\\nidentifying target words based on a given hint and a complete list of words.\\n• MMMU [43] is a comprehensive benchmark for college-level, multi-discipline multi-modal\\nunderstanding and reasoning. MMMU has three levels of difficulty: easy, medium, and hard. We\\nevaluate EVOAGENT against baselines using the multiple-choice questions in the validation set of\\nMMMU, which includes 847 questions spanning 30 different domains.\\n3The data examples of EVOAGENT on these tasks are provided in Appendix F.\\n5\\nCoT\\nSR\\nSPP Ours\\n45\\n50\\n55\\n60\\n65\\nAccuracy(%)\\nAll\\nCoT\\nSR\\nSPP Ours\\n55\\n65\\n75\\nEasy\\nCoT\\nSR\\nSPP Ours\\n45\\n50\\n55\\n60\\nMedium\\nCoT\\nSR\\nSPP Ours\\n25\\n35\\n45\\n55\\nHard\\nGPT-4V\\nGemini-Pro\\nFigure 2: Overall results of GPT-4V and Gemini-Pro with different methods on the MMMU validation\\nset. We also compare the performance of GPT-4V and Gemini-Pro across three difficulty levels.\\nBaselines\\nFor NLP tasks, we select LLama2-13B-Chat [3], GPT-3.5 [44] and GPT-4 [1] as our\\nbackbone networks. We compare EVOAGENT with 0-shot learning (Direct), Chain-of-thought (CoT)\\nprompting [45] and Self-Refine [41] and Solo Performance Prompting (SPP) [42]. For Self-Refine,\\nwe follow [41] to design feedback and refine prompts with three iterations. SPP is not a multi-agent\\ncollaboration framework but a prompting strategy that asks a single LLM to identify and discuss with\\nmultiple personas with few-shot learning. For SPP, we follow the original setting [42] to make a fair\\ncomparison. For MMMU, we select GPT-4V [46] and Gemini-Pro as the backbone and compare\\nEVOAGENT with CoT prompting, Self-Refine, and SPP. 4\\nEvaluation Metrics\\nFor all benchmarks, we adhere to the evaluation metrics specified in the\\noriginal setting. Specifically, for Logic Grid Puzzle and MMMU tasks, we report the accuracy of\\nall questions. For Trivia Creative Writing, we measure the ratio of correctly mentioned answers in\\nthe trivia questions. For Codenames Collaborative, we calculate the overlapping ratio between the\\npredicted words from the Guesser and the target words as the metric.\\nResult & Analysis\\nIn our experiments, we adopt the agent settings of [42] (for NLP tasks) and\\n[43] (for MMMU) as the initial agent. For our method, we denote it as EVOAGENT(N,T ), where N\\nis the population size generated in each iteration, and T is the number of iterations. Here, to align\\nwith Self-Refine, we set N as 1 and T as 3, which means we conduct three iterations, each of which\\ngenerates a new expert agent. Our results are reported in Table 1, and we can observe:\\n1. By utilizing multiple generated agents, EVOAGENT can greatly improve LLM performances in\\nboth NLP knowledge and reasoning tasks. We also compare EVOAGENT with some pre-defined\\nagent generation frameworks, e.g., AgentVerse [25] and AutoAgent [31]. The results shown in\\nAppendix B prove that the EVOAGENT is even better than these agent generation frameworks.\\n2. When using weaker LLMs (e.g., LLama2-13B-Chat), SPP usually produces poor performances,\\nconsistent with the findings in [42]. This suggests the limited effectiveness of SPP in smaller and\\nless capable models. However, EVOAGENT can provide consistent improvements among each\\nLLM, proving its strong generalization by using diverse generated agents.\\nIn addition, Figure 2 shows that Self-Refine (SR) and SPP degrade performance compared to CoT\\nprompting in MMMU task. However, EVOAGENT can generate multiple domain-specific agents and\\nthus improve multi-modal models in addressing scientific questions across various difficulty levels.\\n4.2\\nInteractive Scientific Solving Simulation\\nBenchmark\\nCompared with traditional NLP or multi-modal tasks, autonomous agents usually need\\nto perform problem-solving abilities akin to humans in interactive and open-world environments.\\nCurrently, we choose ScienceWorld [47], a complex interactive environment requiring skills in\\nlong-term memory, sub-task decomposition, and scientific and commonsense knowledge. Here, we\\nevaluate 30 scientific tasks in ScienceWorld to demonstrate the capability of EVOAGENT in solving\\ntasks in more challenging open-world environments.\\n4The detailed model parameters and versions and full prompts for these methods can be found in Appendix A.\\n6\\nTable 2: Average Scores of different methods on Sci-\\nenceWorld. We also report performance on three\\ndifficult-level groups based on the average length of\\nthe oracle agent’s trajectories [48].\\nModel\\nOverall\\nLong Medium Short\\nGPT-3.5\\n17.12\\n6.28\\n19.91\\n27.90\\nw/ EVOAGENT(1,1)\\n19.02\\n7.25\\n18.87\\n33.26\\nGPT-4\\n27.97 10.58\\n36.00\\n42.41\\nw/ EVOAGENT(1,1)\\n30.42 11.38\\n36.17\\n48.67\\nBaseline and Evaluation Metrics\\nFollow-\\ning [48], we require LLMs to perform an\\naction at each step by using in-context learn-\\ning 5. For evaluation, each task in Science-\\nWorld includes some sub-tasks, and we re-\\nport the results by calculating the completed\\nsub-tasks for the whole task.\\nResult & Analysis\\nFor EVOAGENT, we\\nadopt the agent framework with original set-\\ntings in [48] as the initial agent. Since each\\nstep in ScienceWorld requires using EA, we\\nset the population size N as 1 and the itera-\\ntions T as 1 for efficiency, denoted as EVOA-\\nGENT(1,1). Results in Table 2 show that:\\n1. EVOAGENT can also extend interactive agents to multi-agent systems in solving complete scientific\\ntasks in dynamic, open-world environments and consistently improve the performance of LLMs.\\n2. Our method exhibits the most substantial improvement in short-trajectory tasks, with less sig-\\nnificant gains in medium and long-trajectory tasks. We argue that the capability of multi-agent\\nsystems will also be affected by a longer context. We also expect to investigate the effect of long\\ncontext on multi-agent systems in the future.\\nGenerally, these results also demonstrate the generalization of EVOAGENT, which can also be used\\nfor solving interactive tasks in an open-world environment.\\n4.3\\nReal-World Scenarios\\nBenchmark\\nMoreover, in addition to performing actions in interactive environments, planning in\\ncomplex and realistic environments is also a crucial skill for building autonomous agents. To validate\\nthis point, we also select TravelPlanner [49], a benchmark designed to evaluate language agents in\\nreal-world complex planning with multiple constraints.\\nBaseline and Evaluation Metrics\\nFollowing [49], we select Mistral-7B [50], GPT-3.5, Gemini-\\nPro [2] and GPT-4 as our backbone models. We compare EVOAGENT with 0-shot learning (Direct),\\nCoT prompting, SPP, and Self-Refine within each backbone model. Furthermore, we also attempt\\nthe ReAcT method [51] for GPT-3.5, which introduces a virtual ‘think’ action to generate sub-\\ntasks during the action planning process. For evaluation, we adhere to the original metrics from\\nTravelPlanner, reporting the delivery rate, commonsense constraint pass rate, hard constraint pass\\nrate, and final pass rate for all methods 6.\\nResult & Analysis\\nFor EVOAGENT, we adopt the original settings in TravelPlanner as the initial\\nagent. Results in Table 3 show that:\\n1. EVOAGENT can generate specialized agents, such as those focused on culinary experiences,\\ntransportation, and attractions. Therefore, the generated travel plans are more aligned with user\\npreferences (hard constraints) and commonsense rules;\\n2. Although existing paradigms (e.g., CoT, ReAct, Self-Refine, SPP) have demonstrated decent re-\\nsults in some conventional NLP tasks, they still lack capability in handling complex planning tasks\\nwithin TravelPlanner. These results also demonstrate that only using human-design prompting\\nstrategies is insufficient to handle complex planning tasks.\\n3. By using EVOAGENT to automatically generate multiple agents and forming a multi-agent\\ncollaboration paradigm, we can develop higher-quality plans that better meet user preferences.\\nThat also indicates the significance of multi-agent systems for complex planning tasks.\\n5The introduction of the settings of LLMs are shown in Appendix C.\\n6Detailed introduction of experiment settings is provided in Appendix D.\\n7\\nTable 3: Main results of different LLMs and planning strategies on the TravelPlanner validation set.\\nEVOAGENT(N,T ) indicates that the population size per iteration is N and the number of iterations is\\nT. The best results are bolded, and the second best ones are underlined.\\nModel\\nMethod\\nDelivery Commonsense Hard Constraint Final\\nRate\\nMicro Macro Micro\\nMacro\\nMistral-7B\\nDirect\\n100.0\\n64.7\\n2.2\\n3.1\\n0.0\\n0.0\\nCoT\\n100.0\\n60.5\\n1.1\\n1.0\\n0.0\\n0.0\\nSPP\\n100.0\\n55.1\\n0.0\\n0.7\\n0.6\\n0.0\\nSelf-Refine3\\n100.0\\n58.3\\n0.0\\n0.7\\n0.0\\n0.0\\nEVOAGENT(1,3)\\n100.0\\n60.1\\n2.2\\n4.5\\n0.6\\n0.0\\nGPT-3.5\\nDirect\\n100.0\\n57.3\\n3.9\\n11.0\\n3.3\\n0.0\\nCoT\\n100.0\\n61.0\\n2.8\\n10.0\\n3.3\\n0.0\\nReAct\\n82.2\\n42.3\\n0.6\\n11.9\\n4.6\\n0.0\\nSPP\\n99.4\\n54.6\\n1.7\\n3.8\\n1.1\\n0.0\\nSelf-Refine3\\n100.0\\n56.0\\n1.7\\n3.1\\n1.1\\n0.0\\nEVOAGENT(1,3)\\n100.0\\n64.2\\n7.8\\n11.0\\n4.4\\n1.1\\nEVOAGENT(1,5)\\n100.0\\n61.0\\n5.0\\n12.6\\n5.0\\n0.5\\nGemini-Pro\\nDirect\\n90.0\\n61.7\\n7.8\\n16.4\\n7.8\\n0.6\\nCoT\\n90.0\\n61.4\\n7.2\\n10.0\\n6.1\\n1.7\\nSPP\\n100.0\\n67.6\\n7.8\\n10.2\\n3.9\\n1.1\\nSelf-Refine3\\n95.6\\n65.8\\n6.1\\n15.0\\n4.4\\n0.6\\nEVOAGENT(1,3)\\n100.0\\n73.5\\n12.8\\n16.9\\n7.2\\n1.7\\nEVOAGENT(1,5)\\n100.0\\n74.0\\n8.9\\n21.2\\n11.7\\n2.2\\nGPT-4\\nDirect\\n100.0\\n79.4\\n15.8\\n27.5\\n16.1\\n2.2\\nCoT\\n100.0\\n76.7\\n11.7\\n22.4\\n12.8\\n2.2\\nSPP\\n96.7\\n70.6\\n5.6\\n11.4\\n7.8\\n0.6\\nSelf-Refine3\\n98.9\\n75.3\\n7.2\\n12.4\\n7.2\\n1.1\\nEVOAGENT(1,3)\\n100.0\\n81.5\\n21.1\\n31.4\\n18.9\\n7.2\\n4.4\\nAblation Studies\\nTo better understand the value of EVOAGENT, we conduct detailed analyses on TravelPlanner,\\nfocusing on the impact of population size and the effectiveness of the quality-check module in the\\nselection stage.\\nTable 4: Average commonsense constraint\\npass rate (Com.) and hard constraint pass rate\\n(Hard) of ablated variants on TravelPlanner.\\nMethod\\nw/o QC\\nw/ QC\\nCom. Hard Com. Hard\\nDirect\\n-\\n-\\n59.5\\n13.7\\nSuggest3\\n-\\n-\\n61.7\\n8.4\\nOvergen3\\n-\\n-\\n61.4\\n10.7\\nPromptRefine3\\n-\\n-\\n63.0\\n13.8\\nDifferent Population Size\\nEVOAGENT(1,3)\\n68.9\\n14.0\\n68.9\\n14.0\\nEVOAGENT(2,3)\\n62.8\\n12.7\\n67.0\\n15.2\\nEVOAGENT(3,3)\\n62.7\\n13.7\\n66.8\\n15.8\\nDifferent Selection Stategies\\nRandom\\n62.9\\n12.7\\n67.1\\n15.0\\nPK\\n63.5\\n13.6\\n66.4\\n14.5\\nAll-in\\n61.9\\n13.2\\n67.1\\n17.0\\nExperiment Settings\\nWe evaluate the performance\\nof different LLMs at varying population sizes N\\nwith fixed iteration number 3, denoted as EVOA-\\nGENT(N,3), both with and without the quality-check\\nmodule (QC). We employ an LLM that shares the\\nsame backbone as the initial agent for updates. To\\nselect results from candidates for this LLM to update,\\nwe adopt three different selection strategies: 1) Ran-\\ndom: one result is selected randomly from the pool\\nof candidates; 2) PK: we ask an agent with the same\\nbackbone as the initial agent to identify the optimal\\nresults from the pool of candidates; 3) All-in: Rather\\nthan selecting a single result, we update using all\\ncandidates.\\nMoreover, we also attempt Suggest3, Overgen3 and\\nPromptRefine3 as variants to prove the effectiveness\\nof our method. For Suggest3, instead of generating\\nnew results, we ask new generated agents to only give\\nsuggestions for initial agents to revise their results.\\nFor Overgen3, we first ask initial agents to generate\\n3 different results at one time, and then these agents\\n8\\nEvoAgent\\nInvesting in clean energy not only addresses the climate \\ncrisis but also creates jobs and strengthens our economy.\\n...transitioning to renewable energy can create \\nmillions of good-paying, union jobs without \\nsignificant unemployment or economic fallout...\\nLabor \\nEconomist \\nAgent\\nPresident Opinion: Support\\n...Thrusting forward with renewable energy \\nstrengthens our international ties and \\npropels economies reliant on fossil fuel \\nexports towards clean energy transitions...\\nGeopolitical \\nAnalyst \\nAgent\\n...Every moment we delay increases the \\nseverity of climate-related illnesses, straining \\nour health infrastructure and costing us $820 \\nbillion annually...\\nPublic Health \\nAgent\\nMetaGPT Framework\\nTopic: The U.S. should commit more in climate change fighting\\nEvoAgent\\nEnergy Sector \\nAnalyst Agent\\nThe promises of ample job creation overlook \\nthe reality that many displaced workers \\nfrom conventional sectors may struggle to \\nfind roles in the nascent green economy. \\nAn abrupt transition to renewable energy \\ncould cause economic tremors and job \\nlosses.\\nRisk \\nManagement \\nAgent\\nThis isn‘t about alarmism or denial, it’s \\nabout carefully leading our nation \\ntowards a sustainable, prosperous future. \\nAn abrupt shift spells risk!\\nTransition \\nStrategist \\nAgent\\nThe real crisis is the economic disaster under His policies. He \\ntalks about investments, but it\\'s your tax dollars he\\'s spending. \\nPresident Opinion: Oppose\\nFigure 3: The adaption of EVOAGENT on MetaGPT framework. With the EA, we can extend the\\noriginal role in the debate scenario to different expert agents to enrich the opinions.\\ncan output the final results based on these multiple\\ncandidates. For PromptRefine3, instead of generating agents, we ask the initial agent to refine its\\nprompts three times to better answer the query. 7\\nResult & Analysis\\nTo obtain stable findings, we first obtain results from GPT-3.5 and Gemini-Pro\\nacross different population sizes and selection strategies. We then average their results over various\\nmetrics to clearly compare the strengths and weaknesses of these variants.\\nThe results are shown in Table 4.8 We find that EVOAGENT significantly outperforms the Overgen,\\ndemonstrating the effectiveness of generating specialized agents to assist with complex planning.\\nAlthough obtaining suggestions from new generated agents can improve the performance on com-\\nmonsense constraints, these methods greatly harm the agents to meet the user preference. Modifying\\nthe prompt can improve the performance of agents, yet it remains less effective than EVOAGENT.\\nWhen the population size exceeds one, agents may generate similar agents. Thus, lacking a quality-\\ncheck module leads to reduced travel plan quality. Furthermore, when population size increases,\\nthe model aligns travel plans more closely with user preferences but diminishing adherence to\\ncommonsense rules, consistent with the findings in Table 3. Remarkably, the PK strategy initially\\nyields superior results without the quality-check module, but this trend reverses once quality checks\\nare implemented. We speculate that, without the quality-check module, PK partially fulfills this role,\\naiding in selecting better candidates. However, with the quality-check module, PK introduces bias by\\nfavoring specific fields of expertise while neglecting others, resulting in a less effective than random\\nstrategy. Meanwhile, the All-in strategy performs optimally when a quality-check module is included.\\nFuture research can leverage long-context LLMs to expand more agents with EVOAGENT to better\\nsolve complex real-world tasks.\\n4.5\\nEVOAGENT Application\\nPrevious experiments have demonstrated that our method can automatically extend existing agent\\nframeworks to multi-agent systems, thus greatly improving LLM-based agents in various scenarios.\\nWe also attempt to extend our work to real-world multi-agent applications (e.g., MetaGPT [10],\\nCamel [13] and AutoGen [12]), to verify it can scale up the number of agents in building multi-agent\\nscenarios, just as shown in Figure 3. Here, we choose the debate scenario used in MetaGPT, which\\nincludes two debaters with different opinions, leading to dull and repetitive content generation. Here,\\ninstead of manually assigning new roles, we applied EVOAGENT to extend each debate team to more\\nagents with diverse settings, increasing the variety of opinions and the quality of the debate 9.\\n7The full prompts of different ablation settings are shown in Appendix A.1.\\n8The complete results with further analysis are shown in Appendix E\\n9The details of MetaGPT, and the adaption of EVOAGENT on Camel and AutoGen are shown in Appendix G.\\n9\\n5\\nConclusion\\nIn this paper, we propose EVOAGENT, an automatic multi-agent generation system by leveraging\\nevolutionary algorithms. Different from previous methods, EVOAGENT is suitable to any existing\\nagent framework and extends it to multi-agent systems with diverse and effective agents by using\\na series of evolutionary operations, including mutation, crossover, and selection. Experiments on\\nmultiple tasks show that EVOAGENT can significantly improve the capabilities of LLM-based agents\\nin solving complex tasks.\\nReferences\\n[1] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.\\n[2] Gemini Team. Gemini: A family of highly capable multimodal models, 2023.\\n[3] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\\nKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-\\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\\nand fine-tuned chat models. CoRR, abs/2307.09288, 2023.\\n[4] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.\\n[5] Significant Gravitas. Auto-gpt: An autonomous gpt-4 experiment. https://github.com/\\nSignificant-Gravitas/Auto-GPT, 2023.\\n[6] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-\\nginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580,\\n2023.\\n[7] Yohei Nakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023.\\n[8] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle-\\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\\nthemselves to use tools. CoRR, abs/2302.04761, 2023.\\n[9] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\\nmodels. In NeurIPS, 2022.\\n[10] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang,\\nCeyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng\\nXiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent\\ncollaborative framework. In The Twelfth International Conference on Learning Representations,\\n2024.\\n[11] Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang,\\nand Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In\\nProceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,\\nUIST 2023, San Francisco, CA, USA, 29 October 2023- 1 November 2023, pages 2:1–2:22.\\nACM, 2023.\\n10\\n[12] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\\nLi Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via\\nmulti-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.\\n[13] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\\nGhanem. CAMEL: Communicative agents for ”mind” exploration of large language model\\nsociety. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n[14] Thomas Bäck and Hans-Paul Schwefel. An overview of evolutionary algorithms for parameter\\noptimization. Evol. Comput., 1(1):1–23, 1993.\\n[15] Xintao Wang, Yunze Xiao, Jen tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei,\\nZiang Leng, Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. Incharacter: Evaluating\\npersonality fidelity in role-playing agents through psychological interviews, 2024.\\n[16] Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li,\\nRuihan Yang, Tinghui Zhu, et al. From persona to personalization: A survey on role-playing\\nlanguage agents. arXiv preprint arXiv:2404.18231, 2024.\\n[17] Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, and Kyle Richardson. Put\\nyour money where your mouth is: Evaluating strategic planning and execution of llm agents in\\nan auction arena. arXiv preprint arXiv:2310.05746, 2023.\\n[18] Yikai Zhang, Siyu Yuan, Caiyu Hu, Kyle Richardson, Yanghua Xiao, and Jiangjie Chen.\\nTimearena: Shaping efficient multitasking language agents in a time-aware simulation. arXiv\\npreprint arXiv:2402.05733, 2024.\\n[19] Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski,\\nYanghua Xiao, and Deqing Yang. Distilling script knowledge from large language models for\\nconstrained language planning. arXiv preprint arXiv:2305.05252, 2023.\\n[20] Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu,\\nDongsheng Li, and Yueting Zhuang. Taskbench: Benchmarking large language models for task\\nautomation. arXiv preprint arXiv:2311.18760, 2023.\\n[21] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li,\\nand Deqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv\\npreprint arXiv:2401.06201, 2024.\\n[22] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In 3rd International\\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\\nConference Track Proceedings, 2015.\\n[23] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\\ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\\n[24] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human\\nbehaviors for llm-based task-oriented coordination via collaborative generative agents. arXiv\\npreprint arXiv:2310.06500, 2023.\\n[25] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu,\\nYaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong\\nSun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent\\nbehaviors. In The Twelfth International Conference on Learning Representations, 2024.\\n[26] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu,\\nand Maosong Sun.\\nCommunicative agents for software development.\\narXiv preprint\\narXiv:2307.07924, 2023.\\n[27] Zhitao He, Pengfei Cao, Yubo Chen, Kang Liu, Ruopeng Li, Mengshu Sun, and Jun Zhao.\\nLEGO: A multi-agent collaborative framework with role-playing and iterative feedback for\\ncausality explanation generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,\\nFindings of the Association for Computational Linguistics: EMNLP 2023, pages 9142–9163,\\nSingapore, December 2023. Association for Computational Linguistics.\\n11\\n[28] Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong\\nMao. Expertprompting: Instructing large language models to be distinguished experts. arXiv\\npreprint arXiv:2305.14688, 2023.\\n[29] Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik\\nNarasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. In Houda\\nBouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational\\nLinguistics: EMNLP 2023, pages 1236–1270, Singapore, December 2023. Association for\\nComputational Linguistics.\\n[30] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceed-\\nings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST\\n’23, New York, NY, USA, 2023. Association for Computing Machinery.\\n[31] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu,\\nand Yemin Shi. Autoagents: A framework for automatic agent generation. arXiv preprint\\narXiv:2309.17288, 2023.\\n[32] Thomas Bartz-Beielstein, Jürgen Branke, Jörn Mehnen, and Olaf Mersmann. Evolutionary\\nalgorithms. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 4(3):178–\\n195, 2014.\\n[33] Agoston E Eiben, James E Smith, AE Eiben, and JE Smith. What is an evolutionary algorithm?\\nIntroduction to evolutionary computing, pages 25–48, 2015.\\n[34] Jeffrey R Sampson. Adaptation in natural and artificial systems (john h. holland), 1976.\\n[35] John H Holland. Adaptation in natural and artificial systems: an introductory analysis with\\napplications to biology, control, and artificial intelligence. MIT press, 1992.\\n[36] Melanie Mitchell. An introduction to genetic algorithms. MIT press, 1998.\\n[37] Lothar M Schmitt. Theory of genetic algorithms. Theoretical Computer Science, 259(1-2):1–61,\\n2001.\\n[38] Seyedali Mirjalili, Jin Song Dong, Ali Safa Sadiq, and Hossam Faris. Genetic algorithm:\\nTheory, literature review, and application in image reconstruction. Nature-inspired optimizers:\\nTheories, literature reviews and applications, pages 69–85, 2020.\\n[39] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang\\nBian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields\\npowerful prompt optimizers. CoRR, abs/2309.08532, 2023.\\n[40] Angelica Chen, David Dohan, and David R. So. Evoprompting: Language models for code-level\\nneural architecture search. In Advances in Neural Information Processing Systems 36: Annual\\nConference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,\\nUSA, December 10 - 16, 2023, 2023.\\n[41] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine:\\nIterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information\\nProcessing Systems, 2023.\\n[42] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing\\ncognitive synergy in large language models: A task-solving agent through multi-persona\\nself-collaboration. arXiv preprint arXiv:2307.05300, 2023.\\n[43] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\\nunderstanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.\\n12\\n[44] OpenAI. Chatgpt, 2022.\\n[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\\nQuoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language\\nmodels. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,\\nAdvances in Neural Information Processing Systems, 2022.\\n[46] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\\nLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\\narXiv:2309.17421, 9(1):1, 2023.\\n[47] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Science-\\nWorld: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa Kozareva, and\\nYue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 11279–11298, Abu Dhabi, United Arab Emirates, December 2022.\\nAssociation for Computational Linguistics.\\n[48] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula,\\nPrithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with\\nfast and slow thinking for complex interactive tasks. In Thirty-seventh Conference on Neural\\nInformation Processing Systems, 2023.\\n[49] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao,\\nand Yu Su. Travelplanner: A benchmark for real-world planning with language agents. arXiv\\npreprint arXiv:2402.01622, 2024.\\n[50] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\\nSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\\nLavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.\\n[51] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and\\nYuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh\\nInternational Conference on Learning Representations, 2023.\\nA\\nExperiment Settings\\nA.1\\nPrompt for Baselines and EVOAGENT\\nListing 1 and 2 shows the full prompt for 0-shot learning (Direct), Chain-of-thought (CoT) prompt-\\ning [45] and Self-Refine [41] and Solo Performance Prompting, i.e., SPP [42]. Listing 3 and 4 show\\nthe prompt of EVOAGENT and different ablation settings.\\nA.2\\nModel Selection\\nFor OpenAI models,\\nwe use gpt-35-turbo and gpt-4-32k with the version of\\n2024-02-15-preview in Azure.10 For Gemini-pro, we use Google Gemini-Pro APIs to ob-\\ntain results. We set the temperature to 0 for all models.\\nB\\nEVOAGENT v.s. Human-designed Agent Framework\\nAgentVerse [25] and AutoAgent [31] are frameworks designed to generate an unlimited number\\nof agents for collaborative tasks automatically. Despite this automation, they still rely on human-\\ndesigned interventions. AutoAgents requires agent settings to satisfy a “Planner - Agent Observer\\n- Plan Observer” framework, while AgentVerse formulates a pipeline of “Expert Recruitment -\\nCollaborative Decision Making - Action Execution - Evaluation” to build agents. We argue that these\\nhuman-designed architectures limit their scalability and functionality.\\n10https://azure.microsoft.com/en-us/products/ai-services/openai-service\\n13\\nTable 5: Comparison of EVOAGENT\\nwith human-designed agent frameworks\\nin Logic Grid Puzzle, Trivia Creative\\nWriting tasks.\\nFramework Logic Writing\\nEVOAGENT\\n77.00\\n84.40\\nAgentVerse\\n66.50\\n-\\nAutoAgents\\n-\\n82.00\\nIn the original papers, AgentVerse [25] and AutoA-\\ngent [31] also conduct experiments on the Logic Grid\\nPuzzle task and Trivia Creative Writing task, respectively.\\nWe follow their experimental settings and compared them\\nwith our method. As demonstrated in Table 5, EVOAGENT\\noutperforms both AgentVerse and AutoAgent, highlight-\\ning the effectiveness and generality of EVOAGENT.\\nC\\nExperimental Details of ScienceWorld\\nFollowing [48], we adopt the REACT [51] method for\\neach LLM, which introduces a virtual ’think’ action. This\\naction allows LLMs to generate subgoals during the action\\nplanning process.\\nD\\nEvaluation Details of TravelPlanner\\nGrounding to travel planning, a real-world use-case that inherently involves various constraints like\\nuser preferences and commonsense rules, TravelPlanner evaluates whether agents can formulate\\nflexible travel plans using gathered information to meet these constraints. We test EVOAGENT and all\\nbaselines on the TravelPlanner validation set, which consists of 180 user queries with the collected\\ninformation. To evaluate the travel plans generated by agents, TravelPlanner adopts the following\\nevaluation metrics:\\n• Delivery Rate: Assesses if agents can complete a plan within a limited number of steps (30 in our\\nexperimental setting). Failures are due to dead loops, numerous failed attempts, or exceeding the\\nstep limit.\\n• Commonsense Constraint Pass Rate: Evaluates if an agent can incorporate commonsense into their\\nplan.\\n• Hard Constraint Pass Rate: Measures if a plan meets all explicit hard constraints in the query,\\ntesting the agent’s ability to adapt to diverse user preferences.\\n• Final Pass Rate: Indicates the proportion of viable plans that meet all criteria, reflecting the agent’s\\nproficiency in creating practical plans.\\nFurthermore, TravelPlanner uses micro and macro strategies to assess the Commonsense and Hard\\nConstraint Pass Rates. The micro strategy calculates the ratio of met constraints to the total. The\\nmacro strategy measures the proportion of plans that meet all commonsense or hard constraints.\\nTogether, these strategies assess an agent’s ability to satisfy individual constraints and all constraints\\ncomprehensively.\\nE\\nMore Analysis of Ablation Studies\\n0\\n1\\n3\\n5\\n7\\nIteration\\n56\\n58\\n60\\n62\\nAnswer Ratio (%)\\nFigure 4: The performance of\\nGPT-3.5 with EVOAGENT under\\ndifferent iterations on Trivia Cre-\\native Writing task.\\nThe complete results of ablation studies on TravelPlanner are\\nshown in Table 6. This result indicates that the absence of the\\nquality-check module significantly lowers the delivery pass rate\\nwhen the All-in strategy is applied. To explore the reasons, we\\nrevisit the results and discover that sometimes unsuitable agents\\ncreate overly lengthy travel plans that fail to meet the criteria.\\nFor example, the model might erroneously assign a nutritionist to\\ndevise travel plans, resulting in excessively detailed meal arrange-\\nments and nutritional breakdowns. Therefore, the input length\\nsurpasses the context window of LLMs, preventing the final result\\ngeneration.\\nMoreover, we also conduct experiments on the Trivia Creative\\nWriting task to investigate the impact of the number of iterations\\non model performance in traditional NLP tasks. As shown in\\n14\\nTable 6: Comparison of different popularity selection strategies for LLMs on TravelPlanner. The best\\nresults are bolded, and the second best ones are underlined.\\nModel\\nStrategy Method\\nw/o Quality Check\\nw/ Quality Check\\nDelivery Com. Hard Delivery Com. Hard\\nGPT-3.5\\nDirect\\n-\\n-\\n-\\n100.0\\n57.3\\n11.0\\nSuggest3\\n-\\n-\\n-\\n100.0\\n57.5\\n5.7\\nOvergen3\\n-\\n-\\n-\\n98.3\\n56.3\\n9.0\\nPromptRefine3\\n-\\n-\\n-\\n100.0\\n61.2\\n11.0\\nEVOAGENT(1,3)\\n100.0\\n64.2\\n11.0\\n100.0\\n64.2\\n11.0\\nRandom\\nEVOAGENT(2,3)\\n100.0\\n59.4\\n10.2\\n100.0\\n65.4\\n13.8\\nEVOAGENT(3,3)\\n98.9\\n59.2\\n11.4\\n100.0\\n65.8\\n14.0\\nPK\\nEVOAGENT(2,3)\\n99.4\\n59.4\\n7.1\\n100.0\\n66.0\\n11.7\\nEVOAGENT(3,3)\\n98.9\\n58.5\\n11.2\\n100.0\\n61.3\\n12.4\\nAll-in\\nEVOAGENT(2,3)\\n97.2\\n59.4\\n10.0\\n100.0\\n64.2\\n15.5\\nEVOAGENT(3,3)\\n93.3\\n56.0\\n8.3\\n100.0\\n65.2\\n12.6\\nGemini-Pro\\nDirect\\n-\\n-\\n-\\n90.0\\n61.7\\n16.4\\nSuggest3\\n-\\n-\\n-\\n100.0\\n65.8\\n11.0\\nOvergen3\\n-\\n-\\n-\\n100.0\\n66.5\\n12.4\\nPromptRefine3\\n-\\n-\\n-\\n96.7\\n64.9\\n16.7\\nEVOAGENT(1,3)\\n100.0\\n73.5\\n16.9\\n100.0\\n73.5\\n16.9\\nRandom\\nEVOAGENT(2,3)\\n96.7\\n65.9\\n13.1\\n99.4\\n67.3\\n14.0\\nEVOAGENT(3,3)\\n97.2\\n67.0\\n16.0\\n100.0\\n70.0\\n18.1\\nPK\\nEVOAGENT(2,3)\\n97.2\\n67.4\\n19.0\\n99.4\\n69.8\\n17.1\\nEVOAGENT(3,3)\\n97.2\\n68.5\\n17.1\\n99.4\\n68.4\\n16.7\\nAll-in\\nEVOAGENT(2,3)\\n95.0\\n65.1\\n16.7\\n99.4\\n69.0\\n19.0\\nEVOAGENT(3,3)\\n95.0\\n66.9\\n17.9\\n100.0\\n70.1\\n20.7\\nFigure 4, model performance improves with increasing iterations.\\nHowever, the improvement plateaus when the iteration count exceeds three. We suggest that traditional\\nNLP tasks are relatively simple, and beyond a certain iteration number, even with a quality-check\\nmodule in place, the generated agents tend to be similar and thus converge.\\nF\\nExamples of EVOAGENT\\nF.1\\nEVOAGENT Examples of NLP reasoning and knowledge tasks\\nListing 5, 6 and7 presents some multi-agent generation examples generated by GPT-4 based EVOA-\\nGENT in Logic Grid Puzzle, Trivia Creative Writing and Codenames Collaborative for a better\\nunderstanding.\\nF.2\\nEVOAGENT Examples of MMMU\\nListing 8 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nMMMU dataset for a better understanding.\\nF.3\\nEVOAGENT Examples of ScienceWorld\\nListing 9 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nScienceWorld for a better understanding.\\nF.4\\nEVOAGENT Examples of TravelPlanner\\nListing 10 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nTravelPlanner for a better understanding.\\n15\\nCamel Framework\\nAI Assistant\\nAI User\\nAI User\\nAI User\\nEvoAgent\\nAutoGen Framework\\nAssistant Agent\\nUserProxy \\nAgent\\nGeopolitical Analyst \\nAgent\\nLabor Economist \\nAgent\\nEvoAgent\\nPublic Health \\nAgent\\nGroup Chat Manager\\nTopic: The U.S. should commit more in climate change fighting\\nLabor Economist \\nAgent\\nGeopolitical Analyst \\nAgent\\nFigure 5: The adaption of EVOAGENT on Camel and AutoGen frameworks.\\nG\\nExamples of EVOAGENT’s Adaption to Multi-agent Collaboration\\nFrameworks\\nG.1\\nEVOAGENT for MetaGPT\\nMetaGPT [10] is a meta-programming framework that enhances LLM-based multi-agent collabo-\\nrations by integrating efficient human workflows. It employs an assembly line approach to assign\\ndiverse roles to agents, effectively simplifying complex tasks into manageable subtasks that multi-\\nple agents can execute collaboratively. As shown in Figure 3, instead of manually creating roles,\\nEVOAGENT can be used to automatically generate specialized agents for effective collaboration.\\nG.2\\nEVOAGENT for Camel\\nCamel [13] is recognized for its framework that supports communicative role-playing agents. Initially,\\nhumans establish this framework by conceptualizing an idea and designing specific roles, such as the\\nAI assistant role and the AI user role. These roles are then assigned to the assistant and user agents,\\nrespectively, enabling them to fulfill the task. As illustrated in Figure 5, EVOAGENT can be utilized\\nto automatically produce agents from AI assistants for interaction with AI users, bypassing the need\\nfor manual role design.\\nG.3\\nEVOAGENT for AutoGen\\nAutoGen [12] offers a framework that enables the creation of customizable and conversable agents\\nby integrating various LLMs. Initially, humans configure the assistant agents along with a user proxy\\nagent. Then, a group chat manager is responsible for selecting a speaker, gathering responses, and\\ndisseminating the message. As depicted in Figure 5, EVOAGENT facilitates the creation of multiple\\nexpert roles from a single assistant agent, thereby increasing the agent number in group chats without\\nthe need for manual design.\\n16\\nListing 1: Instruction templates for for 0-shot learning (Direct), Chain-of-thought (CoT) prompting\\nand Self-Refine method\\nDirect Method:\\n{question}\\nAnswer:\\nCoT Method:\\n{question}\\nYou need to give reasons first and then give the answer.\\nAnswer:\\nSelf-Refine Method:\\nStep One: Feedback Generation:\\nYou are a helpful assistant that provides feedback on {task}\\n{question}\\nThis is the answer from a student: {answer}.\\nPlease do not refine the answer but give some insightful suggestions for\\nthe student to help him better answer the question.\\nSuggestion:\\nStep Two: Result Refine:\\n{question}\\nThis is your answer:\\n{answer}\\nThere is the suggestion from an assistant:\\nSuggestion: {feedback}\\nNow you can refine your answer with his suggestion to better answer the\\nquestion.\\nKeep in mind that his suggestion may not be correct, so critically decide\\nwhether to accept his response or stick with your original one.\\nYou need to give reasons first and then give the answer.\\nRevised Answer:\\n17\\nListing 2: Instruction templates for SPP method\\nWhen faced with a task, begin by identifying the participants who will\\ncontribute to solving the task. Then, initiate a multi-round\\ncollaboration process until a final solution is reached. The participants\\nwill give critical comments and detailed suggestions whenever necessary.\\nHere are some examples:\\n---\\nExample Task 1: Use numbers and basic arithmetic operations (+ - * /) to\\nobtain 24. You need to use all numbers, and each number can only be used\\nonce.\\nInput: 6 12 1 1\\nParticipants: AI Assistant (you); Math Expert\\nStart collaboration!\\nMath Expert: Let’s analyze the task in detail. You need to make sure that\\nyou meet the requirement, that you need to use exactly the four numbers\\n(6 12 1 1) to construct 24. To reach 24, you can think of the common\\ndivisors of 24 such as 4, 6, 8, 3 and try to construct these first. Also\\nyou need to think of potential additions that can reach 24, such as 12 +\\n12.\\nAI Assistant (you): Thanks for the hints! Here’s one initial solution:\\n(12 / (1 + 1)) * 6 = 24\\nMath Expert: Let’s check the answer step by step. (1+1) = 2, (12 / 2) =\\n6, 6 * 6 = 36 which is not 24! The answer is not correct. Can you fix\\nthis by considering other combinations? Please do not make similar\\nmistakes.\\nAI Assistant (you): Thanks for pointing out the mistake. Here is a\\nrevised solution considering 24 can also be reached by 3 * 8: (6 + 1 + 1)\\n* (12 / 4) = 24.\\nMath Expert: Let’s first check if the calculation is correct. (6 + 1 + 1)\\n= 8, 12 / 4 = 3, 8 * 3 = 24. The calculation is correct, but you used 6\\n1 1 12 4 which is not the same as the input 6 12 1 1. Can you avoid using\\na number that is not part of the input?\\nAI Assistant (you): You are right, here is a revised solution considering\\n24 can be reached by 12 + 12 and without using any additional numbers: 6\\n* (1 - 1) + 12 = 24.\\nMath Expert: Let’s check the answer again. 1 - 1 = 0, 6 * 0 = 0, 0 + 12 =\\n12. I believe you are very close, here is a hint: try to change the \"1 -\\n1\" to \"1 + 1\".\\nAI Assistant (you): Sure, here is the corrected answer:\\n6 * (1+1) + 12 =\\n24\\nMath Expert: Let’s verify the solution. 1 + 1 = 2, 6 * 2 = 12, 12 + 12 =\\n12. You used 1 1 6 12 which is identical to the input 6 12 1 1.\\nEverything looks good!\\nFinish collaboration!\\nFinal answer: 6 * (1 + 1) + 12 = 24\\n---\\n{question}\\n18\\nListing 3: Instruction templates for EVOAGENT\\nCrossover and Mutation:\\n{question}\\nThis is your result:\\n{answer}\\nNow, you can create and collaborate with multiple experts to improve your\\nresult. Therefore, please describe in as much detail as possible the\\ndifferent skills and focuses you need from multiple experts individually.\\nWe will provide each expert with the same information and query. However\\n, please note that each profession has its own specialization, so you can\\nassign each expert to just one sub-task to ensure a more refined\\nresponse. We will relay their responses to you in turn, allowing you to\\nreorganize them into a better answer. Please note that the description\\nshould be narrated in the second person, for example: You are a XXX.\\nThese are the descriptions of the experts you have created before for\\nthis task:\\n{description}\\nTherefore, please remember you should not repeatedly create the same\\nexperts as described above.\\nNow, you can give the description for a new expert (Please note that only\\nbe one, do not give multiple at one time):\\nQuality Check:\\n{question}\\nWe employ mulitple experts to answer this query. The following is a\\nsecond-person introduction to the experts we have hired:\\n{description_ls}\\nNow, we will hire a new expert to help better respond to user query. Here\\nis a second person description of the new expert: {description}\\nPlease evaluate the new expert based on the following criteria to decide\\nwhether they should be retained or not:\\n1. The new expert is distinct and does not duplicate any previously hired\\nexperts.\\n2. Based on the new expert’s description, determine if they can\\neffectively assist in answering users’ questions.\\nGive the reason first and then give the choice. If retaining, please\\nreply with: Retain. If discarding, please reply with: Discard.\\nResult Update:\\n{question}\\nThis is your result:\\n{old_answer}\\nYou invite an expert whose description is: {description}\\nThis expert also give his answer based on his own professional knowledge:\\n{new_answer}.\\nNow you can refine your result with his answer to better answer the\\nquestion.\\nKeep in mind that his answer may not be correct, so critically decide\\nwhether to accept his response or stick with your original one.\\nRevised Answer:\\n19\\nListing 4: Instruction templates of different ablation settings in EVOAGENT.\\nPK:\\n{question}\\nWe invite {n} experts. They give the results based on their own\\nprofessional knowledge:\\nHere are second-person descriptions of these experts with their answers:\\n{select}\\nNow you can should help us select the best result which can meet the\\nquery.\\nYou need to give reasons first and then give the answer with the format:\\n\"Final Answer: Expert #XX\"\\nAll-in:\\n{question}\\nThis is your answer: {old_answer}.\\nFurthermore, you also invite {n} experts. They also give answers based on\\ntheir own professional knowledge:\\nHere are second person descriptions of these experts with their answers:\\n{description_ls}\\nNow you can refine your answer with these answers to better meet the\\nquery.\\nSuggest:\\n{Expert_Agent_description}\\n{question}\\nThis is the result from an AI assistant: {answer}.\\nPlease do not refine the plan but give some insightful suggestions for\\nthe AI assistant to help it better meet the user’s query.\\nSuggestion:\\nOverGen:\\n{question}\\nPlease generate three different results at one time for user to choose\\nfrom. The format can be:\\nResult #1:\\nResult #2:\\nResult #3:\\nThree Different Candidate Results:\\nPromptRefine:\\n{question}\\nThis is the result from an AI assistant, which description is\\n\"{original_description}\":\\n{answer}.\\nPlease do not refine the result but refine the description of the AI\\nassistant to help it better answer the user’s query.\\nPlease note that the description should be narrated in the second person,\\nfor example: You are a XXX.\\nDescription:\\n20\\nListing 5: An example of multi-agent generation from EVOAGENT for Logic Grid Puzzle\\nQuestion: There are 5 houses in a row, numbered 1 on the left to 5 on the\\nright. There is one person living in each house. The people in these\\nhouses have different characteristics:\\n- Each person has a different kind of car: one owns an SUV, one owns a\\nsedan, one drives a convertible, one drives a minivan, and one drives a\\ntruck\\n- Each person has a favorite color: one likes red, one likes yellow, one\\nlikes green, one likes white, and one likes blue\\n- Each person is wearing different shoes: one is wearing running shoes,\\none is wearing boots, one is wearing high heels, one is wearing sandals,\\nand one is wearing loafers\\nClue(s):\\n1. There is one house between where the person who is wearing loafers\\nlives and where the person who drives a minivan lives. 2. The person who\\ndrives a truck does not live in the fourth house. 3. The person who\\ndrives a truck lives directly left of the person who is wearing sandals.\\n4. The person who likes red lives somewhere to the right of the person\\nwho likes yellow. 5. The person who is wearing loafers lives somewhere to\\nthe left of the person who drives a truck. 6. The person who likes blue\\nlives in the fifth house. 7. There are two houses between where the\\nperson who owns a sedan lives and where the person who is wearing running\\nshoes lives. 8. The person who is wearing boots lives somewhere to the\\nleft of the person who drives a truck. 9. The person who likes white\\nlives in the fourth house. 10. There is one house between where the\\nperson who likes green lives and where the person who drives a truck\\nlives. 11. The person who owns an SUV does not live in the first house.\\nWhat is the number of the house where the person who is wearing boots\\nlives? choice: 2, choice: 5, choice: 3, choice: 1, choice: 4.\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a logician, specializing in logical reasoning and problem-solving\\n. Your task is to analyze the given clues and use deductive reasoning to\\ndetermine the correct sequence of houses, their occupants, and their\\ncharacteristics. You will need to consider each clue carefully,\\nunderstanding how it relates to the others and how it narrows down the\\npossibilities. Your focus should be on ensuring the logical consistency\\nof the solution, making sure that all clues are satisfied without\\ncontradiction.\\nReason: From clue 6 and 9, we know that the person who likes blue lives\\nin the fifth house and the person who likes white lives in the fourth\\nhouse. From clue 4, the person who likes red lives somewhere to the right\\nof the person who likes yellow. Since the fourth and fifth houses are\\noccupied by the people who like white and blue, the person who likes red\\nmust live in the third house and the person who likes yellow must live in\\nthe first or second house.From clue 10, there is one house between where\\nthe person who likes green lives and where the person who drives a truck\\nlives. Since the third, fourth, and fifth houses are occupied by the\\npeople who like red, white, and blue, the person who likes green must\\nlive in the first or second house. Therefore, the person who likes yellow\\nmust live in the first house and the person who likes green must live in\\nthe second house.From clue 11, the person who owns an SUV does not live\\nin the first house. So, the person who owns an SUV must live in the\\nsecond, third, or fourth house.From clue 2, the person who drives a truck\\ndoes not live in the fourth house. So, the person who drives a truck\\nmust live in the second or third house.From clue 3, the person who drives\\na truck lives directly left of the person who is wearing sandals. So,\\nthe person who drives a truck must live in the second house and the\\nperson who is wearing sandals must live in the third house.From clue 8,\\nthe person who is wearing boots lives somewhere to the left of the person\\nwho drives a truck. So, the person who is wearing boots must live in the\\nfirst house.\\nAnswer: choice: 1\\n21\\nListing 6: An example of multi-agent generation from EVOAGENT for Trivia Creative Writing\\nWrite a short and coherent story about Zootopia that incorporates the\\nanswers to the following 5 questions:\\n1. Following success at the 1979 election whose party conference speeech\\nincluded the lines ’you turn if you want to, the lady’s not for turning’?\\n2. Which snooker player was simply known as ’The Grinder’?\\n3. Which former West Indian fast bowler was known as ’Whispering Death’?\\n4. Which Eastenders character had a dog called Willy?\\n5. In which year did the first episode of Minder take place?\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a sports historian with a deep understanding of snooker and\\ncricket from the 1970s and 1980s. You are familiar with the careers and\\nplaying styles of Cliff Thorburn, known as ’The Grinder’, and Michael\\nHolding, known as ’Whispering Death’. Your role in this task will be to\\nprovide context and insight into these sports figures and their\\nsignificance in the story about Zootopia.\\nExpert Agent #2 Description:\\nYou are a political historian with a deep understanding of the political\\nclimate and key figures of the 1970s and 1980s, particularly in the\\nUnited Kingdom. You have extensive knowledge about Margaret Thatcher, her\\npolicies, and her impact on society. Your role in this task will be to\\nprovide context and insight into the significance of the reference to\\nThatcher’s famous speech in the story about Zootopia.\\nStory: In the vibrant city of Zootopia, the year was 1979, a time of\\nsignificant change and excitement. The city had just elected its first\\nfemale mayor, a lioness named Margaret Thatcher, known for her unwavering\\ndetermination and strong leadership. Her victory speech at the party\\nconference echoed the famous line from the human world’s Margaret\\nThatcher, \"You turn if you want to, the lady’s not for turning.\" This\\nphrase resonated with the citizens of Zootopia, who admired her\\nsteadfastness and determination. In the heart of the city, a meticulous\\nbadger known as ’The Grinder’ ran a popular snooker club. His nickname\\nwas a tribute to the legendary human snooker player Cliff Thorburn, known\\nfor his relentless and meticulous style of play. The Grinder’s snooker\\nclub was a hub of social activity, where animals from all walks of life\\ncame to unwind and engage in friendly competition. Meanwhile, a new\\nrecruit had joined the Zootopia Police Department, a cheetah named\\nMichael Holding. Known as ’Whispering Death’ during his cricketing days,\\na moniker borrowed from the former West Indian fast bowler, he was now a\\nrespected figure in the community, using his speed and stealth to uphold\\nthe law. In another part of the city, a popular soap opera was being\\nfilmed. The show was called \"Eastenders of Zootopia,\" and one of the\\nbeloved characters was a rabbit named Ethel Skinner, who had a dog named\\nWilly. This was a charming reference to the human Eastenders character of\\nthe same name, symbolizing the enduring friendships that were a\\ncornerstone of Zootopian society. The year 1979 was also significant as\\nit marked the airing of the first episode of the Zootopian version of the\\nhuman show \"Minder\". The show was a reflection of the city’s vibrant and\\ndiverse culture, much like the original British series. It was a\\ntestament to the city’s ability to adapt and evolve, while still holding\\nonto its rich history and traditions. In conclusion, Zootopia was a\\nmelting pot of cultures and histories, much like the television shows and\\ncharacters it referenced. These references served as a bridge between\\nthe past and the present, reminding the citizens of their roots while\\nencouraging them to embrace the future.\\nEvaluation: correct_count=5/5\\n22\\nListing 7: An example of multi-agent generation from EVOAGENT for Codenames Collaborative\\nTry to identify the 3 words best associated with the word \"fruit\" from\\nthe following word list:\\n[\"bowl\", \"einstein\", \"banana\", \"brain\", \"judge\", \"groom\", \"crusader\", \"\\nstamp\", \"peach\", \"halloween\", \"troll\", \"slipper\", \"earthquake\", \"tip\", \"\\nsun\", \"quarter\", \"pad\", \"nude\", \"helmet\", \"laundry\", \"wedding\", \"mirror\",\\n\"sleep\", \"minotaur\", \"wish\"].\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a cultural anthropologist with a focus on symbolism and rituals.\\nYour expertise in understanding how different objects, including fruits,\\nare used and symbolized in various cultures will be beneficial in this\\ntask. You can provide insights into how certain words might be associated\\nwith \"fruit\" from a cultural or symbolic perspective. Your focus will be\\non the cultural and symbolic aspects of the task.\\nReason:\\n1. ’Banana’: This is a type of fruit, rich in potassium and dietary fiber\\n. It is commonly consumed raw and is used in a variety of dishes like\\ndesserts, smoothies, and baked goods.\\n2. ’Peach’: This is also a type of fruit, known for its sweet and juicy\\nnature. Peaches are rich in vitamins A and C and are often eaten raw or\\nused in dishes like pies, jams, and salads.\\n3. ’Bowl’: This word is associated with fruit in the context of a fruit\\nbowl, which is a common way to store and display various types of fruit.\\nA bowl of fruit can also be a healthy snack or dessert option.\\nAnswer: Banana, Peach, Bowl\\nExpert Agent #2 Description:\\nYou are a nutritionist with a deep understanding of different types of\\nfood, including fruits. Your knowledge about the nutritional value,\\ncommon uses, and cultural significance of various fruits will be helpful\\nin this task. You can provide insights into how certain words might be\\nassociated with \"fruit\" from a dietary or nutritional perspective. Your\\nfocus will be on the nutritional and dietary aspects of the task.\\nReason:\\n1. ’Banana’: This is a direct association as a banana is a type of fruit.\\nIn many cultures, bananas symbolize fertility and prosperity. In\\nHinduism, for example, bananas are considered a sacred fruit and are used\\nin religious rituals.\\n2. ’Peach’: Similar to banana, a peach is also a type of fruit. In\\nChinese culture, peaches are symbols of immortality and unity. They are\\noften used in traditional rituals and ceremonies.\\n3. ’Bowl’: A bowl is often used to hold fruits. In many cultures, a bowl\\nof fruit is a symbol of abundance, fertility, and prosperity. It is often\\nused in rituals and ceremonies as an offering or a centerpiece.\\nAnswer: banana, peach, bowl\\nFinal Answer:\\nAfter considering the expert’s input, I find that their reasoning aligns\\nwith my initial thoughts. The words ’banana’ and ’peach’ are indeed types\\nof fruit, and their nutritional value and common uses further strengthen\\ntheir association with the word ’fruit’. The word ’bowl’ is also\\nassociated with fruit in the context of a fruit bowl, which is a common\\nway to store and display various types of fruit. Therefore, I agree with\\nthe expert’s choices.\\nFinal Answer: banana, peach, bowl.\\n23\\nListing 8: An example of multi-agent generation from EVOAGENT for MMMU dataset.\\nQuestion: <image 1> The elliptical orbit of a planet around the Sun is\\nshown on the diagram. Which of the following statements is true?\\nA. the eccentricity of the orbit is less than zero\\nB. the eccentricity of the orbit is greater than 1\\nC. the sun might be at point C\\nD. the sun might be at point D\\nE. the sun might be at point B (Correct)\\nCoT:\\nReason: In an elliptical orbit, the Sun is located at one of the two foci\\nof the ellipse. The eccentricity of an orbit is a measure of how much\\nthe orbit deviates from a perfect circle, with 0 being a perfect circle\\nand 1 being a parabola. Since the orbit shown is an ellipse, the\\neccentricity must be between 0 and 1. Points A and E are the farthest\\npoints from the center of the ellipse, so the Sun cannot be located at\\neither of those points. Point B is at the center of the ellipse, so the\\nSun cannot be located there either. This leaves points C and D as\\npossible locations for the Sun. However, since the Sun must be at one of\\nthe foci and point D is closer to the center of the ellipse than point C,\\nit is more likely that the Sun is located at point C.\\nAnswer: C. the sun might be at point C\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a mathematician with a specialization in geometry, particularly\\nin the study of conic sections which include ellipses. Your expertise\\nallows you to understand the properties and definitions of ellipses,\\nincluding the concept of foci. You can provide a mathematical perspective\\non why the Sun must be located at one of the foci of the elliptical\\norbit and not at any other point. Your knowledge will help explain the\\ngeometric reasons behind the positioning of the Sun in relation to the\\nplanet’s orbit, and why the given options in the statements do not align\\nwith the mathematical definition of an ellipse.\\nReason: An ellipse is defined as the set of all points for which the sum\\nof the distances to two fixed points, called foci, is constant. The Sun,\\nbeing one of the massive bodies in the solar system, must be located at\\none of the foci of the elliptical orbit of a planet due to the\\ngravitational forces that govern the motion of the planets. Option E is\\ncorrect because the Sun might be at point B, which is one of the foci of\\nthe ellipse. The other focus would be at a point symmetrically opposite\\nto B along the major axis.\\nAnswer: E. the sun might be at point B.\\nExpert Agent #2 Description:\\nYou are an astronomer with a specialization in observational astronomy.\\nYour expertise lies in the practical observation and data collection of\\ncelestial bodies, including the use of telescopes and other instruments\\nto study the positions and movements of planets and stars. You have a\\ndeep understanding of how the apparent motion of celestial bodies can be\\nused to infer their actual motion in space. You can provide insight into\\nhow observations of planetary orbits, such as the one depicted in the\\ndiagram, have historically been used to deduce the laws of planetary\\nmotion and the positions of stars like the Sun. Your knowledge will help\\nexplain how practical observations support the theoretical understanding\\nthat the Sun must be located at one of the foci of the elliptical orbit.\\nReason: The eccentricity of an orbit is a measure of how much it deviates\\nfrom being a perfect circle. It ranges from 0 (a perfect circle) to 1 (a\\nparabola). An eccentricity less than zero or greater than 1 is not\\npossible for an elliptical orbit. Therefore, options A and B are\\nincorrect. According to Kepler’s laws of planetary motion, the Sun must\\nbe located at one of the foci of the elliptical orbit. Point B is located\\nat a position that could be one of the foci of the ellipse.\\nAnswer: E. the sun might be at point B.\\n24\\nListing 9: An example of multi-agent generation from EVOAGENT for ScienceWorld.\\nTask 1: Your task is to determine if a metal fork is electrically\\nconductive. The metal fork is located around the kitchen. First, focus on\\nthe metal fork. If conductive, place it in the blue box. If\\nnonconductive, place it in the green box.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a thermodynamics expert. Your expertise in heat transfer and\\ntemperature measurement is crucial for this task. You understand how to\\nproperly use a thermometer and interpret its readings. You also have\\nknowledge about the properties of various substances, including their\\ntypical temperature ranges. Your task is to guide the assistant in using\\nthe thermometer to accurately measure the temperature of the unknown\\nsubstance B.\\nTask 2: Your task is to boil gallium. For compounds without a boiling\\npoint, combusting the substance is also acceptable. First, focus on the\\nsubstance. Then, take actions that will cause it to change its state of\\nmatter.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a metallurgist with a deep understanding of the properties and\\nbehaviors of metals, including gallium. Your expertise includes knowledge\\nof the melting and boiling points of various metals, as well as the\\nprocesses and equipment needed to heat them to these temperatures. You\\nare also familiar with safety procedures and precautions when handling\\nand heating metals. Your task is to provide detailed instructions on how\\nto safely and effectively boil gallium in the foundry.\\nTask 3: Your task is to change the state of matter of gallium. First,\\nfocus on the substance. Then, take actions that will cause it to change\\nits state of matter.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a chemist with a specialization in the properties and behaviors\\nof substances. You have a deep understanding of the state changes of\\nmatter, specifically how substances like gallium can transition from\\nsolid to liquid or gas. You are familiar with the conditions required for\\nthese changes to occur, such as temperature and pressure. You also have\\nknowledge of the equipment and procedures used in a foundry, which will\\nbe useful in this task. Your role is to provide guidance on how to\\nmanipulate the conditions in the foundry to achieve the desired state\\nchange in gallium.\\n25\\nListing 10: An example of multi-agent generation from EVOAGENT for TravelPlanner dataset.\\nQuestion: Can you help me with a travel plan departing from Buffalo to\\nAtlanta for a duration of 3 days, specifically from March 2nd to March 4\\nth, 2022? I plan to travel alone and my planned budget for the trip is\\naround $1,100.\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a culinary expert with a deep understanding of food culture in\\nAtlanta. Your expertise includes knowledge of the best local restaurants,\\npopular dishes, and dietary restrictions. You can provide\\nrecommendations for breakfast, lunch, and dinner spots that offer a\\nunique taste of Atlanta’s food scene. Your suggestions should consider\\nthe traveler’s budget and the proximity of the restaurants to the\\nattractions and accommodation listed in the plan.\\nExpert Agent #2 Description:\\nYou are a travel logistics expert with extensive knowledge of\\ntransportation options and costs between Buffalo and Atlanta. Your\\nexpertise includes understanding flight schedules, costs, and durations,\\nas well as alternative transportation methods such as self-driving and\\ntaxi services. You can provide the most cost-effective and time-efficient\\ntransportation options for the traveler, considering their budget and\\nthe travel dates provided in the plan.\\nExpert Agent #3 Description:\\nYou are a local tourism expert with comprehensive knowledge of Atlanta’s\\nattractions. Your expertise includes understanding the city’s popular\\ntourist spots, historical sites, museums, parks, and unique local\\nexperiences. You can provide recommendations for attractions that the\\ntraveler can visit during their stay, considering their budget and the\\nproximity of these attractions to the traveler’s accommodation and meal\\nspots. Your suggestions should also take into account the operating hours\\nof these attractions to ensure they align with the traveler’s schedule.\\nAnswer:\\nDay 1:\\nCurrent City: from Buffalo to Atlanta\\nTransportation: Flight Number: F3502691, from Buffalo to Atlanta,\\nDeparture Time: 18:48, Arrival Time: 20:51\\nBreakfast: Not applicable as the flight is in the evening.\\nAttraction: Not applicable as the flight is in the evening.\\nLunch: Not applicable as the flight is in the evening.\\nDinner: Saffron, Atlanta\\nccommodation: Spacious private room close St. Barnabas Hospital, Atlanta\\nDay 2:\\nCurrent City: Atlanta\\nTransportation: -\\nBreakfast: Adda, Atlanta\\nAttraction: Atlanta Botanical Garden, Atlanta; World of Coca-Cola,\\nAtlanta.\\nLunch: Baba Au Rhum, Atlanta\\nDinner: Asian Bistro, Atlanta\\nAccommodation: Spacious private room close St. Barnabas Hospital, Atlanta\\nDay 3:\\nCurrent City: from Atlanta to Buffalo\\nTransportation: Flight Number: F3500648, from Atlanta to Buffalo,\\nDeparture Time: 21:24, Arrival Time: 23:26\\nBreakfast: Chef Style, Atlanta\\nAttraction: Georgia Aquarium, Atlanta; Martin Luther King, Jr. National\\nHistorical Park, Atlanta.\\nLunch: Pizza Central, Atlanta\\nDinner: Daawat-e-Kashmir, Atlanta\\nAccommodation: -.\\n26\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xuP-sz0JHtNv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document 객체의 내부 데이터를 딕셔너리로 변환하는 함수\n",
        "def document_to_dict(doc):\n",
        "    return {\n",
        "        'metadata': doc.metadata,\n",
        "        'page_content': doc.page_content,\n",
        "        'type': doc.type\n",
        "    }\n",
        "\n",
        "# 모든 Document 객체를 딕셔너리로 변환\n",
        "docs_dict = [document_to_dict(doc) for doc in docs]"
      ],
      "metadata": {
        "id": "e0oxURO4OPns"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.json_normalize(\n",
        "    docs_dict,\n",
        "    sep='_',\n",
        "    meta=[\n",
        "        'page_content', 'type',\n",
        "        ['metadata', 'Published'],\n",
        "        ['metadata', 'Title'],\n",
        "        ['metadata', 'Authors'],\n",
        "        ['metadata', 'Summary'],\n",
        "        ['metadata', 'entry_id'],\n",
        "        ['metadata', 'published_first_time'],\n",
        "        ['metadata', 'comment'],\n",
        "        ['metadata', 'journal_ref'],\n",
        "        ['metadata', 'doi'],\n",
        "        ['metadata', 'primary_category']\n",
        "    ],\n",
        "    record_path=['metadata', 'categories'],  # 'categories'가 리스트이므로 이를 따로 분리\n",
        "    errors='ignore'\n",
        ")\n",
        "df2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eJwcSEviH0qX",
        "outputId": "16999a4b-acba-4011-d8a9-a54022a484ab"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 0                                       page_content  \\\n",
              "0            cs.AI  EVOAGENT: Towards Automatic Multi-Agent\\nGener...   \n",
              "1            cs.LG  Cooperative Heterogeneous Deep Reinforcement\\n...   \n",
              "2            cs.AI  Cooperative Heterogeneous Deep Reinforcement\\n...   \n",
              "3            cs.HC  Improving of Robotic Virtual Agent’s errors th...   \n",
              "4          math.OC  arXiv:1405.1480v1  [math.OC]  7 May 2014\\nOn N...   \n",
              "5          math.DS  arXiv:1405.1480v1  [math.OC]  7 May 2014\\nOn N...   \n",
              "6            cs.LG  Cooperative and Competitive Biases\\nfor Multi-...   \n",
              "7            cs.AI  Cooperative and Competitive Biases\\nfor Multi-...   \n",
              "8            cs.MA  Cooperative and Competitive Biases\\nfor Multi-...   \n",
              "9            cs.AI  JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, ...   \n",
              "10           cs.MA  World of Computer Science and Information Tech...   \n",
              "11           cs.CY  World of Computer Science and Information Tech...   \n",
              "12           cs.HC  Author note:\\nThis is a preprint. The final ar...   \n",
              "13           cs.GR  Author note:\\nThis is a preprint. The final ar...   \n",
              "14           cs.RO  Author note:\\nThis is a preprint. The final ar...   \n",
              "15           cs.AI  Received: Added at production\\nRevised: Added ...   \n",
              "16  physics.soc-ph   \\nAgents Play Mix-game \\n \\n \\n \\nChengling G...   \n",
              "17        q-fin.TR   \\nAgents Play Mix-game \\n \\n \\n \\nChengling G...   \n",
              "\n",
              "        type metadata_Published  \\\n",
              "0   Document         2024-07-11   \n",
              "1   Document         2020-11-02   \n",
              "2   Document         2020-11-02   \n",
              "3   Document         2023-08-21   \n",
              "4   Document         2014-05-07   \n",
              "5   Document         2014-05-07   \n",
              "6   Document         2021-01-18   \n",
              "7   Document         2021-01-18   \n",
              "8   Document         2021-01-18   \n",
              "9   Document         2023-10-18   \n",
              "10  Document         2015-12-07   \n",
              "11  Document         2015-12-07   \n",
              "12  Document         2021-10-07   \n",
              "13  Document         2021-10-07   \n",
              "14  Document         2021-10-07   \n",
              "15  Document         2024-03-05   \n",
              "16  Document         2005-05-17   \n",
              "17  Document         2005-05-17   \n",
              "\n",
              "                                       metadata_Title  \\\n",
              "0   EvoAgent: Towards Automatic Multi-Agent Genera...   \n",
              "1   Cooperative Heterogeneous Deep Reinforcement L...   \n",
              "2   Cooperative Heterogeneous Deep Reinforcement L...   \n",
              "3   Improving of Robotic Virtual Agent's errors th...   \n",
              "4          On Networks with Active and Passive Agents   \n",
              "5          On Networks with Active and Passive Agents   \n",
              "6   Cooperative and Competitive Biases for Multi-A...   \n",
              "7   Cooperative and Competitive Biases for Multi-A...   \n",
              "8   Cooperative and Competitive Biases for Multi-A...   \n",
              "9   Fact-based Agent modeling for Multi-Agent Rein...   \n",
              "10  Software Agents Interaction Algorithms in Virt...   \n",
              "11  Software Agents Interaction Algorithms in Virt...   \n",
              "12                                         Appearance   \n",
              "13                                         Appearance   \n",
              "14                                         Appearance   \n",
              "15  Partially Observable Mean Field Multi-Agent Re...   \n",
              "16                               Agents Play Mix-game   \n",
              "17                               Agents Play Mix-game   \n",
              "\n",
              "                                     metadata_Authors  \\\n",
              "0   Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan,...   \n",
              "1   Han Zheng, Pengfei Wei, Jing Jiang, Guodong Lo...   \n",
              "2   Han Zheng, Pengfei Wei, Jing Jiang, Guodong Lo...   \n",
              "3                      Takahiro Tsumura, Seiji Yamada   \n",
              "4                                      Tansel Yucelen   \n",
              "5                                      Tansel Yucelen   \n",
              "6             Heechang Ryu, Hayong Shin, Jinkyoo Park   \n",
              "7             Heechang Ryu, Hayong Shin, Jinkyoo Park   \n",
              "8             Heechang Ryu, Hayong Shin, Jinkyoo Park   \n",
              "9                 Baofu Fang, Caiming Zheng, Hao Wang   \n",
              "10                              Zahi A. M. Abu Sarhan   \n",
              "11                              Zahi A. M. Abu Sarhan   \n",
              "12                      Rachel McDonnell, Bilge Mutlu   \n",
              "13                      Rachel McDonnell, Bilge Mutlu   \n",
              "14                      Rachel McDonnell, Bilge Mutlu   \n",
              "15                 Min Yang, Guanjun Liu, Ziyuan Zhou   \n",
              "16                                      Chengling Gou   \n",
              "17                                      Chengling Gou   \n",
              "\n",
              "                                     metadata_Summary  \\\n",
              "0   The rise of powerful large language models (LL...   \n",
              "1   Numerous deep reinforcement learning agents ha...   \n",
              "2   Numerous deep reinforcement learning agents ha...   \n",
              "3   One way to improve the relationship between hu...   \n",
              "4   We introduce an active-passive networked multi...   \n",
              "5   We introduce an active-passive networked multi...   \n",
              "6   Training a multi-agent reinforcement learning ...   \n",
              "7   Training a multi-agent reinforcement learning ...   \n",
              "8   Training a multi-agent reinforcement learning ...   \n",
              "9   In multi-agent systems, agents need to interac...   \n",
              "10  This paper highlights the multi-agent learning...   \n",
              "11  This paper highlights the multi-agent learning...   \n",
              "12  Socially interactive agents (SIAs) are no long...   \n",
              "13  Socially interactive agents (SIAs) are no long...   \n",
              "14  Socially interactive agents (SIAs) are no long...   \n",
              "15  Traditional multi-agent reinforcement learning...   \n",
              "16  In mix-game which is an extension of minority ...   \n",
              "17  In mix-game which is an extension of minority ...   \n",
              "\n",
              "                         metadata_entry_id metadata_published_first_time  \\\n",
              "0        http://arxiv.org/abs/2406.14228v2                    2024-06-20   \n",
              "1        http://arxiv.org/abs/2011.00791v1                    2020-11-02   \n",
              "2        http://arxiv.org/abs/2011.00791v1                    2020-11-02   \n",
              "3        http://arxiv.org/abs/2304.00247v2                    2023-04-01   \n",
              "4         http://arxiv.org/abs/1405.1480v1                    2014-05-07   \n",
              "5         http://arxiv.org/abs/1405.1480v1                    2014-05-07   \n",
              "6        http://arxiv.org/abs/2101.06890v1                    2021-01-18   \n",
              "7        http://arxiv.org/abs/2101.06890v1                    2021-01-18   \n",
              "8        http://arxiv.org/abs/2101.06890v1                    2021-01-18   \n",
              "9        http://arxiv.org/abs/2310.12290v1                    2023-10-18   \n",
              "10        http://arxiv.org/abs/1403.5734v2                    2014-03-23   \n",
              "11        http://arxiv.org/abs/1403.5734v2                    2014-03-23   \n",
              "12       http://arxiv.org/abs/2110.04083v1                    2021-10-07   \n",
              "13       http://arxiv.org/abs/2110.04083v1                    2021-10-07   \n",
              "14       http://arxiv.org/abs/2110.04083v1                    2021-10-07   \n",
              "15       http://arxiv.org/abs/2304.12653v2                    2023-04-25   \n",
              "16  http://arxiv.org/abs/physics/0505112v1                    2005-05-17   \n",
              "17  http://arxiv.org/abs/physics/0505112v1                    2005-05-17   \n",
              "\n",
              "                                     metadata_comment  \\\n",
              "0                                     Work in process   \n",
              "1                                                None   \n",
              "2                                                None   \n",
              "3   13 pages, 4 figures, 5 tables, submitted ICSR2...   \n",
              "4                                                None   \n",
              "5                                                None   \n",
              "6   Accepted as a full paper at the Twentieth Inte...   \n",
              "7   Accepted as a full paper at the Twentieth Inte...   \n",
              "8   Accepted as a full paper at the Twentieth Inte...   \n",
              "9                                                None   \n",
              "10                                               None   \n",
              "11                                               None   \n",
              "12  38 pages, 16 figures, appears in the \"The Hand...   \n",
              "13  38 pages, 16 figures, appears in the \"The Hand...   \n",
              "14  38 pages, 16 figures, appears in the \"The Hand...   \n",
              "15                                               None   \n",
              "16                       8 pages, 6 figures, 3 tables   \n",
              "17                       8 pages, 6 figures, 3 tables   \n",
              "\n",
              "                                 metadata_journal_ref  \\\n",
              "0                                                None   \n",
              "1                                                None   \n",
              "2                                                None   \n",
              "3                                                None   \n",
              "4                                                None   \n",
              "5                                                None   \n",
              "6                                                None   \n",
              "7                                                None   \n",
              "8                                                None   \n",
              "9                                                None   \n",
              "10  The World of Computer Science and Information ...   \n",
              "11  The World of Computer Science and Information ...   \n",
              "12  In B. Lugrin, C. Pelachaud, D. Traum (Eds.), T...   \n",
              "13  In B. Lugrin, C. Pelachaud, D. Traum (Eds.), T...   \n",
              "14  In B. Lugrin, C. Pelachaud, D. Traum (Eds.), T...   \n",
              "15                             Drones 2023, 7(7), 476   \n",
              "16                                               None   \n",
              "17                                               None   \n",
              "\n",
              "                    metadata_doi metadata_primary_category  \n",
              "0                           None                     cs.AI  \n",
              "1                           None                     cs.LG  \n",
              "2                           None                     cs.LG  \n",
              "3                           None                     cs.HC  \n",
              "4                           None                   math.OC  \n",
              "5                           None                   math.OC  \n",
              "6                           None                     cs.LG  \n",
              "7                           None                     cs.LG  \n",
              "8                           None                     cs.LG  \n",
              "9                           None                     cs.AI  \n",
              "10                          None                     cs.MA  \n",
              "11                          None                     cs.MA  \n",
              "12       10.1145/3477322.3477327                     cs.HC  \n",
              "13       10.1145/3477322.3477327                     cs.HC  \n",
              "14       10.1145/3477322.3477327                     cs.HC  \n",
              "15         10.3390/drones7070476                     cs.AI  \n",
              "16  10.1007/978-88-470-0502-0_12            physics.soc-ph  \n",
              "17  10.1007/978-88-470-0502-0_12            physics.soc-ph  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c4fa180-acc0-42a6-b28f-82752c0dde1b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>page_content</th>\n",
              "      <th>type</th>\n",
              "      <th>metadata_Published</th>\n",
              "      <th>metadata_Title</th>\n",
              "      <th>metadata_Authors</th>\n",
              "      <th>metadata_Summary</th>\n",
              "      <th>metadata_entry_id</th>\n",
              "      <th>metadata_published_first_time</th>\n",
              "      <th>metadata_comment</th>\n",
              "      <th>metadata_journal_ref</th>\n",
              "      <th>metadata_doi</th>\n",
              "      <th>metadata_primary_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cs.AI</td>\n",
              "      <td>EVOAGENT: Towards Automatic Multi-Agent\\nGener...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2024-07-11</td>\n",
              "      <td>EvoAgent: Towards Automatic Multi-Agent Genera...</td>\n",
              "      <td>Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan,...</td>\n",
              "      <td>The rise of powerful large language models (LL...</td>\n",
              "      <td>http://arxiv.org/abs/2406.14228v2</td>\n",
              "      <td>2024-06-20</td>\n",
              "      <td>Work in process</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.AI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cs.LG</td>\n",
              "      <td>Cooperative Heterogeneous Deep Reinforcement\\n...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2020-11-02</td>\n",
              "      <td>Cooperative Heterogeneous Deep Reinforcement L...</td>\n",
              "      <td>Han Zheng, Pengfei Wei, Jing Jiang, Guodong Lo...</td>\n",
              "      <td>Numerous deep reinforcement learning agents ha...</td>\n",
              "      <td>http://arxiv.org/abs/2011.00791v1</td>\n",
              "      <td>2020-11-02</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.LG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cs.AI</td>\n",
              "      <td>Cooperative Heterogeneous Deep Reinforcement\\n...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2020-11-02</td>\n",
              "      <td>Cooperative Heterogeneous Deep Reinforcement L...</td>\n",
              "      <td>Han Zheng, Pengfei Wei, Jing Jiang, Guodong Lo...</td>\n",
              "      <td>Numerous deep reinforcement learning agents ha...</td>\n",
              "      <td>http://arxiv.org/abs/2011.00791v1</td>\n",
              "      <td>2020-11-02</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.LG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cs.HC</td>\n",
              "      <td>Improving of Robotic Virtual Agent’s errors th...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2023-08-21</td>\n",
              "      <td>Improving of Robotic Virtual Agent's errors th...</td>\n",
              "      <td>Takahiro Tsumura, Seiji Yamada</td>\n",
              "      <td>One way to improve the relationship between hu...</td>\n",
              "      <td>http://arxiv.org/abs/2304.00247v2</td>\n",
              "      <td>2023-04-01</td>\n",
              "      <td>13 pages, 4 figures, 5 tables, submitted ICSR2...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.HC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>math.OC</td>\n",
              "      <td>arXiv:1405.1480v1  [math.OC]  7 May 2014\\nOn N...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2014-05-07</td>\n",
              "      <td>On Networks with Active and Passive Agents</td>\n",
              "      <td>Tansel Yucelen</td>\n",
              "      <td>We introduce an active-passive networked multi...</td>\n",
              "      <td>http://arxiv.org/abs/1405.1480v1</td>\n",
              "      <td>2014-05-07</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>math.OC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>math.DS</td>\n",
              "      <td>arXiv:1405.1480v1  [math.OC]  7 May 2014\\nOn N...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2014-05-07</td>\n",
              "      <td>On Networks with Active and Passive Agents</td>\n",
              "      <td>Tansel Yucelen</td>\n",
              "      <td>We introduce an active-passive networked multi...</td>\n",
              "      <td>http://arxiv.org/abs/1405.1480v1</td>\n",
              "      <td>2014-05-07</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>math.OC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>cs.LG</td>\n",
              "      <td>Cooperative and Competitive Biases\\nfor Multi-...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2021-01-18</td>\n",
              "      <td>Cooperative and Competitive Biases for Multi-A...</td>\n",
              "      <td>Heechang Ryu, Hayong Shin, Jinkyoo Park</td>\n",
              "      <td>Training a multi-agent reinforcement learning ...</td>\n",
              "      <td>http://arxiv.org/abs/2101.06890v1</td>\n",
              "      <td>2021-01-18</td>\n",
              "      <td>Accepted as a full paper at the Twentieth Inte...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.LG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>cs.AI</td>\n",
              "      <td>Cooperative and Competitive Biases\\nfor Multi-...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2021-01-18</td>\n",
              "      <td>Cooperative and Competitive Biases for Multi-A...</td>\n",
              "      <td>Heechang Ryu, Hayong Shin, Jinkyoo Park</td>\n",
              "      <td>Training a multi-agent reinforcement learning ...</td>\n",
              "      <td>http://arxiv.org/abs/2101.06890v1</td>\n",
              "      <td>2021-01-18</td>\n",
              "      <td>Accepted as a full paper at the Twentieth Inte...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.LG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>cs.MA</td>\n",
              "      <td>Cooperative and Competitive Biases\\nfor Multi-...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2021-01-18</td>\n",
              "      <td>Cooperative and Competitive Biases for Multi-A...</td>\n",
              "      <td>Heechang Ryu, Hayong Shin, Jinkyoo Park</td>\n",
              "      <td>Training a multi-agent reinforcement learning ...</td>\n",
              "      <td>http://arxiv.org/abs/2101.06890v1</td>\n",
              "      <td>2021-01-18</td>\n",
              "      <td>Accepted as a full paper at the Twentieth Inte...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.LG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>cs.AI</td>\n",
              "      <td>JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, ...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2023-10-18</td>\n",
              "      <td>Fact-based Agent modeling for Multi-Agent Rein...</td>\n",
              "      <td>Baofu Fang, Caiming Zheng, Hao Wang</td>\n",
              "      <td>In multi-agent systems, agents need to interac...</td>\n",
              "      <td>http://arxiv.org/abs/2310.12290v1</td>\n",
              "      <td>2023-10-18</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.AI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>cs.MA</td>\n",
              "      <td>World of Computer Science and Information Tech...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2015-12-07</td>\n",
              "      <td>Software Agents Interaction Algorithms in Virt...</td>\n",
              "      <td>Zahi A. M. Abu Sarhan</td>\n",
              "      <td>This paper highlights the multi-agent learning...</td>\n",
              "      <td>http://arxiv.org/abs/1403.5734v2</td>\n",
              "      <td>2014-03-23</td>\n",
              "      <td>None</td>\n",
              "      <td>The World of Computer Science and Information ...</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.MA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>cs.CY</td>\n",
              "      <td>World of Computer Science and Information Tech...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2015-12-07</td>\n",
              "      <td>Software Agents Interaction Algorithms in Virt...</td>\n",
              "      <td>Zahi A. M. Abu Sarhan</td>\n",
              "      <td>This paper highlights the multi-agent learning...</td>\n",
              "      <td>http://arxiv.org/abs/1403.5734v2</td>\n",
              "      <td>2014-03-23</td>\n",
              "      <td>None</td>\n",
              "      <td>The World of Computer Science and Information ...</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.MA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>cs.HC</td>\n",
              "      <td>Author note:\\nThis is a preprint. The final ar...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2021-10-07</td>\n",
              "      <td>Appearance</td>\n",
              "      <td>Rachel McDonnell, Bilge Mutlu</td>\n",
              "      <td>Socially interactive agents (SIAs) are no long...</td>\n",
              "      <td>http://arxiv.org/abs/2110.04083v1</td>\n",
              "      <td>2021-10-07</td>\n",
              "      <td>38 pages, 16 figures, appears in the \"The Hand...</td>\n",
              "      <td>In B. Lugrin, C. Pelachaud, D. Traum (Eds.), T...</td>\n",
              "      <td>10.1145/3477322.3477327</td>\n",
              "      <td>cs.HC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>cs.GR</td>\n",
              "      <td>Author note:\\nThis is a preprint. The final ar...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2021-10-07</td>\n",
              "      <td>Appearance</td>\n",
              "      <td>Rachel McDonnell, Bilge Mutlu</td>\n",
              "      <td>Socially interactive agents (SIAs) are no long...</td>\n",
              "      <td>http://arxiv.org/abs/2110.04083v1</td>\n",
              "      <td>2021-10-07</td>\n",
              "      <td>38 pages, 16 figures, appears in the \"The Hand...</td>\n",
              "      <td>In B. Lugrin, C. Pelachaud, D. Traum (Eds.), T...</td>\n",
              "      <td>10.1145/3477322.3477327</td>\n",
              "      <td>cs.HC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>cs.RO</td>\n",
              "      <td>Author note:\\nThis is a preprint. The final ar...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2021-10-07</td>\n",
              "      <td>Appearance</td>\n",
              "      <td>Rachel McDonnell, Bilge Mutlu</td>\n",
              "      <td>Socially interactive agents (SIAs) are no long...</td>\n",
              "      <td>http://arxiv.org/abs/2110.04083v1</td>\n",
              "      <td>2021-10-07</td>\n",
              "      <td>38 pages, 16 figures, appears in the \"The Hand...</td>\n",
              "      <td>In B. Lugrin, C. Pelachaud, D. Traum (Eds.), T...</td>\n",
              "      <td>10.1145/3477322.3477327</td>\n",
              "      <td>cs.HC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>cs.AI</td>\n",
              "      <td>Received: Added at production\\nRevised: Added ...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2024-03-05</td>\n",
              "      <td>Partially Observable Mean Field Multi-Agent Re...</td>\n",
              "      <td>Min Yang, Guanjun Liu, Ziyuan Zhou</td>\n",
              "      <td>Traditional multi-agent reinforcement learning...</td>\n",
              "      <td>http://arxiv.org/abs/2304.12653v2</td>\n",
              "      <td>2023-04-25</td>\n",
              "      <td>None</td>\n",
              "      <td>Drones 2023, 7(7), 476</td>\n",
              "      <td>10.3390/drones7070476</td>\n",
              "      <td>cs.AI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>physics.soc-ph</td>\n",
              "      <td>\\nAgents Play Mix-game \\n \\n \\n \\nChengling G...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2005-05-17</td>\n",
              "      <td>Agents Play Mix-game</td>\n",
              "      <td>Chengling Gou</td>\n",
              "      <td>In mix-game which is an extension of minority ...</td>\n",
              "      <td>http://arxiv.org/abs/physics/0505112v1</td>\n",
              "      <td>2005-05-17</td>\n",
              "      <td>8 pages, 6 figures, 3 tables</td>\n",
              "      <td>None</td>\n",
              "      <td>10.1007/978-88-470-0502-0_12</td>\n",
              "      <td>physics.soc-ph</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>q-fin.TR</td>\n",
              "      <td>\\nAgents Play Mix-game \\n \\n \\n \\nChengling G...</td>\n",
              "      <td>Document</td>\n",
              "      <td>2005-05-17</td>\n",
              "      <td>Agents Play Mix-game</td>\n",
              "      <td>Chengling Gou</td>\n",
              "      <td>In mix-game which is an extension of minority ...</td>\n",
              "      <td>http://arxiv.org/abs/physics/0505112v1</td>\n",
              "      <td>2005-05-17</td>\n",
              "      <td>8 pages, 6 figures, 3 tables</td>\n",
              "      <td>None</td>\n",
              "      <td>10.1007/978-88-470-0502-0_12</td>\n",
              "      <td>physics.soc-ph</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c4fa180-acc0-42a6-b28f-82752c0dde1b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c4fa180-acc0-42a6-b28f-82752c0dde1b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c4fa180-acc0-42a6-b28f-82752c0dde1b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-21314424-8fa7-40e5-89ca-81e97d628f97\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-21314424-8fa7-40e5-89ca-81e97d628f97')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-21314424-8fa7-40e5-89ca-81e97d628f97 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3a71a85b-536f-4feb-ab8e-2b75e1f30c57\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df2')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3a71a85b-536f-4feb-ab8e-2b75e1f30c57 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df2');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df2",
              "summary": "{\n  \"name\": \"df2\",\n  \"rows\": 18,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"cs.MA\",\n          \"cs.AI\",\n          \"physics.soc-ph\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Received: Added at production\\nRevised: Added at production\\nAccepted: Added at production\\nDOI: https://doi.org/10.3390/drones7070476\\nARTICLE TYPE\\nPartially Observable Mean Field Multi-Agent Reinforcement\\nLearning Based on Graph\\u2013Attention\\nMin Yang | Guanjun Liu* | Ziyuan Zhou\\n1Department of Computer Science, Tongji\\nUniversity, Shanghai, China\\nSummary\\nTraditional multi-agent reinforcement learning algorithms are difficultly applied in\\na large-scale multi-agent environment. The introduction of mean field theory has\\nenhanced the scalability of multi-agent reinforcement learning in recent years. This\\npaper considers partially observable multi-agent reinforcement learning (MARL),\\nwhere each agent can only observe other agents within a fixed range. This partial\\nobservability affects the agent\\u2019s ability to assess the quality of the actions of sur-\\nrounding agents. This paper focuses on developing a method to capture more effective\\ninformation from local observations in order to select more effective actions. Pre-\\nvious work in this field employs probability distributions or weighted mean field\\nto update the average actions of neighborhood agents, but it does not fully con-\\nsider the feature information of surrounding neighbors and leads to a local optimum.\\nIn this paper, we propose a novel multi-agent reinforcement learning algorithm,\\nPartially Observable Mean Field Multi-Agent Reinforcement Learning based on\\nGraph\\u2013Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention\\nmodule and a mean field module to describe how an agent is influenced by the\\nactions of other agents at each time step. This graph attention module consists of a\\ngraph attention encoder and a differentiable attention mechanism, and this mecha-\\nnism outputs a dynamic graph to represent the effectiveness of neighborhood agents\\nagainst central agents. The mean\\u2013field module approximates the effect of a neigh-\\nborhood agent on a central agent as the average effect of effective neighborhood\\nagents. We evaluate GAMFQ on three challenging tasks in the MAgents framework.\\nExperiments show that GAMFQ outperforms baselines including the state-of-the-art\\npartially observable mean-field reinforcement learning algorithms.\\nsectionAbstract\\nKEYWORDS:\\nGraph\\u2013Attention, Multi-agent reinforcement learning, Mean field theory, Partial observation\\n1\\nINTRODUCTION\\nReinforcement learning has been widely used in video games [26] and recently in education [7]. For multi-agent reinforce-\\nment learning (MARL) [33], it involves multiple autonomous agents that make autonomous decisions to accomplish some\\nspecific competitive or cooperative tasks by maximizing global reward, it has been applied in some real-world scenarios such as\\narXiv:2304.12653v2  [cs.AI]  5 Mar 2024\\n2\\nautonomous mobile [21] drone swarm confrontation[1] and multi-UAV collaboratively delivering goods [22]. For example, in\\nsome of the drone swarm adversarial tasks, drones need to make actions based on autonomous decisions. Due to the inevitable\\ndeath of some drones in the confrontation environment [38], the surviving drones must constantly evolve their strategies in\\nreal-time during the interaction with the environment to obtain the overall maximum reward. In order to make better interaction\\namong agents, it is required that each agent in the multi-agent system can effectively perceive environmental information and\\nfully acquire the information of surrounding agents.\\nHowever, the global communication cost among multiple agents is high, and in many practical tasks, each agent only observes\\npart of the environmental information. Take the task of Autonomous Driving as an example, each vehicle makes decision\\nin the limited observation space which is a typical local observation scene. Each agent can only rely on limited observation\\ninformation in the local observation environment, therefore the agent needs to learn a decentralized strategy. There are two\\ncommon decentralization strategies. One is Centralized Training and Decentralized Execution (CTDE), which requires agents\\nto communicate with each other during training and to independently make decisions based on their own observations during\\ntesting in order to adapt to large-scale multi-agent environments. Some classic algorithms using the CTDE framework such\\nas MADDPG [15], QMIX [19] and MAVEN [16]. Another one takes the policy of decentralized training and decentralized\\nexecution, in which each agent can only observe part of the information during the training and testing phases, which is closer\\nto the real environment with limited communication. Especially large-scale multi-agent environments are complex and non-\\nstationary [10], it is difficult for agents to observe the entire environment globally, limiting their ability to find the best actions.\\nFurthermore, as the number of agents increases, joint optimization of all information in a multi-agent environment may result\\nin a huge joint state-action space, which also brings scalability challenges. This paper focuses on the second strategy.\\nTraditional multi-agent reinforcement learning algorithms are difficult to be applied in large-scale multi-agent environments,\\nespecially when the number of agents is exponential. Recent studies address the scalability issues of multi-agent reinforcement\\nlearning [31, 30, 12] by introducing mean-field theory, i.e., the multi-agent problem is reduced to a simple two-agent problem.\\nHowever, Yang et al. [31] assumes that each agent can observe global information, which is difficult to apply in some real tasks.\\nTherefore, it is necessary to study large-scale multi-agent reinforcement learning algorithms in partially observable cases [3].\\nIn addition, researchers have intensively studied mean-field-based multi-agent reinforcement learning algorithms to improve\\nperformance in partially observable cases. One way is to further decompose the Q-function of the mean field-based multi-\\nagent reinforcement learning algorithm [34, 6]. Another way uses probability distribution or weighted mean field to update the\\nmean action of neighborhood agents [5, 37, 23, 28]. Hao [8] combined the graph attention with the mean field to calculate the\\ninteraction strength between agents when agents interact, but only considered the scene where the agent has a fixed relative\\nposition, and the agents can observe the global information. The difference is that when the agent is partially observable, we\\nconsider the dynamic change of the agent\\u2019s position and the death scene of the agent, and construct a more flexible partial\\nobservable graph attention network based on the mean field.\\nHowever, for partially observable multi-agent mean field reinforcement learning, the existing methods do not fully consider\\nthe feature information of the surrounding neighbors, which will lead to falling into local optimum. This paper focuses on\\nidentifying the neighborhood agents that may have the greater influence on the central agent in a limited observation space,\\nin order to avoid the local optimum issue. Since the graph neural network [29] can fully aggregate the relationship between\\nthe central agent and its surrounding neighbors, we propose a graph attention-based mechanism to calculate the importance of\\nneighbor agents to estimate the average action more efficiently.\\nThe main contributions of this paper are as follows:\\n\\u2022 We propose a partially observable mean\\u2013field reinforcement learning based on the graph\\u2013attention (GAMFQ), which can\\nlearn a decentralized agent policy from an environment without requiring global information of an environment. In the\\ncase of partially observable large-scale agents, the judgment of the importance of neighbor agents is insufficient in our\\nGAMFQ.\\n\\u2022 We theoretically demonstrate that the settings of the GAMFQ algorithm are close to Nash equilibrium.\\n\\u2022 Experiments on three challenging tasks in the MAgents framework show that GAMFQ outperforms two baseline\\nalgorithms as well as the state-of-the-art partially observable mean-field reinforcement learning algorithms.\\n3\\n2\\nRELATED WORK\\nMost of the recent MARL algorithms for partial observability research are model-free reinforcement learning algorithms based\\non the CTDE framework. The most classic algorithm MADDPG [15] introduces critics that can observe global information\\nin training to guide actor training, but only use actors with local observation information to take actions in testing. QMIX[19]\\nuses a hybrid network to combine the local value functions of a single agent, and adds global state information assistance in the\\ntraining and learning process to improve the performance of the algorithm. MAVEN [16] is able to solve complex multi-agent\\ntasks by introducing latent spaces for hierarchical control by value-mixing and policy-based approaches. However, these multi-\\nagent reinforcement learning algorithm using the CTDE framework is difficult to scale to large-scale multi-agent environments,\\nbecause there will be hard-to-observe global information that prevents the agents from training better policies.\\nFor large-scale multi-agent environments, Yang et al. [31] introduced the mean\\u2013field theory, which approximates the inter-\\naction of many agents as the interaction between the central agent and the average effects from neighboring agents. However,\\npartially observed multi-agent mean\\u2013field reinforcement learning algorithms still have a space to improve. Some researchers\\nfurther decompose the Q-function of the mean field based multi-agent reinforcement learning algorithm. Zhang et al. [34]\\ntrained agents through the CTDE paradigm, transforming each agent\\u2019s Q-function into its local Q-function and its mean field\\nQ-function, but this approach is not strictly partially observable. Gu et al. [6] proposes a mean field multi-agent reinforcement\\nlearning algorithm with local training and decentralized execution. The Q-function is decomposed by grouping the observable\\nneighbor states of each agent in a multi-agent system, so that the Q-function can be updated locally. In addition, some researchers\\nhave focused on improving the mean action in mean field reinforcement learning. Fang et al. [5] adds the idea of mean field to\\nMADDPG, and proposes a multi-agent reinforcement learning algorithm based on weighted mean field, so that MADDPG can\\nadapt to large-scale multi-agent environment. Wang et al. [28] propose a weighted mean-field multi-agent reinforcement learn-\\ning algorithm based on reward attribution decomposition by approximating the weighted mean field as a joint optimization of\\nimplicit reward distribution between a central agent and its neighbors. Zhou et al. [37] uses the average action of neighbor agents\\nas a label, and trained a mean field prediction network to replace the average action. Subramanian et al. [23] proposed two multi-\\nagent mean field reinforcement learning algorithms based on partially observable settings: POMFQ(FOR) and POMFQ(PDO),\\nextracting partial samples from Dirichlet or Gamma distribution to estimate partial observable mean action. Although these\\nmethods achieve good results, they do not fully consider the feature information of surrounding neighbors.\\nGraph Neural Networks (GNNs) are able to mine graph structures from data for learning. In multi-agent reinforcement learn-\\ning, GNNs can be used to model interactions between agents. In recent work, graph attention mechanisms have been used for\\nmulti-agent reinforcement learning. Zhang et al. [32] integrated the importance of the information of surrounding agents based\\non the multi-head attention mechanism, effectively integrate the key information of the graph to represent the environment and\\nimprove the cooperation strategy of agents with the help of multi-agent reinforcement learning. DCG [2] decomposed the joint\\nvalue function of all agents into gains between pairs of agents according to the coordination graph, which can flexibly balance\\nthe performance and generalization ability of agents. Li et al. [13] proposed a deep implicit coordination graph (DICG) struc-\\nture that can adapt to dynamic environments and learn implicit reasoning about joint actions or values through graph neural\\nnetworks. Ruan et al. [20] proposed a graph-based coordination strategy, which decomposes the joint team strategy into a graph\\ngenerator and a graph-based coordination strategy to realize the coordination behavior between agents. MAGIC [17] more accu-\\nrately represented the interactions between agents during communication by modifying the standard graph attention network\\nand compatible with differentiable directed graphs.\\nIn the dynamic MARL system where competition and confrontation coexist, it is very difficult to directly apply the graph\\nneural network, because the agent will die, the graph structure of the constructed large-scale agent system has the problem of\\nlarge spatial dimension. However, graph neural networks can better mine the relationship between features, and the introduction\\nof mean-field theory can further improve the advantages of mean-field multi-agent reinforcement learning.\\nOur approach differs from related work above in that it uses a graph attention mechanism to select surrounding agents that\\nare more important to the central agent in a partially observable environment. GAMFQ uses a graph attention module and\\na mean field module to describe how an agent is influenced by the actions of other agents at each time step, where graph\\nattention consists of a graph attention encoder and a differentiable attention mechanism, and finally outputs a dynamic graph to\\nrepresent the effectiveness of the neighborhood agent to the central agent. The mean field module approximates the influence of\\na neighborhood agent on a central agent as the average influence of the effective neighborhood agents. Using these two modules\\ntogether is able to efficiently estimate the average action of surrounding agents in partially observable situations. GAMFQ does\\nnot require global information about the environment to learn decentralized agent policies from the environment.\\n4\\n3\\nMOTIVATION & PRELIMINARIES\\nIn this section, we represent discrete-time non-cooperative multi-agent task modeling as a stochastic game (SG). SG can be\\ndefined as a tuple < \\ud835\\udc46, \\ud835\\udc341, \\u2026 , \\ud835\\udc34\\ud835\\udc41, \\ud835\\udc5f1, \\u2026 , \\ud835\\udc5f\\ud835\\udc41, \\ud835\\udc5d, \\ud835\\udefe>, where \\ud835\\udc46represents the true state of the environment. Each agent \\ud835\\udc57\\u2208\\n{1, \\u2026 , \\ud835\\udc41} chooses an action at each time step \\ud835\\udc4e\\ud835\\udc57\\u2208\\ud835\\udc34\\ud835\\udc57. The reward function for agent \\ud835\\udc57is \\ud835\\udc5f\\ud835\\udc57\\u2236\\ud835\\udc46\\u00d7 \\ud835\\udc341 \\u00d7 \\u22ef\\u00d7 \\ud835\\udc34\\ud835\\udc41\\u2192\\ud835\\udc45.\\nState transitions are dynamically represented as \\ud835\\udc5d\\u2236\\ud835\\udc46\\u00d7 \\ud835\\udc341 \\u00d7 \\u22ef\\u00d7 \\ud835\\udc34\\ud835\\udc41\\u2192\\u03a9(\\ud835\\udc46) . \\ud835\\udefeis a constant representing the discount\\nfactor. It represents a stable state, and in this stable state, all agents will not deviate from the best strategy given to others. The\\ndisadvantage is that it cannot be applied to the coexistence of multiple agents. Yang et al. [31] introduced mean field theory, which\\napproximates the interaction of many agents as the interaction between the average effect of a central agent and neighboring\\nagents, and solves the scalability problem of SG.\\nThe Nash equilibrium of general and random games can be defined as a strategy tuple (\\ud835\\udf0b1\\n\\u2217, \\u22ef, \\ud835\\udf0b\\ud835\\udc41\\n\\u2217\\n), for all \\ud835\\udc60\\u2208\\ud835\\udc46and\\n\\u2200\\ud835\\udf0b\\ud835\\udc56\\u2208\\u03a0\\ud835\\udc56, there is \\ud835\\udc63\\ud835\\udc57(\\ud835\\udc60, \\ud835\\udf0b1\\n\\u2217, \\u22ef, \\ud835\\udf0b\\ud835\\udc56\\n\\u2217, \\u22ef, \\ud835\\udf0b\\ud835\\udc41\\n\\u2217\\n) \\u2265\\ud835\\udc63\\ud835\\udc57(\\ud835\\udc60, \\ud835\\udf0b1\\n\\u2217, \\u22ef, \\ud835\\udf0b\\ud835\\udc56, \\u22ef, \\ud835\\udf0b\\ud835\\udc41\\n\\u2217\\n). This shows that when all other agents are implementing\\ntheir equilibrium strategy, no one agent will deviate from this equilibrium strategy and receive a strictly higher reward. When all\\nagents follow the Nash equilibrium strategy, the Nash Q-function of agent \\ud835\\udc57is \\ud835\\udc44\\ud835\\udc57\\n\\u2217(\\ud835\\udc60, \\ud835\\udc4e). Partially observable stochastic games can\\ngenerate a partially observable Markov decision process (POMDP), we review the partially observable Markov decision (Dec-\\nPOMDP) in Section 3.1 and analyze the partially observable model from a theoretical perspective. Section 3.2 first introduces\\nthe globally observable mean-field multi-agent reinforcement learning, and then introduces the partially observable mean-field\\nreinforcement learning algorithm (POMFQ) based on the POMDP framework, and analyzes the existing part of the observable\\nin detail. The limitation of mean-field reinforcement learning POMFQ(FOR)[23] is that the feature information of surrounding\\nneighbors is not fully considered. In a partially observable setting, each agent \\ud835\\udc57observable neighborhood agent information\\n\\ud835\\udc5c\\ud835\\udc57can be used to better mine the relationship between features through a graph attention network. Introducing graph attention\\nnetworks into partially observable mean-field multi-agent reinforcement learning can further improve their performance, and\\nSection 3.3 briefly introduces graph attention networks.\\n3.1\\nPartially observable Markov decision process\\nWe mainly study partially observable Markov decisions (Dec-POMDP) [3, 18, 33]. The partially observable Markov decision\\nprocess of \\ud835\\udc5bagents can be represented as a tuple \\u27e8\\ud835\\udc41, \\ud835\\udc46, {\\ud835\\udc34\\ud835\\udc56}\\ud835\\udc5b\\n\\ud835\\udc56=1 , \\ud835\\udc47, \\ud835\\udc4d, \\ud835\\udc45, \\ud835\\udc42, \\ud835\\udefe\\u27e9, where \\ud835\\udc41= {1, \\u2026 , \\ud835\\udc5b} represents the set of\\nagents, \\ud835\\udc46represents the global state, \\ud835\\udc34\\ud835\\udc57represents the set of action spaces of the \\ud835\\udc57-th agent, \\ud835\\udc4drepresents the observation space\\nof the agents, and the agent \\ud835\\udc57receives observation \\ud835\\udc5c\\ud835\\udc57\\u2208\\ud835\\udc42\\ud835\\udc57through the observation function \\ud835\\udc4d(\\ud835\\udc60, \\ud835\\udc57) \\u2236\\ud835\\udc46\\u00d7 \\ud835\\udc41\\u2192\\ud835\\udc42, and the\\ntransition function \\ud835\\udc47\\u2236\\ud835\\udc46\\u00d7 \\ud835\\udc341 \\u00d7 \\u2026 \\u00d7 \\ud835\\udc34\\ud835\\udc5b\\u00d7 \\ud835\\udc46\\ue0b6\\u2192[0, 1] represents the environment transitions from a state to another one. At\\neach time step \\ud835\\udc61, the agent \\ud835\\udc57chooses an action \\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\u2208\\ud835\\udc34\\ud835\\udc57, gets a reward \\ud835\\udc5f\\ud835\\udc57\\n\\ud835\\udc61\\u2236\\ud835\\udc46\\u00d7 \\ud835\\udc34\\ud835\\udc57\\ue0b6\\u2192\\ud835\\udc45w.r.t. a state and an action. \\ud835\\udefe\\u2208[0, 1]\\nis a reward discount factor. Agent \\ud835\\udc57has a stochastic policy \\ud835\\udf0b\\ud835\\udc57conditioned on its observation \\ud835\\udc5c\\ud835\\udc57or action observation history\\n\\ud835\\udf0f\\ud835\\udc57\\u2208(\\ud835\\udc4d\\u00d7 \\ud835\\udc34\\ud835\\udc57), and according to the all agents\\u2019s joint policy \\ud835\\udf0b\\n\\u0394= [\\ud835\\udf0b1, \\u2026 , \\ud835\\udf0b\\ud835\\udc41], The value function of agent \\ud835\\udc57under the joint\\nstrategy \\ud835\\udf0bis the value function \\ud835\\udc63\\ud835\\udc57\\n\\ud835\\udf0b(\\ud835\\udc60) = \\u2211\\u221e\\n\\ud835\\udc61=0 \\ud835\\udefe\\ud835\\udc61\\ud835\\udc38\\ud835\\udf0b,\\ud835\\udc5d\\n[\\ud835\\udc5f\\ud835\\udc57\\n\\ud835\\udc61|\\ud835\\udc600 = \\ud835\\udc60] can be obtained, and then the Q-function can be formalized as\\n\\ud835\\udc44\\ud835\\udc57\\n\\ud835\\udf0b(\\ud835\\udc60, \\ud835\\udc4e) = \\ud835\\udc5f\\ud835\\udc57(\\ud835\\udc60, \\ud835\\udc4e) + \\ud835\\udefe\\ud835\\udc38\\ud835\\udc60\\u2032\\u223c\\ud835\\udc5d\\n[\\ud835\\udc63\\ud835\\udc57\\n\\ud835\\udf0b\\n(\\ud835\\udc60\\u2032)]. Our work is based on the POMDP framework.\\n3.2\\nPartially Observable Mean Field Reinforcement Learning\\nMean-field theory-based reinforcement learning algorithm [31] approximates interactions among multiple agents as two-agent\\ninteractions, where the second agent corresponds to the average effect of all other agents. Yang et al. [31] decomposes the\\nmulti-agent Q-function into pairwise interacting local Q-functions as follows:\\n\\ud835\\udc44\\ud835\\udc57\\n\\ud835\\udf0b(\\ud835\\udc60, \\ud835\\udc4e) = 1\\n\\ud835\\udc41\\ud835\\udc57\\n\\u2211\\n\\ud835\\udc58\\u2208\\ud835\\udc41(\\ud835\\udc57)\\n\\ud835\\udc44\\ud835\\udc57\\n\\ud835\\udf0b\\n(\\ud835\\udc60, \\ud835\\udc4e\\ud835\\udc57, \\ud835\\udc4e\\ud835\\udc58)\\n(1)\\nwhere \\ud835\\udc41\\ud835\\udc57is the index set of the neighbors of the agent \\ud835\\udc57and \\ud835\\udc4e\\ud835\\udc57represents the discrete action of the agent \\ud835\\udc57and is represented\\nby one-shot coding. Mean field Q-function is cyclically updated according to Eq.2-5:\\n\\ud835\\udc44\\ud835\\udc57\\n\\ud835\\udf0b\\n(\\ud835\\udc60\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61, \\u0304\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\n) = (1 \\u2212\\ud835\\udefc)\\ud835\\udc44\\ud835\\udc57\\n\\ud835\\udf0b\\n(\\ud835\\udc60\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61, \\u0304\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\n) + \\ud835\\udefc[\\ud835\\udc5f\\ud835\\udc57\\n\\ud835\\udc61+ \\ud835\\udefe\\ud835\\udc63\\ud835\\udc57(\\ud835\\udc60\\ud835\\udc61+1\\n)]\\n(2)\\n5\\nwhere\\n\\ud835\\udc63\\ud835\\udc57(\\ud835\\udc60\\ud835\\udc61+1\\n)=\\n\\u2211\\n\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61+1\\n\\ud835\\udf0b\\ud835\\udc57(\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61+1 \\u2223\\ud835\\udc60\\ud835\\udc61+1, \\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\n) \\ud835\\udc44\\ud835\\udc57\\n\\ud835\\udf0b\\n(\\ud835\\udc60\\ud835\\udc61+1, \\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61+1, \\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\n)\\n(3)\\n\\u0304\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61= 1\\n\\ud835\\udc41\\n\\u2211\\n\\ud835\\udc58\\u2260\\ud835\\udc57\\n\\ud835\\udc4e\\ud835\\udc58\\n\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc58\\n\\ud835\\udc61\\u223c\\ud835\\udf0b\\ud835\\udc58(\\u22c5\\u2223\\ud835\\udc60\\ud835\\udc61, \\u0304\\ud835\\udc4e\\ud835\\udc58\\n\\ud835\\udc61\\u22121\\n)\\n(4)\\n\\ud835\\udf0b\\ud835\\udc57(\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\u2223\\ud835\\udc60\\ud835\\udc61, \\u0304\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\u22121\\n)=\\nexp (\\u2212\\ud835\\udefd\\ud835\\udc44\\ud835\\udc57\\n\\ud835\\udf0b\\n(\\ud835\\udc60\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61, \\u0304\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\u22121\\n))\\n\\u2211\\n\\ud835\\udc4e\\ud835\\udc57\\u2032\\n\\ud835\\udc61\\u2208\\ud835\\udc34\\ud835\\udc57\\nexp\\n(\\n\\u2212\\ud835\\udefd\\ud835\\udc44\\ud835\\udc57\\n\\ud835\\udf0b\\n(\\n\\ud835\\udc60\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc57\\u2032\\n\\ud835\\udc61, \\u0304\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\u22121\\n))\\n(5)\\nwhere \\u0304\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61is the mean action of the neighborhood agent, \\ud835\\udc5f\\ud835\\udc57\\n\\ud835\\udc61is the reward for agent \\ud835\\udc57at time step \\ud835\\udc61, \\ud835\\udc63\\ud835\\udc57is the value function of agent\\n\\ud835\\udc57, and \\ud835\\udefdis the Boltzmann parameter. Literature [31] assumes that each agent has global information, and for the central agent,\\nthe average action of the neighboring agents is updated by Eq. 4. However, in a partially observable multi-agent environment,\\nthe way of calculating the average action in Eq. 4 is no longer applicable.\\nIn the case of partial observability, Subramanian et al. [23] take \\ud835\\udc48samples from the Dirichlet distribution to update the average\\naction of Eq. 4, and achieve better performance than the mean field reinforcement learning algorithm. The formula is as follows:\\n\\ud835\\udc37\\ud835\\udc57(\\ud835\\udf03) \\u221d\\ud835\\udf03\\ud835\\udf021\\u22121+\\ud835\\udc501\\n1\\n\\u22ef\\ud835\\udf03\\ud835\\udf02\\ud835\\udc3f\\u22121+\\ud835\\udc50\\ud835\\udc3f\\n\\ud835\\udc3f\\n;\\n\\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc56,\\ud835\\udc61\\u223c\\ud835\\udc37\\ud835\\udc57(\\ud835\\udf03; \\ud835\\udf02+ \\ud835\\udc50); \\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61= 1\\n\\ud835\\udc48\\n\\ud835\\udc56=\\ud835\\udc48\\n\\u2211\\n\\ud835\\udc56=1\\n\\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc56,\\ud835\\udc61\\n(6)\\nwhere \\ud835\\udc3fdenotes the size of the action space, \\ud835\\udc501, \\u2026 , \\ud835\\udc50\\ud835\\udc3fdenotes the number of occurrences of each action, \\ud835\\udf02is the Dirichlet\\nparameter, \\ud835\\udf03is the classification distribution. But the premise of the Dirichlet distribution is to assume that the characteristics of\\neach agent are independent to achieve better clustering based on the characteristics of neighboring agents. In fact, in many multi-\\nagent environments, the characteristics of each agent has a certain correlation, but the Dirichlet distribution does not consider\\nthis correlation, which makes it unable to accurately describe the central agent and the neighborhood agents. There will be some\\ndeviations in the related information. Figure 1 shows the process of a battle between the red and green teams, in which each\\nagent can observe the information of the friendly agent, and the action space of the agent is {\\ud835\\udc62\\ud835\\udc5d, \\ud835\\udc51\\ud835\\udc5c\\ud835\\udc64\\ud835\\udc5b, \\ud835\\udc59\\ud835\\udc52\\ud835\\udc53\\ud835\\udc61, \\ud835\\udc5f\\ud835\\udc56\\ud835\\udc54\\u210e\\ud835\\udc61}. The central\\nagent enclosed by the red circle is affected by the surrounding friendly agents. We use the Dirichlet distribution to simulate and\\ncalculate the probability of the central agent moving in each direction, as shown below:\\n\\u23a7\\n\\u23aa\\n\\u23aa\\n\\u23a8\\n\\u23aa\\n\\u23aa\\u23a9\\n\\ud835\\udc5d\\ud835\\udc62\\ud835\\udc5d= 0.31\\n\\ud835\\udc5d\\ud835\\udc51\\ud835\\udc5c\\ud835\\udc64\\ud835\\udc5b= 0.42\\n\\ud835\\udc5d\\ud835\\udc59\\ud835\\udc52\\ud835\\udc53\\ud835\\udc61= 0.14\\n\\ud835\\udc5d\\ud835\\udc5f\\ud835\\udc56\\ud835\\udc54\\u210e\\ud835\\udc61= 0.13\\n(7)\\nIt can be obtained that the probability of the agent moving down is the highest, which is essentially due to the large number\\nof agents moving \\ud835\\udc51\\ud835\\udc5c\\ud835\\udc64\\ud835\\udc5b. However, moving \\ud835\\udc62\\ud835\\udc5dis the optimal action for the agent to form an encirclement trend with friends. The\\nDirichlet distribution results in a local optimal solution rather than finding the optimal action.\\nZhang et al. [35] believes that the correlation between two agents is crucial for multi-agent reinforcement learning. First, the\\npaper calculates the correlation coefficient between each pair of agents, and then shields the communication among weakly cor-\\nrelated agents, thereby reducing the dimensionality of the state-action value network in the input space. Inspired by Zhang et\\nal. [35], for large-scale partially observable multi-agent environments, it is more necessary to select the importance of neigh-\\nborhood agents. In our paper, we will adopt a graph attention method to filter out more important neighborhood agents, discard\\nunimportant agent information, and achieve more accurate estimation of the average actions of neighborhood agents.\\n3.3\\nGraph Attention Network\\nGraph neural network [29] can better mine the graph structure form between data. Graph Attention Network (GAT) [25] is\\ncomposed of a group of graph attention layers, each graph attention layer acts on the node feature vector of node \\ud835\\udc56denoting as\\n\\ud835\\udc5a\\ud835\\udc56through a weight matrix \\ud835\\udc4a, and then uses softmax to normalize the neighbor nodes of the central node:\\n\\ud835\\udc52\\ud835\\udc56\\ud835\\udc57= (\\ud835\\udc4a\\ud835\\udc5a\\ud835\\udc56\\u2016\\ud835\\udc4a\\ud835\\udc5a\\ud835\\udc57)\\n(8)\\n6\\nleft\\n0.42\\n0.31\\n0.13\\n0.14\\nup\\nright\\ndown\\nFIGURE 1 A battle environment of the red and blue groups, where the red agent in the center is distributed by Dirichlet to\\ncalculate the action.\\n\\ud835\\udefc\\ud835\\udc56\\ud835\\udc57= softmax\\ud835\\udc57(\\ud835\\udc52\\ud835\\udc56\\ud835\\udc57\\n) =\\nexp (\\ud835\\udc52\\ud835\\udc56\\ud835\\udc57\\n)\\n\\u2211\\n\\ud835\\udc58\\u2208\\ud835\\udc41\\ud835\\udc57\\nexp (\\ud835\\udc52\\ud835\\udc57\\ud835\\udc58\\n)\\n(9)\\nwhere \\ud835\\udc52\\ud835\\udc56\\ud835\\udc57is the attention coefficient of each node, indicating the importance of node \\ud835\\udc56to node \\ud835\\udc57. Finally, the output features are\\nobtained by weighting the input features \\u210e\\ud835\\udc56, and the update rule for each node \\ud835\\udc57is:\\n\\ud835\\udc52\\ud835\\udc57= \\ud835\\udf0e\\n\\u239b\\n\\u239c\\n\\u239c\\u239d\\n\\u2211\\n\\ud835\\udc56\\u2208\\ud835\\udc41\\ud835\\udc57\\n\\ud835\\udefc\\ud835\\udc56\\ud835\\udc57\\ud835\\udc4a\\u210e\\ud835\\udc56\\n\\u239e\\n\\u239f\\n\\u239f\\u23a0\\n(10)\\nwhere \\ud835\\udc52\\ud835\\udc57represents the feature of node \\ud835\\udc57, \\ud835\\udc41\\ud835\\udc57is the set of adjacent nodes of node \\ud835\\udc57, and \\ud835\\udf0e(\\u22c5) is a nonlinear activation function.\\n4\\nAPPROACH\\nIn this section, we propose a novel method called Partially Observable Mean Field Multi-Agent Reinforcement Learning based\\non Graph\\u2013Attention (GAMFQ), which can be applied to large-scale partially observable MARL tasks, where the observation\\nrange of each agent is limited, and the feature information of other agents in the fixed neighborhood is intelligently observed.\\nThe overall architecture of the GAMFQ algorithm is depicted in Figure 2, including two important components: the Graph\\nAttention Module and the Mean Field Module: (i) In our Graph\\u2013Attention Module, the information observed locally by each\\nagent is spliced firstly. Then the high-dimensional feature representations are obtained by a latent space mapping process which\\nfollowed by a one-layer LSTM network to obtain the time-series correlation of the target agent, and the hidden layer of the LSTM\\nis used as the input of the graph attention module to initialize the constructed graph nodes. Then to enhance the aggregation of\\nneighbor agents to target agent, a similar process is implemented as a FC mapping network followed by a GAT layer. After that,\\nthe final representation of agents are obtained by a MLP layer with the input of the representations of target agent and other\\nobservable agents. Finally, we adopt layer-normalized method to obtain the adjacency matrix {\\ud835\\udc3a\\ud835\\udc61}\\ud835\\udc41\\n1 via Gumbel Softmax. (ii)\\nThe Mean Field Module utilizes the adjacency matrix {\\ud835\\udc3a\\ud835\\udc61}\\ud835\\udc41\\n1 from Graph Attention Module to obtain adopting action from\\nimportant neighbor agents, in which the joint Q-function of each agent \\ud835\\udc57approximates the Mean-Field Q-function \\ud835\\udc44\\ud835\\udc57(\\ud835\\udc60, \\ud835\\udc4e) \\u2248\\n\\ud835\\udc44\\ud835\\udc57\\nPOMF\\n(\\ud835\\udc60, \\ud835\\udc4e\\ud835\\udc57, \\u0303\\ud835\\udc4e\\ud835\\udc57) of important neighbor agents, where the Q-value is partially observable mean-field(POMF) Q-value, and \\u0303\\ud835\\udc4e\\ud835\\udc57\\nis the average action of the important neighborhood a gents that is partially observable by agent \\ud835\\udc57. Each component is described\\nin detail below.\\n7\\nGraph-Attention\\nModule\\nFeature Vector\\nEffective Feature Vector\\nAgent 1\\nAgent \\ud835\\udc57\\nAgent \\ud835\\udc41\\n\\u2026\\n\\u2026\\n\\u2026\\n\\u2026\\nGet  \\ud835\\udc74Effective neighbors\\n\\ud835\\udc3a\\ud835\\udc61\\nFC\\nLSTM\\nFC\\nGumbel Softmax\\nMLP\\nGAT Encoder\\n\\u2026\\n\\u2026\\n\\ud835\\udc521\\n\\ud835\\udc61\\n\\ud835\\udc52\\ud835\\udc56\\n\\ud835\\udc61\\n\\ud835\\udc52\\ud835\\udc41\\ud835\\udc57\\n\\ud835\\udc61\\n\\u2026\\n\\ud835\\udc38\\ud835\\udc61\\n\\u2026\\nAgent 1\\nAgent \\ud835\\udc57\\nAgent \\ud835\\udc41\\n\\u2026\\n\\ud835\\udc651\\n\\ud835\\udc61\\n\\ud835\\udc65\\ud835\\udc41\\ud835\\udc57\\n\\ud835\\udc61\\n\\ud835\\udc41\\ud835\\udc57neighbors\\n\\u2026\\n\\u2026\\n\\u2026\\n\\u2026\\n\\u2026\\n\\u2026\\nLocal Observable\\nGet  Mean  action\\nMean Field Module\\n\\u2026\\n\\ud835\\udc65\\ud835\\udc56\\n\\ud835\\udc61\\nFIGURE 2 Schematic of GAMFQ. Each agent can observe the feature information of other agents within a fixed range, input\\nit into the Graph\\u2013Attention Module, and output an adjacency matrix to represent the effectiveness of the neighborhood agent to\\nthe central agent.\\n4.1\\nGraph\\u2013Attention Module\\nTo more accurately re-determine the influence of agent \\ud835\\udc57\\u2019s neighbor \\ud835\\udc41\\ud835\\udc57on itself, we need to be able to extract useful infor-\\nmation from the local observations of agent \\ud835\\udc57. The local observations of each agent include the embedding information of\\nneighboring agents. For each agent \\ud835\\udc57and each time step \\ud835\\udc61, the information of a local observation of length \\ud835\\udc3f\\ud835\\udc57, is expressed as\\n\\ud835\\udc5c\\ud835\\udc61\\n\\ud835\\udc57=\\n(\\n\\ud835\\udc65\\ud835\\udc61\\n1, \\ud835\\udc65\\ud835\\udc61\\n2, \\u22ef, \\ud835\\udc65\\ud835\\udc61\\n\\ud835\\udc41\\ud835\\udc57\\n)\\n, where \\ud835\\udc65\\ud835\\udc61\\n\\ud835\\udc41\\ud835\\udc57represents the feature of the \\ud835\\udc41\\ud835\\udc57-th neighbor agent of agent \\ud835\\udc57, and \\ud835\\udc5c\\ud835\\udc61\\n\\ud835\\udc57\\u2208\\ud835\\udc45\\ud835\\udc41\\ud835\\udc57\\u00d7\\ud835\\udc37, \\ud835\\udc65\\ud835\\udc61\\n\\ud835\\udc56\\u2208\\ud835\\udc451\\u00d7\\ud835\\udc37.\\n\\ud835\\udc3f\\ud835\\udc57is concatenated from the embedding features of each neighbor. Our goal is to learn an adjacency matrix {\\ud835\\udc3a\\ud835\\udc61}\\ud835\\udc41\\n1 to extract\\nmore important embedding information for the agent \\ud835\\udc57from local observations at each time step \\ud835\\udc61. Since graph neural networks\\ncan better mine the information of neighbor nodes, we propose a graph attention structure suitable for large-scale multi-agent\\nsystems. This structure focuses on information from different agents by associating weights to observations based on the rel-\\native importance of other agents in their local observations. The Graph\\u2013Attention structure is constructed by concatenating a\\ngraph attention encoder and a differentiable attention mechanism. For the local observation \\ud835\\udc5c\\ud835\\udc61\\n\\ud835\\udc57of agent \\ud835\\udc57at time step \\ud835\\udc61, \\ud835\\udc5c\\ud835\\udc61\\u2032\\n\\ud835\\udc57is first\\nencoded using a fully connected layer (FC) , and is passed to the LSTM layerin order to generate the hidden state \\u210e\\ud835\\udc61\\n\\ud835\\udc57and cell\\nstate \\ud835\\udc50\\ud835\\udc61\\n\\ud835\\udc57of agent \\ud835\\udc57, where \\u210e\\ud835\\udc61\\n\\ud835\\udc57serves as the input of the graph attention module to initialize the constructed graph nodes:\\n\\u210e\\ud835\\udc61\\n\\ud835\\udc57, \\ud835\\udc50\\ud835\\udc61\\n\\ud835\\udc57= \\ud835\\udc3f\\ud835\\udc46\\ud835\\udc47\\ud835\\udc40\\n(\\n\\ud835\\udc52\\n(\\n\\ud835\\udc5c\\ud835\\udc61\\n\\ud835\\udc57\\n)\\n, \\u210e\\ud835\\udc61\\n\\ud835\\udc57, \\ud835\\udc50\\ud835\\udc61\\n\\ud835\\udc57\\n)\\n(11)\\nwhere \\ud835\\udc52(\\u22c5) is a fully connected layer representing the observed encoder. \\u210e\\ud835\\udc61\\n\\ud835\\udc57is encoded as a message:\\n\\ud835\\udc5a\\ud835\\udc61\\n\\ud835\\udc57= \\ud835\\udc52\\n(\\n\\u210e\\ud835\\udc61\\n\\ud835\\udc57\\n)\\n(12)\\nwhere \\ud835\\udc5a\\ud835\\udc61\\n\\ud835\\udc57is the aggregated information of the neighborhood agents observed by agent \\ud835\\udc57at time step \\ud835\\udc61. The input encoding\\ninformation \\ud835\\udc40\\ud835\\udc61is passed to the GAT encoder and hard attention mechanism, where the hard attention mechanism consists\\nof MLP and Gumbel Softmax function. Finally, the output adjacency matrix {\\ud835\\udc3a\\ud835\\udc61}\\ud835\\udc41\\n1 is used to determine which agents in the\\nneighborhood have an influence on the current agent. The GAT encoder helps to efficiently encode the agent\\u2019s local information,\\nwhich is expressed as:\\n{\\ud835\\udc40\\ud835\\udc61}\\ud835\\udc41\\n1 = \\ud835\\udc53\\ud835\\udc46\\ud835\\udc50\\u210e\\ud835\\udc52\\ud835\\udc51\\n(\\ud835\\udc5a\\ud835\\udc61\\n1, \\u22ef, \\ud835\\udc5a\\ud835\\udc61\\n\\ud835\\udc41\\n)\\n(13)\\nAdditionally, we take the form of the same attention mechanism as GAT [25], expressed as:\\n\\ud835\\udefc\\ud835\\udc46\\n\\ud835\\udc56\\ud835\\udc57=\\nexp\\n(\\n\\ud835\\udc3f\\ud835\\udc52\\ud835\\udc4e\\ud835\\udc58\\ud835\\udc66Re\\ud835\\udc3f\\ud835\\udc48\\n(\\n\\ud835\\udc4e\\ud835\\udc47\\n\\ud835\\udc46\\n[\\n\\ud835\\udc4a\\ud835\\udc46\\ud835\\udc5a\\ud835\\udc61\\n\\ud835\\udc56||\\ud835\\udc4a\\ud835\\udc46\\ud835\\udc5a\\ud835\\udc61\\n\\ud835\\udc57\\n]))\\n\\u2211\\n\\ud835\\udc58\\u2208\\ud835\\udc41\\ud835\\udc61\\n\\ud835\\udc57\\u222a{\\ud835\\udc57}\\nexp\\n(\\n\\ud835\\udc3f\\ud835\\udc52\\ud835\\udc4e\\ud835\\udc58\\ud835\\udc66Re\\ud835\\udc3f\\ud835\\udc48\\n(\\n\\ud835\\udc4e\\ud835\\udc47\\n\\ud835\\udc46\\n[\\n\\ud835\\udc4a\\ud835\\udc46\\ud835\\udc5a\\ud835\\udc61\\n\\ud835\\udc57||\\ud835\\udc4a\\ud835\\udc46\\ud835\\udc5a\\ud835\\udc61\\n\\ud835\\udc58\\n]))\\n(14)\\n8\\nwhere \\ud835\\udc3f\\ud835\\udc52\\ud835\\udc4e\\ud835\\udc58\\ud835\\udc66\\ud835\\udc45\\ud835\\udc52\\ud835\\udc3f\\ud835\\udc48(\\u22c5) is the activation function, \\ud835\\udc4e\\ud835\\udc46\\u2208\\ud835\\udc45\\ud835\\udc37is the weight vector, \\ud835\\udc41\\ud835\\udc61\\n\\ud835\\udc57\\u222a{\\ud835\\udc57} represents the central agent \\ud835\\udc57and its\\nobservable neighborhood agent set, and \\ud835\\udc4a\\ud835\\udc46\\u2208\\ud835\\udc45\\ud835\\udc37\\u00d7\\ud835\\udc37is the weight matrix. The node feature of agent \\ud835\\udc57is expressed as:\\n\\ud835\\udc52\\ud835\\udc61\\n\\ud835\\udc57= \\ud835\\udc38\\ud835\\udc3f\\ud835\\udc48\\n\\u239b\\n\\u239c\\n\\u239c\\u239d\\n\\u2211\\n\\ud835\\udc56\\u2208\\ud835\\udc41\\ud835\\udc61\\n\\ud835\\udc57\\u222a\\ud835\\udc57\\n\\ud835\\udefc\\ud835\\udc46\\n\\ud835\\udc56\\ud835\\udc57\\ud835\\udc4a\\ud835\\udc46\\ud835\\udc5a\\ud835\\udc61\\n\\ud835\\udc56\\n\\u239e\\n\\u239f\\n\\u239f\\u23a0\\n(15)\\nwhere \\ud835\\udc38\\ud835\\udc3f\\ud835\\udc48(\\u22c5) is an exponential linear unit function. Connecting the features of each node in pairs: \\ud835\\udc38\\ud835\\udc61\\n\\ud835\\udc56,\\ud835\\udc57=\\n(\\n\\ud835\\udc52\\ud835\\udc61\\n\\ud835\\udc56||\\ud835\\udc52\\ud835\\udc61\\n\\ud835\\udc57\\n)\\n, we can get a\\nmatrix \\ud835\\udc38\\ud835\\udc61\\u2208\\ud835\\udc45\\ud835\\udc41\\u00d7\\ud835\\udc41\\ud835\\udc57\\u00d72\\ud835\\udc37, where \\ud835\\udc38\\ud835\\udc61\\n\\ud835\\udc56,\\ud835\\udc57represents the relevant features of agent \\ud835\\udc57. Taking \\ud835\\udc38\\ud835\\udc61as the input of MLP which is followed\\nby a Gumbel Softmax function, the connected vector \\ud835\\udc3a\\ud835\\udc61\\n\\ud835\\udc57can be obtained. The connected vector \\ud835\\udc3a\\ud835\\udc61\\n\\ud835\\udc57consists of elements \\ud835\\udc54\\ud835\\udc56\\ud835\\udc57,\\nwhere \\ud835\\udc56represents the neighbors of the central agent \\ud835\\udc57. The element \\ud835\\udc54\\ud835\\udc61\\n\\ud835\\udc56\\ud835\\udc57= 1 in the adjacency matrix indicates that the action of\\nthe agent \\ud835\\udc56will have an impact on the agent \\ud835\\udc57. Conversely, \\ud835\\udc54\\ud835\\udc61\\n\\ud835\\udc56\\ud835\\udc57= 0 means that the agent\\u2019s actions have no effect on the agent \\ud835\\udc57.\\n4.2\\nMean Field Module\\nThis Graph-Attention method selects important \\ud835\\udc40\\ud835\\udc57agents from the neighbors \\ud835\\udc41\\ud835\\udc57of agent \\ud835\\udc57, and compute the average of the\\nactions of the choosed neighbor agents:\\n\\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61=\\n1\\n\\ud835\\udc40\\ud835\\udc57\\n\\u2211\\n\\ud835\\udc58\\u2208\\ud835\\udc41\\ud835\\udc57\\n\\ud835\\udc4e\\ud835\\udc58\\n\\ud835\\udc61\\u22c5\\ud835\\udc3a\\ud835\\udc61\\n\\ud835\\udc57,\\n\\ud835\\udc4e\\ud835\\udc58\\n\\ud835\\udc61\\u223c\\ud835\\udf0b\\ud835\\udc58(\\u22c5\\u2223\\ud835\\udc60\\ud835\\udc61, \\u0303\\ud835\\udc4e\\ud835\\udc58\\n\\ud835\\udc61\\n)\\n(16)\\nwhere \\u22c5is the element-wise multiplication.\\nIn the above formula, \\ud835\\udc4e\\ud835\\udc58represents the important neighborhood agent for agent \\ud835\\udc57. Then the Q\\u2013value of each agent is shown\\nin Eq. 17. Note that the Q\\u2013value here is a partially observable Q\\u2013value.\\n\\ud835\\udc44\\ud835\\udc57\\nGAMF\\n(\\ud835\\udc60\\ud835\\udc57\\n\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61, \\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\n) = (1 \\u2212\\ud835\\udefc)\\ud835\\udc44\\ud835\\udc57\\nGAMF\\n(\\ud835\\udc60\\ud835\\udc57\\n\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61, \\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\n) + \\ud835\\udefc[\\ud835\\udc5f\\ud835\\udc57\\n\\ud835\\udc61+ \\ud835\\udefe\\ud835\\udc63(\\ud835\\udc60\\ud835\\udc57\\n\\ud835\\udc61+1\\n)]\\n(17)\\nwhere the value function \\ud835\\udc63\\ud835\\udc57is expressed as\\n\\ud835\\udc63\\ud835\\udc57(\\ud835\\udc60\\ud835\\udc57\\n\\ud835\\udc61+1\\n) =\\n\\u2211\\n\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61+1\\n\\ud835\\udf0b\\ud835\\udc57(\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61+1 \\u2223\\ud835\\udc60\\ud835\\udc57\\n\\ud835\\udc61+1, \\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\n) \\ud835\\udc44\\ud835\\udc57\\nGAMF\\n(\\ud835\\udc60\\ud835\\udc57\\n\\ud835\\udc61+1, \\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61+1, \\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\n)\\n(18)\\nAccording to the above graph attention mechanism, more important neighborhood agents are obtained. The new average\\naction \\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61is calculated by Eq.16, and then the strategy \\ud835\\udf0b\\ud835\\udc57\\n\\ud835\\udc61of agent \\ud835\\udc57is updated by the following formula:\\n\\ud835\\udf0b\\ud835\\udc57(\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\u2223\\ud835\\udc60\\ud835\\udc57\\n\\ud835\\udc61, \\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\u22121\\n) =\\nexp (\\u2212\\ud835\\udefd\\ud835\\udc44\\ud835\\udc57\\nGAMF\\n(\\ud835\\udc60\\ud835\\udc57\\n\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61, \\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\u22121\\n))\\n\\u2211\\n\\ud835\\udc4e\\ud835\\udc57\\u2032\\n\\ud835\\udc61\\u2208\\ud835\\udc34\\ud835\\udc57\\nexp\\n(\\n\\u2212\\ud835\\udefd\\ud835\\udc44\\ud835\\udc57\\nGAMF\\n(\\n\\ud835\\udc60\\ud835\\udc57\\n\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc57\\u2032\\n\\ud835\\udc61, \\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\u22121\\n))\\n(19)\\n4.3\\nTheoretical Proof\\nThis subsection is devoted to proving that the setting of GAMFQ is close to the Nash equilibrium. Subramanian et al. [23]\\nshowed that in partially observable cases, the fixed observation radius (FOR) setting is close to a Nash equilibrium, where the\\nmean action of each agent\\u2019s neighborhood agents is approximated by a dirichlet distribution. First, we state some assumptions,\\nwhich are the same as literature[23], and are followed by all the theorems and analyses below.\\nAssumption 1. For any \\ud835\\udc56and \\ud835\\udc57, there is lim\\ud835\\udc61\\u2192\\u221e\\ud835\\udf0f\\ud835\\udc56\\n\\ud835\\udc57(\\ud835\\udc61) = \\u221e. \\ud835\\udc64.\\ud835\\udc5d.1.\\nThis assumption guarantees a probability of 1 that old information is eventually discarded.\\nAssumption 2. Suppose some measurability conditions are as follow: (1) \\ud835\\udc65(0) is \\ue232(0)-measurable. (2) For each \\ud835\\udc56,\\ud835\\udc57and \\ud835\\udc61, \\ud835\\udc64\\ud835\\udc56(\\ud835\\udc61)\\nis \\ue232(\\ud835\\udc61+1)-measurable. (3) For each \\ud835\\udc56, \\ud835\\udc57and \\ud835\\udc61, \\ud835\\udefc\\ud835\\udc56(\\ud835\\udc61) and \\ud835\\udf0f\\ud835\\udc56\\n\\ud835\\udc57(\\ud835\\udc61) are \\ue232(\\ud835\\udc61)-measurable. (4) For each \\ud835\\udc56and \\ud835\\udc61, satisfy B [\\ud835\\udc64\\ud835\\udc56(\\ud835\\udc61)|\\ue232(\\ud835\\udc61)] = 0.\\n(5) B [\\ud835\\udc642\\n\\ud835\\udc56(\\ud835\\udc61)|\\ue232(\\ud835\\udc61)] \\u2264\\ud835\\udc34+ \\ud835\\udc35max\\ud835\\udc57max\\ud835\\udf0f\\u2264\\ud835\\udc61|||\\ud835\\udc65\\ud835\\udc57(\\ud835\\udf0f)|||\\n2\\n, where \\ud835\\udc34and \\ud835\\udc35are deterministic constants.\\nAssumption 3. The learning rates satisfy 0 \\u2264\\ud835\\udefc\\ud835\\udc56(\\ud835\\udc61) < 1.\\nAssumption 4. Suppose some conditions for the \\ud835\\udc39mapping are as follows: (1) If \\ud835\\udc65\\u2264\\ud835\\udc66, then \\ud835\\udc39(\\ud835\\udc65) \\u2264\\ud835\\udc39(\\ud835\\udc66), that is, \\ud835\\udc39is\\nmonotonic; (2) \\ud835\\udc39is continuous; (3) When \\ud835\\udc61\\u2192\\u221e, \\ud835\\udc39is limited to the interval [\\ud835\\udc65\\u2217\\u2212\\ud835\\udc37, \\ud835\\udc65\\u2217+ \\ud835\\udc37], where \\ud835\\udc65\\u2217is some arbitrary\\n9\\npoint; (4) If \\ud835\\udc52\\u2208\\ue23e\\ud835\\udc5bis a vector that satisfies all components equal to 1, then \\ud835\\udc39(\\ud835\\udc65) \\u2212\\ud835\\udc5d\\ud835\\udc52\\u2264\\ud835\\udc39(\\ud835\\udc65+ \\ud835\\udc5d\\ud835\\udc52) \\u2264\\ud835\\udc39(\\ud835\\udc65+ \\ud835\\udc5d\\ud835\\udc52) + \\ud835\\udc5d\\ud835\\udc52, where \\ud835\\udc5d\\nis a positive scalar.\\nAssumption 5. Each action-value pair can be accessed indefinitely, and the reward is limited.\\nAssumption 6. Under the limit \\ud835\\udc61\\u2192\\u221eof infinite exploration, the agent\\u2019s policy is greedy.\\nThis assumption ensures that the agent is rational.\\nAssumption 7. In each stage of a stochastic game, a Nash equilibrium can be regarded as a global optimum or saddle point.\\nBased on these assumptions, Subramanian et al. [23] give the following lemma.\\nLemma 1. [23] When the Q-function is updated using the partially observable update rule in Eq.2, and assumptions 3, 5, and\\n7 hold, the following holds for \\ud835\\udc61\\u2192\\u221e:\\n|\\ud835\\udc44\\u2217(\\ud835\\udc60\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc61) \\u2212\\ud835\\udc44\\ud835\\udc43\\ud835\\udc42\\ud835\\udc40\\ud835\\udc39(\\ud835\\udc60\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc61, \\u0303\\ud835\\udc4e\\ud835\\udc61)| \\u22642\\ud835\\udc37\\n(20)\\nwhere \\ud835\\udc44\\u2217is the Nash Q-value, \\ud835\\udc44\\ud835\\udc43\\ud835\\udc42\\ud835\\udc40\\ud835\\udc39is the partially observable mean-field Q-function, and \\ud835\\udc37is the bound of the \\ud835\\udc39map. The\\nprobability that the above formula holds is at least \\ud835\\udeff\\ud835\\udc3f\\u22121 , where \\ud835\\udc3f= |\\ud835\\udc34|.\\nIn our GAMFQ setting, for partially observable neighborhood agents, we choose to select a limited number of important agents\\nby using graph attention, and then update the POMF Q function. The following theorem proves that the setting of GAMFQ is\\nclose to Nash equilibrium.\\nTheorem 1. The distance between the MFQ (globally observable) mean action \\u0304\\ud835\\udc4eand the GAMFQ (partially observable) mean\\naction \\u0303\\ud835\\udc4esatisfies the following formula:\\n||| \\u0303\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\u2212\\u0304\\ud835\\udc4e\\ud835\\udc57\\n\\ud835\\udc61\\n||| \\u2264\\n\\u221a\\n1\\n2\\ud835\\udc41\\ud835\\udc57\\nlog 2\\n\\ud835\\udeff\\n(21)\\nWhen \\ud835\\udc61\\u2192\\u221e, the probability >= \\ud835\\udeff, where \\ud835\\udc41\\ud835\\udc57is the number of observed neighbor agents, \\u0303\\ud835\\udc4eis the partially observable mean\\naction obtained by graph attention in Eq. 16, \\u0304\\ud835\\udc4eis the globally observable mean action in Eq. 4.\\nAssuming that each agent is globally observable, the mean of important agents selected by graph attention is close to the true\\nunderlying global observable \\u0304\\ud835\\udc4e. Since the GAMF Q-function is updated by taking finite samples through graph attention, the\\nempirical mean is \\u0303\\ud835\\udc4e.\\nTheorem 2. If the Q-function is Lipschitz continuous with respect to the mean action, i.e. \\ud835\\udc40is constant, then the MF Q-function\\n\\ud835\\udc44\\ud835\\udc40\\ud835\\udc39and GAMF Q-function \\ud835\\udc44\\ud835\\udc3a\\ud835\\udc34\\ud835\\udc40\\ud835\\udc39satisfy the following relation:\\n|||\\ud835\\udc44\\ud835\\udc3a\\ud835\\udc34\\ud835\\udc40\\ud835\\udc39\\n(\\ud835\\udc60\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc61, \\u0303\\ud835\\udc4e\\ud835\\udc61\\u22121\\n) \\u2212\\ud835\\udc44\\ud835\\udc40\\ud835\\udc39\\n(\\ud835\\udc60\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc61, \\u0304\\ud835\\udc4e\\ud835\\udc61\\u22121\\n)||| \\u2264\\ud835\\udc40\\u00d7 \\ud835\\udc3f\\u00d7 log 2\\n\\ud835\\udeff\\u00d7\\n1\\n2\\ud835\\udc41\\ud835\\udc57\\n(22)\\nWhen the limit \\ud835\\udc61\\u2192\\u221e, the probability is \\u2265(\\ud835\\udeff)\\ud835\\udc3f\\u22121, where \\ud835\\udc3f= |\\ud835\\udc34|, \\ud835\\udc34is the action space of the agent.\\nIn the proof of theorem 2, first consider a Q-function that is Lipschitz continuous for all \\u0304\\ud835\\udc4eand \\u0303\\ud835\\udc4e.According to theorem 1, the\\nabove formula can further deduce the result of theorem 2. The total number of components is equal to the action space \\ud835\\udc3f. The\\nbound of theorem 1 is probability >= \\ud835\\udeff, and since there are \\ud835\\udc3frandom variables, the probability of theorem 2 is at least (\\ud835\\udeff)\\ud835\\udc3f\\u22121.\\nWhen the first \\ud835\\udc3f\\u22121 random variable is fixed, the deterministic last \\u0304\\ud835\\udc4ecomponent satisfies the relationship that the sum of the\\nindividual components is 1. Since each agent\\u2019s action is represented by a one-hot encoding, the \\u0303\\ud835\\udc4e\\u2032 component of GAMFQ also\\nsatisfies the relationship that the sum of the individual components is 1, and the component of the agent\\u2019s average action does\\nnot change due to the application of graph attention. The proof of theorem 2 ends.\\nTheorem 3. A stochastic process in form \\ud835\\udc65\\ud835\\udc56(\\ud835\\udc61+ 1) = \\ud835\\udc65\\ud835\\udc56(\\ud835\\udc61) + \\ud835\\udefc\\ud835\\udc56(\\ud835\\udc61) (\\ud835\\udc39\\ud835\\udc56\\n(\\ud835\\udc65\\ud835\\udc56(\\ud835\\udc61)) \\u2212\\ud835\\udc65\\ud835\\udc56(\\ud835\\udc61) + \\ud835\\udc64\\ud835\\udc56(\\ud835\\udc61)) remains bounded in the range\\n[\\ud835\\udc65\\u2217\\u22122\\ud835\\udc37, \\ud835\\udc65\\u2217+ 2\\ud835\\udc37] on limit \\ud835\\udc61\\u2192\\u221eif assumptions 1,2,3 and 4 are satisfied, and are guaranteed not to diverge to infinity.Where\\n\\ud835\\udc37is the boundary of the \\ud835\\udc39map in assumption 4(4).\\nThis theorem can be proved in terms of Tsitsiklis[24] and by extension. The result of theorem 3 can then be used to derive\\ntheorem 4.\\n10\\nTheorem 4. When the Q-function is updated using the partially observable update rule in Eq.17, and assumptions 3, 5, and 7\\nhold, the following holds for \\ud835\\udc61\\u2192\\u221e:\\n|\\ud835\\udc44\\u2217(\\ud835\\udc60\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc61) \\u2212\\ud835\\udc44\\ud835\\udc3a\\ud835\\udc34\\ud835\\udc40\\ud835\\udc39(\\ud835\\udc60\\ud835\\udc61, \\ud835\\udc4e\\ud835\\udc61, \\u0303\\ud835\\udc4e\\ud835\\udc61)| \\u22642\\ud835\\udc37\\n(23)\\nwhere \\ud835\\udc44\\u2217is the Nash Q-value, \\ud835\\udc44\\ud835\\udc3a\\ud835\\udc34\\ud835\\udc40\\ud835\\udc39is the partially observable mean-field Q-function, and \\ud835\\udc37is the bound of the \\ud835\\udc39map. The\\nprobability that the above formula holds is at least \\ud835\\udeff\\ud835\\udc3f\\u22121 , where \\ud835\\udc3f= |\\ud835\\udc34|.\\nTheorem 4 shows that the GAMFQ update is very close to the Nash equilibrium at the limit \\ud835\\udc61\\u2192\\u221e, i.e. reaching a plateau\\nfor stochastic policies. Therefore, the strategy of Eq.19 is approximately close to this plateau. Theorem 4 is an application of\\ntheorem 3, using assumptions 3, 5 and 7 .However, in MARL, reaching a Nash equilibrium is not optimal, but only a fixed-point\\nguarantee. Therefore, to achieve better performance, each selfish agent will still tend to pick a limited number of samples. To\\nbalance theory and performance when selecting agents from the neighborhood, an appropriate number of agents (more efficient\\nagents) need to be used for better multi-agent system performance. This paper uses the graph attention structure to filter out\\nmore important proxies, which can better approximate the Nash equilibrium.\\n4.4\\nAlgorithm\\nThe implementation of GAMFQ follows the related work of the previous POMFQ [23], the difference is that the graph attention\\nstructure is used to select the neighborhood agents that are more important to the central agent when updating the average action.\\nAlgorithm 1 gives the pseudocode of the GAMFQ algorithm. It obtains effective neighbor agents by continuously updating the\\nadjacency matrix \\ud835\\udc3a\\ud835\\udc61\\n\\ud835\\udc57to update the agent\\u2019s strategy.\\nAlgorithm 1 Partially Observable Mean Field MARL Based on Graph\\u2013Attention\\nInitialize the weights of Q-function \\ud835\\udc44\\ud835\\udf19\\ud835\\udc57, \\ud835\\udc44\\ud835\\udf19\\ud835\\udc57\\n\\u2212, replay buffer \\ud835\\udc35, GAT encoder, MLP layers and mean action \\u0304\\ud835\\udc4e\\ud835\\udc57for each agent\\n\\ud835\\udc57\\u22081, \\u2026 , \\ud835\\udc41.\\nfor \\ud835\\udc52\\ud835\\udc5d\\ud835\\udc56\\ud835\\udc60\\ud835\\udc5c\\ud835\\udc51\\ud835\\udc52= 1, 2, \\u2026 , \\ud835\\udc38do\\nfor \\ud835\\udc61\\u2264\\ud835\\udc47and not terminal do\\nFor each agent \\ud835\\udc57, calculate the hidden state \\u210e\\ud835\\udc61\\n\\ud835\\udc57according to Eq.11, and encode \\u210e\\ud835\\udc61\\n\\ud835\\udc57as a message \\ud835\\udc5a\\ud835\\udc61\\n\\ud835\\udc57(Eq.12).\\nFor each agent \\ud835\\udc57, sample \\ud835\\udc4e\\ud835\\udc57fron policy induced by \\ud835\\udc44\\ud835\\udf19\\ud835\\udc57(Eq.19).\\nFor each agent \\ud835\\udc57, pass the encoded information \\ud835\\udc5a\\ud835\\udc61\\n\\ud835\\udc57to the GAT encoder and hard attention mechanism to output the\\nadjacency matrix \\ud835\\udc3a\\ud835\\udc61\\n\\ud835\\udc57.\\nFor each agent \\ud835\\udc57, calculate the new neighborhood agent mean action \\u0304\\ud835\\udc4e\\ud835\\udc57by Eq.16.\\nReceive the full state of environment \\ud835\\udc60\\ud835\\udc61, action \\ud835\\udc4e= [\\ud835\\udc4e1, \\u2026 , \\ud835\\udc4e\\ud835\\udc41], reward [\\ud835\\udc5f= \\ud835\\udc5f1, \\u2026 , \\ud835\\udc5f\\ud835\\udc41], and the next state \\ud835\\udc60\\u2032 =\\n[\\ud835\\udc601, \\u2026 , \\ud835\\udc60\\ud835\\udc41].\\nStore transition \\u27e8\\ud835\\udc60, \\ud835\\udc4e, \\ud835\\udc5f, \\ud835\\udc60\\u2032, \\u0304\\ud835\\udc4e\\u27e9in \\ud835\\udc35, where \\u0304\\ud835\\udc4e= [ \\u0304\\ud835\\udc4e1, \\u2026 , \\u0304\\ud835\\udc4e\\ud835\\udc41] is the mean action.\\nend for\\nfor \\ud835\\udc57= 1, \\u2026 , \\ud835\\udc41do\\nSample a minibatch of K experiences \\u27e8\\ud835\\udc60, \\ud835\\udc4e, \\ud835\\udc5f, \\ud835\\udc60\\u2032, \\u0304\\ud835\\udc4e\\u27e9from replay buffer \\ud835\\udc35.\\nSet \\ud835\\udc66\\ud835\\udc57= \\ud835\\udc5f\\ud835\\udc57+ \\ud835\\udefe\\ud835\\udc63\\ud835\\udf19\\n(\\ud835\\udc60\\u2032) according to Eq.18.\\nminimize the loss \\ud835\\udc3f(\\ud835\\udf19\\ud835\\udc57) = (\\ud835\\udc66\\ud835\\udc57\\u2212\\ud835\\udc44\\ud835\\udf13\\ud835\\udc57\\n(\\ud835\\udc60\\u2032, \\ud835\\udc4e\\ud835\\udc57, \\u0304\\ud835\\udc4e\\ud835\\udc57))2\\nto update Q network.\\nend for\\nFor each agent \\ud835\\udc57, update params of target network :\\ud835\\udf19\\ud835\\udc57\\u2190\\ud835\\udf0f\\ud835\\udf19\\ud835\\udc57+ (1 \\u2212\\ud835\\udf0f)\\ud835\\udf19\\ud835\\udc57.\\nend for\\n5\\nEXPERIMENTS\\nIn this section, we describe three different tasks based on the MAgent framework and give some experimental setup and training\\ndetails for evaluating the GAMFQ performance.\\n11\\n5.1\\nEnvironments and Tasks\\nSubramanian et al. [23] designed three different cooperative-competitive strategies in the MAgent framework [36] as experimen-\\ntal environments, and our experiments adopt the same environments. In these three tasks, the map size is set to 28*28, where the\\nobservation range of each agent is 6 units. The state space is the concatenation of the feature information of other agents within\\neach agent\\u2019s field of view, including location, health, and grouping information. The action space includes 13 move actions and\\n8 attack actions. In addition, each agent is required to handle at most 20 other agents that are closest. We will evaluate against\\nthese three tasks:\\n\\u2022 Multibattle environment: There are two groups of agents fighting each other, each containing 25 agents. The agent gets\\n-0.005 points for each move, -0.1 points for attacking an empty area, 200 points for killing an enemy agent, and 0.2 points\\nfor a successful attack. Each agent is 2*2 in size, has a maximum health of 10 units, and a speed of 2 units. After the\\nbattle, the team with the most surviving agents wins. If both teams have the same number of surviving agents, the team\\nwith the highest reward wins. The reward for each team is the sum of the rewards for the individual agents in the team.\\n\\u2022 Battle-Gathering environment: There is a uniform distribution of food in the environment, each agent can observe the\\nlocation of all the food. In addition to attacking the enemy to get rewards, each agent can also eat food to get rewards. Agents\\nget 5 points for attacking enemy agents, and the rest of the reward settings are the same as the Multibattle environment.\\n\\u2022 Predator-Prey environment: There are 40 predators and 20 prey, where each predator is a square grid of size 2*2 with a\\nmaximum health of 10 units and a speed of 2 units. Prey is a 1*1 square with a maximum health of 2 units and a speed of\\n2.5 units. To win the game, the predator must kill more prey, and the prey must find a way to escape. In addition, predators\\nand prey have different reward functions, predators get -0.3 points for attacking space, 1 point for successfully attacking\\nprey, 100 points for killing prey, -1 point for attacked prey, and 0.5 points for dying. Unlike the Multibattle environment,\\nwhen the round ends for a fairer duel, if the two teams have the same number of surviving agents, it is judged as a draw.\\n5.2\\nEvaluation\\nWe consider four algorithms for the above three games: MFQ, MFAC [31], POMFQ(FOR) and GAMFQ, where MFQ and\\nMFAC are baselines and POMFQ(FOR) [23] is the state-of-the-art algorithm.\\nThe original baselines MFQ and MFAC were proposed by Yang et al. [31] based on global observability, and the idea was to\\napproximate the influence of the neighborhood agents on the central agent as their average actions, thereby updating the actions\\nof the neighborhood agents. We fix the observation radius of each agent in the baseline MFQ and MFAC and apply it to a partially\\nobservable environment, where neighbor agents are agents within a fixed range. The POMFQ(FOR) algorithm introduces noise\\nin the mean action parameters to encourage exploration, uses Bayesian inference to update the Dirichlet distribution, and samples\\n100 samples from the Dirichlet distribution to estimate partially observable mean field actions. The GAMFQ algorithm judges\\nthe effectiveness of neighborhood agents within a fixed range through the graph attention mechanism, selects more important\\nneighborhood agents, and updates the average action by averaging the actions of these agents.\\n5.3\\nHyperparameters\\nIn the three tasks, each algorithm was trained for 2000 epochs in the training phase, generating two sets of A and B sets of models.\\nIn the test phase, 1000 rounds of confrontation were conducted, of which the first 500 rounds were the first group A of the first\\nalgorithm against the second group B of the second algorithm, and the last 500 groups were the opposite. The hyperparameters\\nof MFQ, MFAC, POMFQ(FOR) and GAMFQ are basically the same. Table 1 lists the hyperparameters during training of the\\nfour algorithms, and the remaining parameters can be seen in [23].\\n6\\nRESULTS AND DISCUSSION\\nIn this section, we evaluate the performance of GMAFQ in three different environments, including Multibattle, Battle-Gathering,\\nand Predator-Prey. We benchmark against two algorithms, MFQ and MFAC, and compare with the state-of-the-art POMFQ\\n12\\nTABLE 1 Hyperparameters for four algorithms training.\\nParameter\\nValue\\nDescription\\n\\ud835\\udefc\\n10\\u22124\\nlearning rate\\n\\ud835\\udefd\\ndecays linearly from 1 to 0\\nexploration rate\\n\\ud835\\udefe\\n0.95\\ndiscount rate\\n\\ud835\\udc35\\n1024\\nreplay buffer\\n\\u210e\\n64\\nthe hidden layer size in GAT\\n\\ud835\\udc3e\\n64\\nmini-batch\\n\\ud835\\udc61\\ud835\\udc52\\ud835\\udc5a\\ud835\\udc5d\\ud835\\udc52\\ud835\\udc5f\\ud835\\udc4e\\ud835\\udc61\\ud835\\udc62\\ud835\\udc5f\\ud835\\udc52\\n0.1\\nthe soft-max layer temperature of the actor in MFAC\\n(FOR). We implement our method and comparative methods on three different tasks. Note that we only used 50 agents in our\\nexperiments and did not test more agents, this is because the proportion of other agents that each agent can see is more important\\nthan the absolute number.\\n6.1\\nReward\\nFigure 3 shows how the reward changes as the number of iterations increases during training. We plot the reward changes for\\nthe four algorithms in different game environments during the first 1000 iterations. Since each algorithm is self-training which\\nresults in a large change in the reward of the algorithm, we use the least squares method to fit the reward change graph. In Figure\\n3, the solid black line represents the reward change graph of the GAMFQ algorithm. From Figure 3 (a), (b) and (c), it can be\\nseen that the reward of the GAMFQ algorithm can increase rapidly, indicating that the GAMFQ algorithm can converge rapidly\\nin the early stage, and the convergence performance is better than the other three algorithms.\\n0\\n200\\n400\\n600\\n800\\n1000\\nEpisode\\n1000\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\n6000\\nReward\\n* MFQ\\n* MFAC\\n* POMFQ(FOR)\\n* GAMFQ\\n(a) Train results of Multibattle game.\\n0\\n200\\n400\\n600\\n800\\n1000\\nEpisode\\n0\\n2000\\n4000\\n6000\\n8000\\n10000\\n12000\\n14000\\nReward\\n* MFQ\\n* MFAC\\n* POMFQ(FOR)\\n* GAMFQ\\n(b) Train results of Battle-Gathering game.\\n0\\n200\\n400\\n600\\n800\\n1000\\nEpisode\\n1000\\n0\\n1000\\n2000\\n3000\\n4000\\nReward\\n* MFQ\\n* MFAC\\n* POMFQ(FOR)\\n* GAMFQ\\n(c) Train results of Predator-Prey game.\\nFIGURE 3 Train results of three games. The reward curve for each algorithm is fitted by the least squares method.\\n6.2\\nElo Calculation\\nWe use ELO Score [11] to evaluate the performance of the two groups of agents, the advantage of which is that it takes into\\naccount the strength gap between the opponents themselves. ELO ratings are commonly used in chess to evaluate one-on-one\\nsituations, and this approach can similarly be extended to N-versus-N situations. For the algorithm proposed in the paper, we\\nrecord the total rewards of the two teams of agents during each algorithm confrontation, which are \\ud835\\udc451 and \\ud835\\udc452, respectively.\\nThen the expected win rates of the two groups of agents are:\\n\\ud835\\udc381 =\\n1\\n1 + 10(\\ud835\\udc452\\u2212\\ud835\\udc451)\\u2215400 , \\ud835\\udc382 =\\n1\\n1 + 10(\\ud835\\udc451\\u2212\\ud835\\udc452)\\u2215400\\n(24)\\n13\\nwhere \\ud835\\udc381 + \\ud835\\udc382 = 1. By analyzing the actual and predicted winning rates of the two groups of agents, the new ELO score of\\neach team after the game ends can be obtained:\\n\\ud835\\udc451\\n\\u2032 = \\ud835\\udc451 + \\ud835\\udc3e(\\ud835\\udc461 \\u2212\\ud835\\udc381), \\ud835\\udc452\\n\\u2032 = \\ud835\\udc452 + \\ud835\\udc3e(\\ud835\\udc462 \\u2212\\ud835\\udc382)\\n(25)\\nwhere \\ud835\\udc451 represents the actual winning or losing value, 1 means the team wins, 0.5 means the two teams are tied, and 0 means\\nthe team loses. \\ud835\\udc3eis represented as a floating coefficient. To create a gap between agents, we set \\ud835\\udc3eto 32. For each match, we\\nfaced off 500 times and calculated the average ELO value for all matches.\\nTABLE 2 The ELO Score of four algorithms in Multibattle environment.\\nTask\\nAlgorithm1\\nAlgorithm2\\nELO Score1\\nELO Score2\\nGAMFQ vs POMFQ(FOR)\\nGAMFQ-1\\nPOMFQ(FOR)-2\\n3579\\n820\\nGAMFQ-2\\nPOMFQ(FOR)-1\\n2696\\n2838\\nGAMFQ vs MFQ\\nGAMFQ-1\\nMFQ-2\\n2098\\n1508\\nGAMFQ-2\\nMFQ-1\\n2535\\n1695\\nGAMFQ vs MFAC\\nGAMFQ-1\\nMFAC-2\\n1350\\n-49\\nGAMFQ-2\\nMFAC-1\\n-856\\n-78\\nPOMFQ(FOR) vs MFQ\\nPOMFQ(FOR)-1\\nMFQ-2\\n3145\\n2577\\nPOMFQ(FOR)-2\\nMFQ-1\\n2569\\n2857\\nPOMFQ(FOR) vs MFAC\\nPOMFQ(FOR)-1\\nMFAC-2\\n-205\\n-64\\nPOMFQ(FOR)-2\\nMFAC-1\\n826\\n-42\\nMFQ vs MFAC\\nMFQ-1\\nMFAC-2\\n-142\\n-49\\nMFQ-2\\nMFAC-1\\n610\\n-46\\nTABLE 3 The ELO Score of four algorithms in Battle-Gathering environment.\\nTask\\nAlgorithm1\\nAlgorithm2\\nELO Score1\\nELO Score2\\nGAMFQ vs POMFQ(FOR)\\nGAMFQ-1\\nPOMFQ(FOR)-2\\n7770\\n8931\\nGAMFQ-2\\nPOMFQ(FOR)-1\\n8293\\n9310\\nGAMFQ vs MFQ\\nGAMFQ-1\\nMFQ-2\\n6374\\n10870\\nGAMFQ-2\\nMFQ-1\\n8510\\n8313\\nGAMFQ vs MFAC\\nGAMFQ-1\\nMFAC-2\\n5525\\n10\\nGAMFQ-2\\nMFAC-1\\n10751\\n-31\\nPOMFQ(FOR) vs MFQ\\nPOMFQ(FOR)-1\\nMFQ-2\\n8526\\n8760\\nPOMFQ(FOR)-2\\nMFQ-1\\n8632\\n8227\\nPOMFQ(FOR) vs MFAC\\nPOMFQ(FOR)-1\\nMFAC-2\\n12722\\n0\\nPOMFQ(FOR)-2\\nMFAC-1\\n12171\\n-88\\nMFQ vs MFAC\\nMFQ-1\\nMFAC-2\\n12649\\n49\\nMFQ-2\\nMFAC-1\\n13788\\n-48\\n14\\nTable 2, 3, 4 shows the ELO scores of the four algorithms on the three tasks. It can be seen from Table 2 that in Multibattle\\nenvironment, the GAMFQ algorithm has the highest ELO score of 3579, which is significantly better than the other three\\nalgorithms. As shown in Table 3, in Battle-Gathering environment, the ELO score of the MFQ algorithm is the highest, and the\\nELO score of the GAMFQ algorithm is average. This is because the collection environment contains food, and some algorithms\\ntend to eat food to get rewards quickly, rather than attacking enemy agents. However, the final game winning or losing decision\\nis made by comparing the number of remaining agents between the two teams of agents. As shown in Table 4, in Predator-Prey\\nenvironment, the ELO score of the GAMFQ algorithm has the highest ELO score of 860, which is significantly better than\\nthe other three algorithms. From the experimental results in the three environments, we can summarize that ELO score of the\\nGAMFQ algorithm is better than other three algorithms, showing better performance.\\nTABLE 4 The ELO Score of four algorithms in Predator-Prey environment.\\nTask\\nAlgorithm1\\nAlgorithm2\\nELO Score1\\nELO Score2\\nGAMFQ vs POMFQ(FOR)\\nGAMFQ-1\\nPOMFQ(FOR)-2\\n421\\n-32\\nGAMFQ-2\\nPOMFQ(FOR)-1\\n16\\n7\\nGAMFQ vs MFQ\\nGAMFQ-1\\nMFQ-2\\n714\\n-27\\nGAMFQ-2\\nMFQ-1\\n-15\\n-94\\nGAMFQ vs MFAC\\nGAMFQ-1\\nMFAC-2\\n860\\n-28\\nGAMFQ-2\\nMFAC-1\\n16\\n16\\nPOMFQ(FOR) vs MFQ\\nPOMFQ(FOR)-1\\nMFQ-2\\n66\\n18\\nPOMFQ(FOR)-2\\nMFQ-1\\n13\\n24\\nPOMFQ(FOR) vs MFAC\\nPOMFQ(FOR)-1\\nMFAC-2\\n16\\n-16\\nPOMFQ(FOR)-2\\nMFAC-1\\n47\\n16\\nMFQ vs MFAC\\nMFQ-1\\nMFAC-2\\n16\\n-16\\nMFQ-2\\nMFAC-1\\n174\\n17\\n6.3\\nResults\\nFigure 4 shows the face-off results of the four algorithms in the three tasks. Figure 4(a) shows the faceoff results of Multibattle\\ngame. The different colored bars for each algorithm represent the results of an algorithm versus others. We do not conduct\\nadversarial experiments between the same algorithms because we consider that the adversarial properties of the same algorithms\\nare equal. The vertical lines in the bar graph represent the standard deviation of wins for groups A and B over 1,000 face-offs.\\nFigure 4(a) shows GAMFQ against three other algorithms, all with a win rate above 0.7.\\nFigure 4(b) shows the faceoff results of Battle-Gathering game. In addition to getting rewards for killing enemies, agents can\\nalso get rewards from food. It can be seen that MFQ loses to all other algorithms, MFAC and POMFQ (FOR) perform in general,\\nand our GAMFQ is clearly ahead of other algorithms.\\nFigure 4(c) shows thwe faceoff results of Predator-Prey game.The standard deviation of this game is significantly higher than\\nthe previous two games, due to the fact that both groups A and B are trying to beat each other in the environment. It can be seen\\nthat the GAMFQ algorithm is significantly better than other three algorithms, reaching a winning rate of 1.0.\\nExperiments in the above three multi-agent combat environments show that GAMFQ can show good performance over MFQ,\\nMFAC and POMFQ(FOR).\\n15\\nMFQ\\nMFAC\\nPOMFQ(FOR) GAMFQ\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\nNumber of Games won\\n* MFQ\\n* MFAC\\n* POMFQ(FOR)\\n* GAMFQ\\n(a) Faceoff results of Multibattle game.\\nMFQ\\nMFAC\\nPOMFQ(FOR) GAMFQ\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\nNumber of Games won\\n* MFQ\\n* MFAC\\n* POMFQ(FOR)\\n* GAMFQ\\n(b) Faceoff results of Battle-Gathering game.\\nMFQ\\nMFAC\\nPOMFQ(FOR) GAMFQ\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\nNumber of Games won\\n* MFQ\\n* MFAC\\n* POMFQ(FOR)\\n* GAMFQ\\n(c) Faceoff results of Predator-Prey game.\\nFIGURE 4 Faceoff results of three games. The * in the legend indicates the enemy. For example, the first blue bar in the bar\\ngraph corresponding to the GAMFQ algorithm is the result of the confrontation between GAMFQ and MFQ, and we do not\\nconduct confrontation experiments between the same algorithms.\\n6.4\\nVisualization\\n(a) Multibattle begin\\n(b) Multibattle process \\n(c) Multibattle end \\ncooperative \\nattack\\nattack\\nescape\\nGAMFQ   alive\\uff1a22\\nPOMFQ(FOR)    alive\\uff1a2\\nFIGURE 5 Visualization of the standoff between GAMFQ and POMFQ (FOR) in a Multibattle game.\\n(a) Battle-Gathering begin\\n(b) Battle-Gathering process \\n(c) Battle-Gathering end \\nBesiege \\nbegin\\nBesiege \\nend\\nfood\\nGAMFQ   alive\\uff1a24\\nPOMFQ(FOR)    alive\\uff1a9\\nFIGURE 6 Visualization of the standoff between GAMFQ and POMFQ (FOR) in a Battle-Gathering game.\\n16\\n(a) Predator-Prey begin\\n(b1) Predator-Prey process \\n(b2) Predator-Prey process \\nGAMFQ   alive\\uff1a25\\nPOMFQ(FOR)    alive\\uff1a21\\nPOMFQ(FOR)    alive\\uff1a25\\nGAMFQ   alive\\uff1a25\\nencircle\\nescape\\nFIGURE 7 Visualization of the standoff between GAMFQ and POMFQ (FOR) in a Predator-Prey game.\\nTo visualize the effectiveness of the GAMFQ algorithm, we visualize the confrontation between GAMFQ and POMFQ (FOR)\\nin a Multibattle environment, as shown in Figure 5, where the red side is GMAFQ and the blue side is POMFQ (FOR). It can\\nbe seen from the confrontation process that for the GAMFQ algorithm, when an agent decides to attack, the surrounding agents\\nwill also decide to attack under its influence, forming a good cooperation mechanism. On the contrary, for the POMFQ (FOR)\\nalgorithm, some blue-side agents are chosen to attack, some are chosen to escape, and no common fighting mechanism was\\nformed. Similarly, in the Battle-Gathering environment of Figure 6, GAMFQ can learn the surrounding mechanism well. In the\\nPredator-Prey environment of Figure 7, when GAMFQ acts as a predator, the technique of surrounding the prey POMFQ (FOR)\\ncan be learned. On the contrary, when POMFQ (FOR) acted as a predator, it failed to catch the prey GMAFQ.\\n6.5\\nAblation study\\n0\\n200\\n400\\n600\\n800\\n1000\\nEpisode\\n-1000\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\n6000\\nReward\\nR=2\\nR=4\\nR=6\\nR=8\\nR=10\\nFIGURE 8 Ablation study. R represents the observation radius of the agent.\\nFigure 8 is an ablation study that investigates the performance of the GAMFQ algorithm for different observation radius in a\\nMultibattle environment. where the solid line represents the least squares fit of the reward change. It can be seen from the figure\\nthat when the number of training is small, the performance of the algorithm is better as the observation distance increases. But\\nwith the increase of training times, when R=4, the performance of the algorithm is the best, so the appropriate observation\\ndistance can achieve better performance. What is more important in this paper is to explore the effect of the ratio of observable\\ndistance to the number of agents on the performance of the algorithm, so there is no experiment with more agents.\\n17\\n7\\nCONCLUSION\\nIn this paper, we proposed a new multi-agent reinforcement learning algorithm, Graph Attention-based Partially Observable\\nMean Reinforcement Learning (GAMFQ), to address the problem of large-scale partially observable multi-agent environments.\\nAlthough existing methods are close to Nash equilibrium, they do not take into account the direct correlation of agents. Based\\non the correlation between agents, GAMFQ uses a Graph-Attention module to describe how each agent is affected by the actions\\nof other agents at each time step. Experimental results on three challenging tasks in the MAgents framework illustrate that,\\nour proposed method outperforms baselines in all these games and outperforms the state-of-the-art partially observable mean-\\nfield reinforcement learning algorithms. In the future, we will further explore the correlation between agents to extend to more\\ncommon cooperation scenarios.\\nConflict of interest\\nThe authors declare no potential conflict of interests.\\nArticle Description\\nThe\\nexpanded\\nversion\\nof\\nthis\\narticle\\nis\\npublished\\nin\\nDrones\\n2023,\\n7(7),\\n476,\\nwith\\na\\nDOI\\nof\\nhttps://doi.org/10.3390/drones7070476.\\nReferences\\n[1] A. T. Azar et al., Drone deep reinforcement learning: A review, Electronics 10 (2021), no. 9, 999.\\n[2] W. Boehmer, V. Kurin, and S. Whiteson, Deep coordination graphs, Proceedings of the 37th International Conference on\\nMachine Learning, ICML 2020, 13-18 July 2020, Virtual Event, Proceedings of Machine Learning Research, vol. 119,\\nPMLR, 980\\u2013991.\\n[3] Q. Cai, Z. Yang, and Z. Wang, Reinforcement learning from partial observation: Linear function approximation with\\nprovable sample efficiency, International Conference on Machine Learning, PMLR, 2485\\u20132522.\\n[4] J. Fan et al., A theoretical analysis of deep q-learning, Learning for Dynamics and Control, PMLR, 486\\u2013489.\\n[5] B. Fang et al., Large-scale multi-agent reinforcement learning based on weighted mean field, Cognitive Systems and Signal\\nProcessing - 5th International Conference, ICCSIP 2020, Zhuhai, China, December 25-27, 2020, Revised Selected Papers,\\nCommunications in Computer and Information Science, vol. 1397, Springer, 309\\u2013316.\\n[6] H. Gu et al., Mean-field multi-agent reinforcement learning: A decentralized network approach, arXiv preprint\\narXiv:2108.02731 (2021).\\n[7] J. Gu et al., A metaverse-based teaching building evacuation training system with deep reinforcement learning, IEEE\\nTransactions on Systems, Man, and Cybernetics: Systems (2023).\\n[8] Q. Hao, Very large scale multi-agent reinforcement learning with graph attention mean field, https://openreview.net/forum?\\nid=MdiVU9lMmVS (2023).\\n[9] K. He, P. Doshi, and B. Banerjee, Reinforcement learning in many-agent settings under partial observability, The 38th\\nConference on Uncertainty in Artificial Intelligence.\\n[10] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, A survey and critique of multiagent deep reinforcement learning,\\nAutonomous Agents and Multi-Agent Systems 6 (2019), no. 33, 750\\u2013797.\\n[11] M. Jaderberg et al., Human-level performance in first-person multiplayer games with population-based deep reinforcement\\nlearning, ArXiv abs/1807.01281 (2018).\\n18\\n[12] M. Lauri\\u00e8re et al., Learning mean field games: A survey, arXiv preprint arXiv:2205.12944 (2022).\\n[13] S. Li et al., Deep implicit coordination graphs for multi-agent reinforcement learning, AAMAS \\u201921: 20th International\\nConference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021, ACM, 764\\u2013\\n772.\\n[14] M. L. Littman, Markov games as a framework for multi-agent reinforcement learning, Machine learning proceedings 1994,\\nElsevier, 1994. 157\\u2013163.\\n[15] R. Lowe et al., Multi-agent actor-critic for mixed cooperative-competitive environments, Advances in Neural Information\\nProcessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA, 6379\\u20136390.\\n[16] A. Mahajan et al., Maven: Multi-agent variational exploration, Advances in Neural Information Processing Systems 32\\n(2019), 7611\\u20137622.\\n[17] Y. Niu, R. R. Paleja, and M. C. Gombolay, Multi-agent graph-attention communication and teaming, AAMAS \\u201921: 20th\\nInternational Conference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021,\\nACM, 964\\u2013973.\\n[18] F. A. Oliehoek and C. Amato, A concise introduction to decentralized POMDPs, Springer, 2016.\\n[19] T. Rashid et al., QMIX: monotonic value function factorisation for deep multi-agent reinforcement learning, Proceedings of\\nthe 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\\u00e4ssan, Stockholm, Sweden, July 10-15,\\n2018, Proceedings of Machine Learning Research, vol. 80, PMLR, 4292\\u20134301.\\n[20] J. Ruan et al., Gcs: Graph-based coordination strategy for multi-agent reinforcement learning, arXiv preprint\\narXiv:2201.06257 (2022).\\n[21] L. M. Schmidt et al., An introduction to multi-agent reinforcement learning and review of its application to autonomous\\nmobility, arXiv preprint arXiv:2203.07676 (2022).\\n[22] H. Shi et al., Marl sim2real transfer: Merging physical reality with digital virtuality in metaverse, IEEE Transactions on\\nSystems, Man, and Cybernetics: Systems (2022).\\n[23] S. G. Subramanian et al., Partially observable mean field reinforcement learning, AAMAS \\u201921: 20th International\\nConference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021, ACM,\\n537\\u2013545.\\n[24] J. N. Tsitsiklis, Asynchronous stochastic approximation and q-learning, Machine learning 16 (1994), no. 3, 185\\u2013202.\\n[25] P. Velickovic et al., Graph attention networks, stat 1050 (2017), 20.\\n[26] O. Vinyals et al., Grandmaster level in starcraft ii using multi-agent reinforcement learning, Nature 575 (2019), no. 7782,\\n350\\u2013354.\\n[27] L. Wang et al., Neural policy gradient methods: Global optimality and rates of convergence, arXiv preprint\\narXiv:1909.01150 (2019).\\n[28] T. Wu et al., Weighted mean-field multi-agent reinforcement learning via reward attribution decomposition, International\\nConference on Database Systems for Advanced Applications, Springer, 301\\u2013316.\\n[29] Z. Wu et al., A comprehensive survey on graph neural networks, IEEE transactions on neural networks and learning systems\\n32 (2020), no. 1, 4\\u201324.\\n[30] Q. Xie et al., Learning while playing in mean-field games: Convergence and optimality, International Conference on\\nMachine Learning, PMLR, 11436\\u201311447.\\n19\\n[31] Y. Yang et al., Mean field multi-agent reinforcement learning, Proceedings of the 35th International Conference on\\nMachine Learning, ICML 2018, Stockholmsm\\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, Proceedings of Machine\\nLearning Research, vol. 80, PMLR, 5567\\u20135576.\\n[32] H. Zhang et al., H2gnn: Hierarchical-hops graph neural networks for multi-robot exploration in unknown environments,\\nIEEE Robotics and Automation Letters 7 (2022), no. 2, 3435\\u20133442.\\n[33] K. Zhang, Z. Yang, and T. Ba\\u015far, Multi-agent reinforcement learning: A selective overview of theories and algorithms,\\nHandbook of Reinforcement Learning and Control (2021), 321\\u2013384.\\n[34] T. Zhang et al., MFVFD: A multi-agent q-learning approach to cooperative and non-cooperative tasks, Proceedings of the\\nThirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27\\nAugust 2021, ijcai.org, 500\\u2013506, .\\n[35] Y. Zhang et al., Coordination between individual agents in multi-agent reinforcement learning, Thirty-Fifth AAAI Confer-\\nence on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence,\\nIAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event,\\nFebruary 2-9, 2021, AAAI Press, 11387\\u201311394.\\n[36] L. Zheng et al., Magent: A many-agent reinforcement learning platform for artificial collective intelligence, Proceedings\\nof the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial\\nIntelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New\\nOrleans, Louisiana, USA, February 2-7, 2018, AAAI Press, 8222\\u20138223.\\n[37] S. Zhou et al., Multi-agent mean field predict reinforcement learning, 2020 IEEE International Conference on Advances\\nin Electrical Engineering and Computer Applications (AEECA), IEEE, 625\\u2013629.\\n[38] Z. Zhou and G. Liu, Romfac: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on\\nstates, arXiv preprint arXiv:2205.07229 (2022).\\nHow to cite this article: Yang M, Liu G, Zhou Z, Wang J. Partially Observable Mean Field Multi-\\nAgent Reinforcement Learning Based on Graph Attention Network for UAV Swarms. Drones. 2023; 7(7):476.\\nhttps://doi.org/10.3390/drones7070476\\n\",\n          \"Cooperative Heterogeneous Deep Reinforcement\\nLearning\\nHan Zheng\\nAAII,University of Technology Sydney\\nHan.Zheng-1@student.uts.edu.au\\nPengfei Wei\\nNational University of Singapore\\nwpf89928@gmail.com\\nJing Jiang\\nAAII,University of Technology Sydney\\njing.jiang@uts.edu.au\\nGuodong Long\\nAAII,University of Technology Sydney\\nguodong.long@uts.edu.au\\nQinghua Lu\\nData61, CSIRO\\nqinghua.lu@data61.csiro.au\\nChengqi Zhang\\nAAII,University of Technology Sydney\\nChengqi.Zhang@uts.edu.au\\nAbstract\\nNumerous deep reinforcement learning agents have been proposed, and each\\nof them has its strengths and \\ufb02aws. In this work, we present a Cooperative\\nHeterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn\\na policy by integrating the advantages of heterogeneous agents. Speci\\ufb01cally, we\\npropose a cooperative learning framework that classi\\ufb01es heterogeneous agents into\\ntwo classes: global agents and local agents. Global agents are off-policy agents\\nthat can utilize experiences from the other agents. Local agents are either on-policy\\nagents or population-based evolutionary algorithms (EAs) agents that can explore\\nthe local area effectively. We employ global agents, which are sample-ef\\ufb01cient,\\nto guide the learning of local agents so that local agents can bene\\ufb01t from sample-\\nef\\ufb01cient agents and simultaneously maintain their advantages, e.g., stability. Global\\nagents also bene\\ufb01t from effective local searches. Experimental studies on a range of\\ncontinuous control tasks from the Mujoco benchmark show that CHDRL achieves\\nbetter performance compared with state-of-the-art baselines.\\n1\\nIntroduction\\nDeep reinforcement learning (DRL) integrates deep neural networks with reinforcement learning\\nprinciples, e.g.,Q-learning and policy-gradient, to create a more ef\\ufb01cient agent. Recent studies have\\nshown a great success of DRL in numerous challenging real-world problems, e.g., video games and\\nrobotic control [1]. Although promising, existing DRL algorithms still suffer from several challenges\\nincluding sample complexity, instability, and temporal credit assignment problems [2, 3].\\nOne popular research line of DRL is policy-gradient based on-policy methods attempting to evaluate\\nor improve the same policy that is used to make decisions [4], e.g., trust region policy optimization\\n(TRPO) [5] and proximal policy optimization (PPO) [6]. Recent works [7, 8] have proved that\\npolicy-gradient based methods can converge to a stationary point under some conditions, which\\ntheoretically guarantees their stability. However, they are extremely sample-expensive since they\\nrequire new samples to be collected in each gradient step [4].\\nOn the contrary, Q-learning based off-policy methods, which is another research line evaluating or\\nimproving a policy different from the one that is used to generate the behavior, can improve sample\\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\\narXiv:2011.00791v1  [cs.LG]  2 Nov 2020\\nFigure 1: The high-level structure of CHDRL for one iteration\\nef\\ufb01ciency by reusing past experiences [4]. Existing off-policy based methods include deep Q-learning\\nnetwork (DQN) [1] and Soft Actor-Critic (SAC) [9] etc. These methods involve the approximation of\\nsome high-dimensional and nonlinear functions, usually through deep neural networks, which poses\\na signi\\ufb01cant challenge on convergence and stability [10, 3]. It is also well known that off-policy Q\\nlearning is not to converge even with linear function approximation [11]. Moreover, recent studies\\n[12, 13] identify some other key sources of instability for off-policy methods, i.e., bootstrapping and\\nextrapolation errors. As shown in [12], off-policy methods are highly sensitive to data distribution,\\nand can only make limited progress without exploiting additional on-policy data.\\nIn addition to the pros and cons discussed above, on-policy and off-policy methods based on temporal\\ndifference learning suffer from some common issues. The one that received much research attention\\nis the so-called temporal credit assignment problem [2]. When rewards become sparse or delayed,\\nwhich is quite common in real-world problems, DRL algorithms may yield an inferior performance\\nas reward sparsity downgrades the learning ef\\ufb01ciency and hinders exploration. To alleviate this issue,\\nevolutionary algorithms (EAs) [14, 15] have recently been introduced to DRL [16, 17]. The usage of\\na \\ufb01tness metric that consolidates returns across the entire episode makes EAs indifferent to reward\\nsparsity and robust to long time horizons [18]. However, EAs suffer from high sample complexity\\nand struggle to solve high-dimension problems involving massive parameters.\\nIn this paper, we are interested in an algorithm that takes the essence and discards the dross of\\ndifferent DRL algorithms to achieve high sample ef\\ufb01ciency and maintain good stability in various\\ncontinuous control tasks. To do so, we propose a framework called CHDRL. Speci\\ufb01cally, CHDRL\\nworks on an agent pool containing three classes of agents: an off-policy agent, an on-policy agent,\\nand a opulation-based EAs agent. All the agents cooperate based on the following three mechanisms.\\nFirstly, all agents collaboratively explore the solution space following a hierarchical policy transfer\\nrule. As the off-policy agent is sample-ef\\ufb01cient, we take it as the global agent to obtain a relatively\\ngood policy or value function at the beginning. The on-policy agent and the population-based EAs\\nagent are taken as local agents and start their exploration with the prior knowledge transferred from\\nthe global agent. As the EAs agent is population-based, we further allow it to accept policies from\\nthe on-policy agent.\\nSecondly, we employ a local-global memory replay to enable global (off-policy) agents to replay the\\nnewly generated experiences by local (on-policy) agents more frequently so that global agents can\\nbene\\ufb01t from local search. Note that, with policy transfer as stated above, local agents start exploration\\nwith a policy transferred from global agents, and thus their generated experiences can be taken as\\nclose to the on-policy data of global agents\\u2019 current policy [12, 13]. By allowing global agents\\nexploits more often from these local experiences, we can alleviate the bootstrapping or extrapolation\\nerror and further boost global agents\\u2019 learning. Consequently, global agents provide a better starting\\npoint for local agents who in turn generate more diverse local experiences for global agents\\u2019 replay,\\nwhich forms a good win-win cycle.\\nThirdly, although we encourage the cooperation among agents in exploration, we also tend to\\nmaintain the independence of each agent; that is, we do not want the learning of local agents to be\\ncompletely dominated by that of global agents. This is to enable each agent to still maintain its policy\\nupdating scheme and preserve its learning advantage. To do so, we \\ufb01rstly develop a loosely coupled\\nhierarchical framework with global agents at the upper-level and local agents at the lower-level1.\\nSuch a framework not only makes each agent generally run in a relatively independent environment\\n1Policy transfer only happens from upper-level agents to lower-level agents.\\n2\\nwith different random settings, but also achieves the easy and \\ufb02exible deployment or replacement\\nof the agent candidates used in the framework. Secondly, to avoid over-policy-transfer, i.e., policy\\ntransfer happening too frequently thus interrupting the learning stability of local agents, we set a\\nthreshold to control the frequency of policy transfer.\\nThe high-level structure of CHDRL is shown in Figure 1. In this work, we instantiated a CHDRL\\nwith PPO, SAC, and Cross-Entropy-Method (CEM) based EA [19], named CPSC. Experimental\\nstudies showed the superiority of CPSC to several state-of-the-art baselines in a range of continuous\\ncontrol benchmarks. We also conducted ablation studies to verify the three mechanisms.\\n2\\nPreliminaries\\nIn this section, we review the representation of on-policy method, off-policy method, and EAs,\\nnamely, PPO [6], SAC [9], and Cross-Entropy based EA [19].\\n2.1\\nProximal Policy Optimization (PPO)\\nPPO is an on-policy algorithm that trains a stochastic policy. It explores by sampling actions\\naccording to the latest version of its stochastic policy. During training, the policy typically becomes\\nprogressively less random, as the update rule encourages it to exploit rewards that it has already\\nfound. PPO tries to keep new policies close to old.\\n2.2\\nSoft Actor-critic (SAC)\\nSAC is an off-policy algorithm that incorporates an entropy measure of the policy into the reward\\nto encourage exploration. The idea is to learn a policy that acts as randomly as possible while still\\nbeing able to succeed in the task. It is an off-policy actor-critic model that follows the maximum\\nentropy RL framework. The policy is trained with the objective of maximizing the expected return\\nand entropy at the same time.\\n2.3\\nEvolutionary Algorithms and CEM-ES\\nEAs [14, 15] are a class of black-box search algorithms that apply heuristic search procedures inspired\\nby natural evolution. Among EAs, Estimation of Distribution Algorithms (EDAs) are a speci\\ufb01c\\nfamily where the population is represented as a distribution using a covariance matrix [20]. CEM is a\\nsimple EDA where the number of elite individuals is \\ufb01xed at a certain value. After all individuals of\\na population are evaluated, the top \\ufb01ttest individuals are used to compute the new mean and variance\\nof the population.\\n3\\nRelated Works\\nExperience replay mechanism [21] is widely used in off-policy reinforcement learning to improve\\nsample ef\\ufb01ciency. DQN [1] randomly and uniformly samples experience from a replay memory. [22]\\nsubsequently expands DQN to develop a prioritized experience replay (PER), which uses a temporal\\ndifference error to prioritize experiences. Zhizheng Zhang et al. [23] introduce an episodic control\\nexperience replay method to quickly latch on to good trajectories. Our local-global memory uses a\\ndifferent strategy: let the off-policy agent learn more from effective on-policy experiences.\\nCHDRL\\u2019s cooperative learning mechanism can be discussed in terms of guided policy search (GPS)\\n[24, 25] or evolutionary reinforcement learning (ERL) [16, 17, 26]. For GPS, they generally need to\\nuse the KL divergence to guide how policies are improved. ERL [17] directly transfers the RL agent\\u2019s\\npolicy to the EA population, while Pourchot et al. [16] uses the RL\\u2019s critic to update half of the EA\\npopulation using the gradient-based technique. The proposed CHDRL is related to GPS and ERL in\\nthe sense that multiple polices work in a hybrid way. However, the main difference between CHDRL\\nand other similar methods is how heterogeneous agents cooperate. Moreover, CHDRL can bene\\ufb01t\\nnot just from off-policy and EA learning schemes but also from the on-policy learning scheme.\\nAnother related area of work is in the training architectures. A3C [27] introduce an asynchronous\\ntraining framework for deep reinforcement learning, showing parallel actor-learners have a stabilizing\\n3\\neffect on training. Babaeizadeh et al. [28] adapt this approach to make ef\\ufb01cient use of GPUs.\\nIMPALA [29] uses a central learner to run SGD while asynchronously pulling sample batches from\\nmany actor processes. Horgan et al. [30] proposes a distributed architecture for training DRL that\\nemploys many actors to explore using different policies and prioritizing the generated experiences.\\nHan Zheng et al. [31] introduces a training method to select the best agent for different tasks. All\\nthese methods only focus on one learning scheme, and/or all actors involved are treated equally. On\\nthe contrary, CHDRL distinguishes actors as global actors and local actors that serve for different\\npurposes respectively. Moreover, CHDRL focuses on the cooperation of diverse learning schemes.\\n4\\nCooperative Heterogeneous Deep Reinforcement Learning(CHDRL)\\nIn this section, we \\ufb01rstly introduce the proposed CHDRL framework and then suggest a practical\\nalgorithm based on it. Our CHDRL mainly follows three mechanisms to achieve cooperative learning\\nof heterogeneous agents: cooperative exploration (CE), local-global memory relay (LGM) and\\ndistinctive update (DU).\\nCooperative Exploration (CE). The key idea of CE is to utilize a sample-ef\\ufb01cient agent, such as\\nan off-policy agent, to guide the exploration of the agent with a relatively lower sample ef\\ufb01ciency,\\ne.g., an on-policy agent. This is done by transferring policies across agents. More precisely, the\\nsample-ef\\ufb01cient agent acts as a global agent and conducts a global search \\ufb01rst. In every iteration, we\\nwant to use the policy and/or value function obtained by the global agent as the prior knowledge to\\nre-initialize local agents so that they can start to exploit from a relatively better position. To do so, we\\nneed to solve three key points: what to transfer, how to transfer, and when to transfer, following the\\nbasic mechanism of transfer learning[32, 33].\\nWhat to Transfer. Different agents may have different policy architectures. The policy could be\\ndeterministic, where it is denoted by a .= \\u00b5\\u03c6(s), or stochastic, where it is denoted by a \\u223c\\u03c0\\u03c6(\\u00b7|s). In\\ncontinuous control tasks, the stochastic policy is usually assumed to be sampled from a Gaussian\\ndistribution, and thus it can be represented as:\\na .= \\u00b5\\u03c6(s) + \\u03a3\\nwhere \\u00b5\\u03c6(s) is the mean action, \\u03a3 represent a covariance matrix. Typically, \\u03a3 may have different\\nforms, e.g., PPO uses a state-independent \\u03a3 while SAC utilizes a state-dependent one. However, a\\nsimilar mean policy architecture \\u00b5\\u03c6(s) is used in different methods. Inspired by this, we propose to\\nuse the structurally identical mean function \\u00b5\\u03c6(s) to establish a link between the deterministic and\\nstochastic policies. Then the policy among heterogeneous agents can be shared by transferring \\u00b5\\u03c6(s).\\nHow to Transfer. As shown in Figure 1, policies are transferred following a hierarchical manner. The\\nprinciple is that policies are transferred from upper-level agents with higher sample ef\\ufb01ciency to the\\nlower-level agents with lower sample ef\\ufb01ciency. More speci\\ufb01cally, policies are transferred (1) from\\noff-policy agents to both on-policy agents and EAs agents, and (2) from on-policy agents to EAs\\nagents. Note that EAs agents are population-based, and thus we allow them to accept the on-policy\\nagent\\u2019s policy to maximize the transfer capacity. To avoid collisions, we use different individuals of\\nEAs\\u2019 population to accept policies from different upper-level agents. As EAs agents accept policies\\nfrom both off-policy and on-policy agents, they naturally serve as a pool that stores all the transferred\\npolicies.\\nWhen to Transfer. Policy transfer happens only when upper-level agents \\ufb01nd a better policy than\\nthe current one of lower-level agents. Lower-level agents then re-initialize the exploration with the\\npolicy transferred from their upper-level agents as the new starting point. In order to compare the\\nperformance of policies, we use the average return as the evaluation metric. To be statistically stable,\\nwe use the average return over \\ufb01ve episodes as the policy\\u2019s performance score. Moreover, to avoid\\nthat policy transfer happens too frequently to interrupt the learning stability of lower-level agents, we\\nenable policy transfer only when the performance gap is larger than a prede\\ufb01ned threshold.\\nLocal-Global Memory Relay (LGM): Off-policy agents can make more progress when considering\\non-policy data in their learning [12, 13]. Following this observation, we employ a local-global memory\\nreplay mechanism to enable global off-policy agents to bene\\ufb01t from diverse local experiences from\\nboth on-policy agents and EAs agents. In particular, we propose two memory buffers \\u2013 a global\\none and a local one \\u2013 to store the generated exploration experiences. The global memory serves to\\nstore the entire exploration experiences of all the agents, while the local memory only stores the\\n4\\nAlgorithm 1 CSPC\\nRequire:\\nGs with policy \\u03c0s .= \\u00b5\\u03c6s(s) + \\u03a3s and value \\u03c8s;\\nLp with \\u03c0p .= \\u00b5\\u03c6p(s) + \\u03a3p and value \\u03c8p; local\\nmemory Ml,global memory Mg; Iteration steps T;\\nLc with policies as \\u00b5\\u03c6c0 (s), ..., \\u00b5\\u03c6cn (s); initial\\nsteps Tg; gap f, terminate step Tm, and initial test\\nscore Ss, Sp Sc. Initialize transfer label Ap, Ac to\\nFalse.\\n1: repeat\\n2:\\nTRAIN(Gs, Tg), t \\u2190t + Tg\\n3:\\nfor Agent a in Gs, Lp, Lc do\\n4:\\nTRAIN(a, Ml, Mg, T)\\n5:\\nif a is not Gs then\\n6:\\nUPDATE(\\u03c6s, Ml, Mg, T)\\n7:\\nend if\\n8:\\nt \\u2190t + T\\n9:\\nend for\\n10:\\nUpdate test scores Ss, Sp and Sc\\n11:\\nif Ss \\u2212Sp > f then\\n12:\\n\\u03c6p \\u2190\\u03c6s, \\u03c8p \\u2190\\u03c8s, Ap \\u2190True\\n13:\\nend if\\n14:\\nif Ss \\u2212Sc > f then\\n15:\\n\\u03c6c0 \\u2190\\u03c6s, Ac \\u2190True\\n16:\\nend if\\n17:\\nif Sp \\u2212Sc > f then\\n18:\\n\\u03c6c1 \\u2190\\u03c6p\\n19:\\nend if\\n20: until t > Tm\\nAlgorithm 2 TRAIN\\nRequire:\\nInput agent a,\\ntraining steps Ta,\\nepisode reward R = 0,\\nRm \\u2190min(Ss, Sp, Sc),\\nstep t = 0, te = 0,\\nglobal memory Mg,local memory Ml\\nepisode memory Me.\\n1: repeat\\n2:\\nObserve state s and select action a \\u223c\\u00b5\\u03c6s(s) +\\n\\u03a3s or a \\u223c\\u00b5\\u03c6p(s) + \\u03a3p or a .= \\u00b5\\u03c6ci (s)\\n3:\\nExecute a in the environment\\n4:\\nObserve next state s\\u2032,reward r,and done signal\\nd\\n5:\\nStore (s, a, r, s\\u2032, d) in Me, R \\u2190r + R\\n6:\\nt \\u2190t + 1, te \\u2190te + 1\\n7:\\nif s\\u2032 is terminal then\\n8:\\n\\u03c6\\u2032 \\u2190UPDATE(\\u03c6, Mg, Ml, te) where \\u03c6 \\u2208\\n{\\u03c6s, \\u03c6p, \\u03c6c}\\n9:\\nif R > Rm and (a is Gs or Ap or Ac is True)\\nthen\\n10:\\nStore Me in Ml and Mg\\n11:\\nelse\\n12:\\nStore Me in Mg\\n13:\\nend if\\n14:\\nR \\u21900, Me \\u2190[], te \\u21900\\n15:\\nend if\\n16: until t > Ta\\nrecently generated ones. Thus, we set an expandable global memory size increasing while learning,\\nbut a \\ufb01xed shared memory size with a \\ufb01rst-in-\\ufb01rst-out rule. Whenever new experiences arrive, the\\nearliest saved experiences in local memory are overridden. We aim to use the experience saved in the\\nlocal memory to simulate on-policy data. However, instead of exploiting a brute-force storage that\\nindiscriminately saves every new episode experience, we set an intuitive rule to determine whether to\\nstore an experience in local memory or not. Speci\\ufb01cally, we only save a newly generated episode\\nfrom a local agent when (1) the local agent successfully accepts a policy from the global agent 2, and\\n(2) when its episode return is not worse than the minimum of all agents\\u2019 current performance. By\\ndoing so, we can avoid out-of-distribution data being saved in local memory to some extent, so as\\nto reduce variance and stabilize learning [12]. We then allow global agents to replay experiences\\nfrom the two memories drawn from a Bernoulli distribution, that is, sample experiences from the\\nlocal memory with a probability p, and from the global memory with a probability 1 \\u2212p. Such a\\nLocal-Global Memory Relay mechanism plays a very important role in guaranteeing global agents\\nto consistently bene\\ufb01t from on-policy data as, if only a single global memory buffer is used, the\\nprobability of sampling a newly generated experience in it becomes lower and lower with more and\\nmore experiences saved alongside learning.\\nDistinctive Update (DU): Although global agents guide local agents for exploration, each agent still\\nmaintains its own policy updating schemes to preserve learning advantages. When an agent accepts a\\npolicy from its upper-level agent, it keeps updating using its update algorithms, e.g., policy gradient,\\nstarting from the accepted policy. This is naturally achieved by the hierarchical framework stated\\nabove as well as by the performance gap determining when to transfer.\\nTo understand CHDRL better, we provide a CHDRL instantiation, which employs a state-of-the-art\\noff-policy agent SAC, an on-policy agent PPO and EAs agent CEM, called Cooperative SAC-PPO-\\nCEM (CSPC). The pseudo code of the instantiated CSPC is presented in detail in algorithms 1 to 3.\\nGs, Lp, Lc represent global off-policy agent SAC, local on-policy agent PPO and EA agent CEM\\nrespectively. Algorithm 1 shows the general learning \\ufb02ow of CSPC. Firstly, global agent Gs is\\n2It ensures the local experiences are close to the on-policy data of the global agent\\u2019s current policy.\\n5\\nAlgorithm 3 UPDATE\\nRequire:\\nAgent a\\u03c6, update steps tu, step t = 0, sample probability p; Global shared memory Mg, local memory Ml;\\n1: if a is G then\\n2:\\nwhile t < tu do\\n3:\\no \\u2190Bernoulli(k, p) with k \\u2208{0, 1}\\n4:\\nif o = 1 then\\n5:\\nRandomly sample a batch B from Ml\\n6:\\nelse\\n7:\\nRandomly sample a batch B from Mg\\n8:\\nend if\\n9:\\nUpdate agent\\u2019s policy \\u03c6s and value function \\u03c8s following [6]\\n10:\\nt \\u2190t + 1\\n11:\\nend while\\n12: end if\\n13: if a is Lp then\\n14:\\nUpdate agent\\u2019s policy \\u03c6p and value function \\u03c8p following [9].\\n15: end if\\n16: if a is Lc then\\n17:\\nUpdate agent\\u2019s new mean \\u03c0\\u00b5c and covariance matrix P\\nc following [19].\\n18:\\nDraw the current population Lc from N(\\u03c0\\u00b5c, \\u03a3c),\\n19: end if\\ntrained for speci\\ufb01c steps Tg. This is to ensure the off-policy agent reaches a relatively good solution.\\nAfterwards, we orderly train Gs, Lp, and Lc to search the solution space for one iteration step T.\\nNote that Gs keeps learning from the experiences when other agents explore. After that, we evaluate\\nthe updated agent to get its new policy score Ss, Sp and Sc. We then transfer policies based on these\\nupdated scores following the above principle of policy transfer. Speci\\ufb01cally, if the score of Ss is\\nbetter than those of Sp and Sc with at least f improvement, we re-initialize Lp and one individual of\\nLc with Gs\\u2019s policy. A similar transfer is done from Lp to Lc.\\nAlgorithm 1 shows what, how, and when to transfer policies, which are the three key factors in CE.\\nLines 9-13 in Algorithm 2 show how generated experiences are stored in global memory or local\\nmemory. Lines 3-8 in Algorithm 3 show how global agents replay experiences from the global and\\nlocal memories. These lines combined consist of the implementation of LGM. Lastly, lines 9, 14,\\nand 17 re\\ufb02ect DU, where each agent updates following its own update rules. The above procedure\\nproceeds iteratively until termination.\\nNote that CHDRL also accepts the same type of agents. In this case, cooperation only exists between\\nthe global agent and local agent, not across local ones. In the ablation study, we test a case where\\nthree off-policy agents are used in CHDRL. Moreover, our CHDRL is loosely coupled in the sense\\nthat it is \\ufb02exible enough to involve any other agents, e.g., DQN [1] and TRPO [5] etc., into it.\\n5\\nExperiments\\nWe conducted an empirical evaluation to verify the performance superiority of CSPC to other\\nbaselines, and ablation studies to show the effectiveness of each mechanism used in CHDRL.\\n5.1\\nExperiment Setup\\nAll the evaluations were done on a continuous control benchmark: Mujoco [34]. We used state-of-the-\\nart SAC, PPO and CEM to represent the off-policy agent, on-policy agent, and EA, respectively. Note\\nthat other off-policy (e.g., TD3), on-policy (e.g., TRPO) and gradient-free agents (e.g. CEM-ES), are\\napplicable to our framework. For SAC, PPO and CEM, we used the code from OpenAISpinningUp\\nfor the \\ufb01rst two, and code from CEM-RL for CEM 3. For hyper-parameters in these methods, we\\nfollowed the defaults speci\\ufb01ed by the authors. For CSPC, we set the gap f as 100, global agent initial\\nlearning steps Tg as 5e4, iteration time steps T as 1e4, global memory size Mg as 1e6, local memory\\nsize Ml as 2e4, and sample probability from local memory p as 0.3.\\n3OpenAISpinningUp: github.com/openai/spinningup; CEM-RL:github.com/apourchot/CEM-RL\\n6\\n(a) Hopper-v2\\n(b) Walker2d-v2\\n(c) Ant-v2\\n(d) Humanoid-v2\\n(e) Swimmer-v2\\nFigure 2: Training curves on Mujoco continuous control tasks.\\nTable 1: The max average return.\\nTask\\nCSPC\\nPPO\\nSAC\\nCEM\\nHumanoid-v2\\n5412\\u00b1239\\n626\\u00b123\\n5142\\u00b1133\\n616\\u00b188\\nAnt-v2\\n5337\\u00b1220\\n1169\\u00b1207\\n3766\\u00b12359\\n1019\\u00b133\\nWalker2d-v2\\n5317\\u00b1256\\n1389\\u00b1387\\n4222\\u00b1290\\n1041\\u00b165\\nHopper-v2\\n3619\\u00b152\\n2923\\u00b188\\n3558 \\u00b1139\\n1057\\u00b153\\nSwimmer-v2\\n261\\u00b1117\\n68\\u00b131\\n44\\u00b13\\n274\\u00b1118\\nTable 2: The elite agent.\\nTask\\nHumanoid-v2\\nAnt-v2\\nWalker2d-v2\\nHopper-v2\\nSwimmer-v2\\nseed 0\\nSAC\\nSAC\\nPPO\\nCEM\\nCEM\\nseed 1\\nCEM\\nSAC\\nCEM\\nCEM\\nCEM\\nseed 2\\nPPO\\nCEM\\nCEM\\nCEM\\nCEM\\nseed 3\\nPPO\\nCEM\\nCEM\\nSAC\\nCEM\\nseed 4\\nSAC\\nPPO\\nCEM\\nCEM\\nCEM\\n5.2\\nComparative Evaluation\\nWe evaluated CSPC on \\ufb01ve continuous control tasks from Mujoco in comparison to three baselines:\\nSAC, PPO, and CEM. We also used SAC, CEM, and PPO as our candidate agents in CSPC. We ran\\nthe training process for all the methods over one million time steps on four tasks with \\ufb01ve different\\nseeds, and for the Swimmer-v2 task, we ran it for four million time steps. Time steps are accumulated\\ninteraction steps with the environment. For a fair comparison, we used the accumulated time steps\\nof three algorithms used in CSPC. Speci\\ufb01cally, we summed up each agent\\u2019s time steps so that the\\ntotal time-steps stayed consistent with the other baselines. The \\ufb01nal performance was reported as the\\nmax average return of 5 independent trials for each seed. We reported the scores of all the methods\\ncompared against the number of time steps.\\nFigure 2 shows the comparison results for all methods on \\ufb01ve Mujoco learning tasks. From the results,\\nwe \\ufb01rst observe that there is no clear winner among the existing state-of-the-art baselines SAC, PPO,\\nand CEM in terms of stability and sample ef\\ufb01ciency. No one consistently outperforms the others on\\nthe \\ufb01ve learning tasks. Speci\\ufb01cally, it can be seen that, for four of \\ufb01ve tasks (except for Swimmer\\ntask), SAC yields better results than PPO and CEM, which veri\\ufb01es its sample ef\\ufb01ciency for a long\\nrun. However, we can also observe a signi\\ufb01cant variance of SAC, which indicates its high instability,\\nespecially in Ant task. In contrast, PPO and CEM have a lower variance but achieve unsatisfactory\\naverage returns. A special case is Swimmer task where both SAC and PPO fail to learn a good policy\\nbut CEM succeeds. Figure 2 also demonstrates that our proposed CSPC performs consistently better\\nor with comparable results to the best baseline methods on all tasks. This veri\\ufb01es the capability\\nof CSPC to improve the performance of each individual agent by utilizing the cooperation among\\nthem. On Swimmer task where both gradient-based methods fail, CSPC still achieves a comparable\\nresult with CEM. This is because CSPC does not bene\\ufb01t from SAC and PPO, and only maintains the\\ncapacity of CEM. Table 1 shows the maximum average return for each method.\\nOne may wonder about the possible computation cost of CPSC. In our experiments, it mainly comes\\nfrom the global agent, as it keeps learning for other agents\\u2019 experiences in the background. The local\\nagents run much faster than global agents, especially the CEM agent, as it is gradient-free. The total\\nrunning time of CSPC is only slightly longer than the SAC agent.\\n7\\n(a) Walker2d-v2\\n(b) Walker2d-v2\\n(c) Walker2d-v2\\n(d) Swimmer-v2\\n(e) Swimmer-v2\\n(f) Swimmer-v2\\nFigure 3: Ablation study on two tasks: Walker2d and Swimmer.\\n5.3\\nLocal Agent vs Global Agent\\nThe main motivation of this study is to \\ufb01gure out whether local agents really help in \\ufb01nding the best\\n\\ufb01nal policy in different random settings. To do so, we show the elite agent, that is, the agent yielding\\nthe best performance among heterogenous agents after training has terminated, in different random\\nseeds. The results are shown in Table 2. It can be seen that CSPC could obtain different elite agents\\non the same task under different random seeds. Such an observation indicates that local search agents\\ndo help to \\ufb01nd a better policy around the global guided agent. Surprisingly, the EA-based CEM\\nagent performs better than other local agent (PPO) in most cases. However, on the complex task,\\nHumanoid-v2, the gradient-based agents perform much better than CEM.\\n5.4\\nAblation Studies\\nIn this section, we conducted ablation studies to understand the contributions of each key component\\nof CSPC. To do this, we built three variants of CSPC: CSPC without cooperative exploration (CE),\\ni.e., CSPC-CE, CSPC without local memory(LM), i.e., CSPC-LM, and CSPC without global memory\\n(GM), i.e., CSPC-GM. Speci\\ufb01cally, in CSPC-CE, we stopped the policy transfer and let each agent\\nexplore and exploit by itself. In CSPC-LM, the off-policy agent SAC replays from all experiences\\nuniformly. In CSPC-GM, the off-policy agent SAC only learns from its own experiences. We further\\nanalyzed the in\\ufb02uence of each individual agent to CSPC. To do so, we developed a CSPC without\\nPPO, called CSPC-PPO, a CSPC without CEM, called CSPC-CEM, and a CSPC without SAC, called\\nCSPC-SAC. As CHDRL also allows the same types of agents, to verify that heterogeneous agents\\nindeed matters, a variant of CHDRL consisting of only one type of agent was proposed. In this case,\\nwe introduced two variants: three SAC agents with CE and LGM, and three SAC agents without\\nthem. We called the former C3SAC and the latter 3SAC. For 3SAC, the three agents only shared\\nglobal memory and no policy transfer existed. We evaluated all the variants on Walker2d-v2 and\\nSwimmer-v2. The results are shown in Figure 3.\\nAs shown in Figure 3, for task Walker2d-v2, CSPC achieved the best among all the ablation variants\\nin terms of \\ufb01nal average performance. From Figure 3(a), it is easy to deduce that LGM and CE\\nindeed matter in CSPC, as without these two elements, the \\ufb01nal performance drops quickly. From\\nFigure 3(b), we can see that the results of CSPC-PPO and CSPC-CEM are satisfactory and only\\nslightly worse than that of CSPC, while the result of CSPC-SAC dramatically decreases. This implies\\nthat the global agent has a more signi\\ufb01cant impact on the \\ufb01nal performance than local agents. This\\nis reasonable as the global agent determines the starting position of CSPC, and highly affects the\\nfollowing search ef\\ufb01ciency. Note that CSPC-PPO and CSPC-CEM are CSPC without one speci\\ufb01c\\nlocal agent, but still follow CHDRL\\u2019s core mechanism: CE and LGM. From the fact that their\\nperformances are much higher than CSPC-CE and CSPC-LM/GM, we again verify the signi\\ufb01cance\\n8\\nof LGM and CE. From Figure 3(c), we can see that C3SAC performs better than 3SAC and SAC.\\nEven though the three agents are with the same type, local agents still provide a diverse local search\\nas they explore in different random settings. However, our CSPC performs much better than C3SAC,\\nwhile 3SAC performs only slightly better than SAC. With this, we deduce that CHDRL still improves\\nthe performance when using the same type agents, but using heterogeneous agents would further\\nboost the performance.\\nFor the Swimmer-v2 task, the results are different as SAC and PPO agents typically fail on this\\ntask. In other words, the global agent is incapable of \\ufb01nding a relatively good position, and only\\nthe CEM agent works. The most likely explanation is that in Swimmer-v2, existing DRL methods\\nprovide deceptive gradient information that is detrimental to convergence towards ef\\ufb01cient policy\\nparameters [16]. Hence, LM/GM/CL cannot enhance the \\ufb01nal performance, which is shown by\\nCSPC-LM,CSPC-GM and CSPC-CL in Figure 3 (d). In such a case, the learning curves of the\\nthree methods mostly overlap. On the other hand, CSPC-PPO and CSPC-SAC gain a better \\ufb01nal\\nperformance than CSPC, which is also reasonable as the CEM agent has more iterations leading to a\\nbetter \\ufb01nal performance, as shown in Figure 3(e). For the same reason, C3SAC and 3SAC both fail.\\n6\\nConclusion\\nIn this paper, we present CHDRL, a framework that incorporates the bene\\ufb01ts of off-policy agents,\\npolicy gradient on-policy agents and EAs agents. The proposed CHDRL is based on three key\\nmechanisms, i.e., cooperation exploration, local-global memory and distinctive update. We also\\nprovide a practical algorithm CSPC by using SAC, PPO, and CEM. Experiments in a range of\\ncontinuous control tasks show that CSPC achieves a better or comparable performance compared\\nwith baselines. We also note that CHDRL introduces some new hyper-parameters which may have\\na crucial impact on performance, however, we do not tune that too much. Moreover, we should\\ncarefully select the agents, as the \\ufb01nal performance highly depends on the agents used, particularly\\nthe global one.\\nBroader Impact\\nThe DRL agent that learns from an incompletely known environment runs the risk of making wrong\\ndecisions. This could lead to catastrophic consequences in practice, such as automated driving,\\nthe stock market, or medical robots. One approach to alleviate this risk is to combine with other\\ntechniques or involve human beings\\u2019 supervision. In terms of bene\\ufb01ts, DRL can be deployed in a\\nsafe environment where a wrong decision will not lead to a signi\\ufb01cant loss, e.g., the recommendation\\nsystem. Moreover, in some environments that we can simulate well, it would be very promising to\\ndevelop an intelligent robot to work in such an environment.\\nAcknowledgments and Disclosure of Funding\\nThis research is partially funded by the Australian Government through the Australian Research\\nCouncil (ARC) under grant LP180100654.\\nReferences\\n[1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\\n[2] Richard S Sutton. Temporal credit assignment in reinforcement learning. 1985.\\n[3] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David\\nMeger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on\\nArti\\ufb01cial Intelligence, 2018.\\n[4] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2011.\\n[5] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\\npolicy optimization. In International conference on machine learning, pages 1889\\u20131897, 2015.\\n9\\n[6] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n[7] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods:\\nGlobal optimality and rates of convergence. September 2019.\\n[8] Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy\\noptimization attains globally optimal policy. In Advances in Neural Information Processing\\nSystems, pages 10564\\u201310575, 2019.\\n[9] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-\\npolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint\\narXiv:1801.01290, 2018.\\n[10] Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba\\nSzepesv\\u00e1ri. Convergent temporal-difference learning with arbitrary smooth function approxi-\\nmation. In Advances in Neural Information Processing Systems, pages 1204\\u20131212, 2009.\\n[11] Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In\\nMachine Learning Proceedings 1995, pages 30\\u201337. Elsevier, 1995.\\n[12] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy\\nq-learning via bootstrapping error reduction. In Advances in Neural Information Processing\\nSystems, pages 11761\\u201311771, 2019.\\n[13] S Fujimoto, D Meger, and D Precup. Off-policy deep reinforcement learning without exploration.\\narXiv preprint arXiv:1812.02900, 2018.\\n[14] David B Fogel and Evolutionary Computation. Toward a new philosophy of machine intelligence.\\nIEEE Evolutionary Computation, 1995.\\n[15] William M Spears, Kenneth A De Jong, Thomas B\\u00e4ck, David B Fogel, and Hugo De Garis. An\\noverview of evolutionary computation. In European Conference on Machine Learning, pages\\n442\\u2013459. Springer, 1993.\\n[16] Alo\\u00efs Pourchot and Olivier Sigaud. Cem-rl: Combining evolutionary and gradient-based\\nmethods for policy search. In International Conference on Learning Representations, 2018.\\n[17] Shauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement\\nlearning. In Advances in Neural Information Processing Systems, pages 1188\\u20131200, 2018.\\n[18] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies\\nas a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.\\n[19] Freek Stulp and Olivier Sigaud. Path integral policy improvement with covariance matrix\\nadaptation. arXiv preprint arXiv:1206.4621, 2012.\\n[20] Pedro Larra\\u00f1aga and Jose A Lozano. Estimation of distribution algorithms: A new tool for\\nevolutionary computation, volume 2. Springer Science & Business Media, 2001.\\n[21] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and\\nteaching. Machine learning, 8(3-4):293\\u2013321, 1992.\\n[22] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.\\nIn International Conference on Learning Representations, 2016.\\n[23] Zhizheng Zhang, Jiale Chen, Zhibo Chen, and Weiping Li. Asynchronous episodic deep deter-\\nministic policy gradient: Towards continuous control in computationally complex environments.\\narXiv preprint arXiv:1903.00827, 2019.\\n[24] Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine. Divide-and-\\nconquer reinforcement learning. arXiv preprint arXiv:1711.09874, 2017.\\n[25] Whiyoung Jung, Giseung Park, and Youngchul Sung. Population-guided parallel policy search\\nfor reinforcement learning. International Conference on Learning Representations, 2020.\\n[26] Shauharda Khadka, Somdeb Majumdar, Tarek Nassar, Zach Dwiel, Evren Tumer, Santiago\\nMiret, Yinyin Liu, and Kagan Tumer. Collaborative evolutionary reinforcement learning. In\\nInternational Conference on Machine Learning, 2019.\\n[27] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-\\ncrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep\\nreinforcement learning. In International conference on machine learning, pages 1928\\u20131937,\\n2016.\\n10\\n[28] Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, and Jan Kautz. Reinforce-\\nment learning through asynchronous advantage actor-critic on a gpu. International Conference\\non Learning Representations, 2017.\\n[29] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam\\nDoron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA:\\nScalable distributed deep-RL with importance weighted actor-learner architectures. In Jennifer\\nDy and Andreas Krause, editors, Proceedings of the 35th International Conference on Ma-\\nchine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1407\\u20131416,\\nStockholmsm\\u00e4ssan, Stockholm Sweden, 10\\u201315 Jul 2018. PMLR.\\n[30] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Has-\\nselt, and David Silver. Distributed prioritized experience replay. In International Conference on\\nLearning Representations, 2018.\\n[31] Han Zheng, Jing Jiang, Pengfei Wei, Guodong Long, and Chengqi Zhang. Competitive and\\ncooperative heterogeneous deep reinforcement learning. In Proceedings of the 19th International\\nConference on Autonomous Agents and MultiAgent Systems, AAMAS \\u201920, page 1656\\u20131664,\\nRichland, SC, 2020.\\n[32] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on\\nknowledge and data engineering, 22(10):1345\\u20131359, 2009.\\n[33] Pengfei Wei, Yiping Ke, and Chi Keong Goh. Deep nonlinear feature coding for unsupervised\\ndomain adaptation. In IJCAI, pages 2189\\u20132195, 2016.\\n[34] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\\ncontrol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages\\n5026\\u20135033. IEEE, 2012.\\n11\\n\",\n          \"JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n1\\nFact-based Agent modeling for Multi-Agent\\nReinforcement Learning\\nBaofu Fang, Caiming Zheng and Hao Wang\\nAbstract\\u2014In multi-agent systems, agents need to interact and\\ncollaborate with other agents in environments. Agent modeling\\nis crucial to facilitate agent interactions and make adaptive\\ncooperation strategies. However, it is challenging for agents to\\nmodel the beliefs, behaviors, and intentions of other agents\\nin non-stationary environment where all agent policies are\\nlearned simultaneously. In addition, the existing methods realize\\nagent modeling through behavior cloning which assume that\\nthe local information of other agents can be accessed during\\nexecution or training. However, this assumption is infeasible\\nin unknown scenarios characterized by unknown agents, such\\nas competition teams, unreliable communication and federated\\nlearning due to privacy concerns. To eliminate this assumption\\nand achieve agent modeling in unknown scenarios, Fact-based\\nAgent modeling (FAM) method is proposed in which fact-based\\nbelief inference (FBI) network models other agents in partially\\nobservable environment only based on its local information. The\\nreward and observation obtained by agents after taking actions\\nare called facts, and FAM uses facts as reconstruction target\\nto learn the policy representation of other agents through a\\nvariational autoencoder. We evaluate FAM on various Multiagent\\nParticle Environment (MPE) and compare the results with\\nseveral state-of-the-art MARL algorithms. Experimental results\\nshow that compared with baseline methods, FAM can effectively\\nimprove the efficiency of agent policy learning by making adap-\\ntive cooperation strategies in multi-agent reinforcement learning\\ntasks, while achieving higher returns in complex competitive-\\ncooperative mixed scenarios.\\nIndex Terms\\u2014Multi-agent Reinforcement Learning, Multi-\\nagent Systems, Agent Modeling.\\nI. INTRODUCTION\\nR\\nEINFORCEMENT Learning (RL) has achieved rapid\\nprogress in cooperative and competitive multi-agent\\ngames, such as OpenAI Five[1] and AlphaStar[2]. In multi-\\nagent environments, agents must interact with each other,\\nwhere the interaction relationship includes competition and\\ncooperation. Due to the policy of all agents are simulta-\\nneously learning, it affects the state transitions and reward\\nfunctions experienced by an individual agent[3]. From the\\nperspective of a single agent, interacting with other agents\\nwhose policies change makes the environment non-stationary.\\nTherefore, other agents cannot be simply treated as part of the\\nenvironment. Agent modeling promotes the agent to adjust its\\nown policy to adapt to the policy changes of other agents by\\nManuscript received 4, June 2023. This work was supported by the Univer-\\nsity Synergy Innovation Program of Anhui Province (Grant No.GXXT-2022-\\n055), Open Fund of Key Laboratory of Flight Techniques and Flight Safety,\\nCAAC (Grant No.FZ2022KF09), and the R&D Program of Key Laboratory\\nof Flight Techniques and Flight Safety, CAAC(Grant No.FZ2022ZZ02).\\nThe authors are with the School of Computer Science and Information\\nEngineering, Hefei University of Technology, Hefei, 230601, China (e-mail:\\nfangbf@hfut.edu.cn; 2502282770@@qq.com; jsjwangh@hfut.edu.cn).\\nexplicitly modeling the beliefs, behaviors and intentions of\\nother agents[4]. Since the agent learns in the same partially\\nobservable environment while other agents whose strategies\\nare complex, diverse, and time-varying. Therefore, modeling\\nother agents in non-stationary environments is a major chal-\\nlenge for multi-agent reinforcement learning.\\nTraditional agent modeling assumes that agents can access\\nthe local information of other agents during execution and\\ntraining[4], [5], [6], including the local observations and\\nactions taken by other agents. However, this assumption often\\ndoes not hold in many multi-agent scenarios. In practical,\\nagents may have limited visibility of their surrounding en-\\nvironment and communication with competing agents may be\\nprohibited, while communication between cooperating agents\\nis often unreliable[7], for example in federated learning tasks.\\nIn such situations, agents must inference and make decisions\\nbased on their local information. To weaken this assumption,\\nLIAM[8] and SMA2C[9] utilize the local information of\\nagents, including their own observations, actions, and rewards,\\nto infer the representations of other agents in a recurrent man-\\nner. These methods relax the assumption of traditional agent\\nmodeling by allowing access to the local information of other\\nagents only during the training stage. However, in unknown\\nscenarios, agents may also be prohibited from accessing the\\ninformation of other agents during both execution and training\\nstages. It is infeasible for behavior cloning-based approaches\\nto explicitly minimize the difference between an agent\\u2019s policy\\nmodel and the true policy. Therefore, the agent modeling in\\nthis case requires the agent to rely on its own local information,\\nthat is, it does not access the local information of other agents\\nduring the training and execution phases.\\nConsider a simple real-world scenario as shown in Figure\\n1, where the fruits collection task requires three people (Alice,\\nBob and Carol) to collect three fruits include apples, oranges,\\nand pears with the shortest time. In order to achieve this\\ntask without communication, each person should go through\\nfour stages: 1).Preliminary decision making, 2).Observing and\\ninferring, 3).Interacting, 4).Repeating steps 2) and 3) to avoid\\nconflits and achieve collaborative consensus until the fruits\\ncollection task is complete. In this process, each person needs\\nto start from the recent observation to infer other person policy\\nrepresentations to help itself make adaptive decisions. At the\\nsame time, the facts that happened after decision making are\\nused to verify the inference result. In multi-agent systems, the\\nrewards and observation received by the agent after performing\\nthe action imply rich information about the actions of other\\nagents at the same moment.\\nBased on this viewpoint, We propose Fact-based Agent\\narXiv:2310.12290v1  [cs.AI]  18 Oct 2023\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n2\\nA\\nB\\nC\\nA\\nB\\nC\\nAlice wants to collect \\noranges, if so, then she \\nwill be closer to \\noranges!\\nBob wants to collect \\noranges, but I am \\nfaster than him!\\nA\\nB\\nC\\nOrange is closest to \\nme and I will collect \\nit.\\nApple is closest to me \\nand I will collect it.\\nOrange is closest \\nto me and I will \\ncollect it.\\nA\\nB\\nC\\nAlice is closer to the \\norange, I am sure she \\nwants to collect them, but \\nshe is faster than me. \\nAlso, Carol wants to \\ncollect apple, so I am \\ngoing to collect pear.\\nStep 1. Preliminary decision making\\nStep 2. Observing and inferring\\nStep 3. Interacting\\nStep 4. Repeating Steps 2). and 3)\\nFig. 1. Fruit collection example. Everyone needs to cooperate to collect three konds of fruits which are apple, orange, and pear in shortest possible time. The\\nwhole process of fruit collection can be divided into 4 stages: 1).Preliminary decision making: Observing the surrounding environment, independently select\\nfruits, and making effective decisions. 2).Observing and inferring: Observing information related others and inferring their behavioral intentions. 3).Interacting:\\nMaking adaptive decisions based on the inferred results to interact with environment and other agents and verifying the previous inferences through the fact\\nof consequences after decisions. 4).Repeating steps 2). and 3). to avoid cooperative goal conflicts and achieve collaborative consensus until the fruit collection\\ntask is completed.\\nModeling (FAM) for multi-agent learning, which eliminates\\nthe assumption of accessing local information of other agents\\nduring execution and training phases. We build fact-based\\nbelief inference (FBI) network to model other agents based\\non own local information which is a variational autoEncoder\\n(VAE) that has the advantage of being able to compensate\\nfor the information difference between the execution and\\ntraining phases. The difference from the existing work is that\\nSMA2C[9] adopts the method of behavior cloning during\\ntraining phase. Howerver, in this paper, the reward signal\\ncontaining global information and the observation of local\\ninformation received by the agent after performing the action\\nare used as the reconstruction target. The proposed FAM\\nis suitable for non-stationary and partially observable envi-\\nronments. In addition, the complexity of agent modeling of\\nSMA2C[9] and LIAM[8] is O(N) while FAM is O(1) that\\nis independent of the number of agents. FAM is also more\\nsuitable for unknown scearios. The main contributions of this\\narticle as follows.\\n1) In order to elimate the assumption of accessing the local\\ninformation of other agents for agent modeling, fact-\\nbased belief inference (FBI) network is proposed, which\\nmodels other agents based on own local information\\nusing variational autoencoder.\\n2) Combining FBI with Actor-Critc, Fact-based Agent\\nModeling (FAM) is proposed for multi-agent learning,\\nwhich learns adaptive collaboration strategies by con-\\nsidering the policies of other agents. It can effectively\\napplicable to partially observable environments.\\n3) Extensive experimental was conducted to verify the\\neffectiveness and feasibility of the proposed FAM, and\\nanalyze the information encoded by FBI.\\nThe remainder of this article is organised as follows. Section\\nII describes the background of deep reinforcement learning\\nand variational autoencoder. Section III reviews the related\\nwork about multi-agent reinforcement learning and agent\\nmodeling. Section IV presents the proposed FBI and FAM.\\nSection V describes the detailed results and analysis of the\\nexperiments. Ultimately, conclusions are provided in Section\\nVI.\\nII. NOTATION AND BACKGROUND\\nThe fully cooperative multi-agent task can be modeled\\nas a decentralized partially observable Markov decision\\nprocess(Dec-POMDP)[10]. It is represented by the tuple\\nG =< I, S, U, P, r, Z, O, \\u03b3 >, where I = {1, 2, ..., n} is a\\nfinite set of agents, and n represents the number of agents.\\ns \\u2208S describes the global state of the environment. At each\\ntimestep t, each agent a \\u2208I receives an observation oa\\nt \\u2208O\\nthrough the observation function Z(s, a) : S \\u00d7 I \\u2192O and\\nselects an action ua\\nt , forming a joint action ut \\u2208U. After\\nexecuting the actions, the agents receive rewards signal rt,\\nwhere all agents share the same reward function r(s, u) :\\nS \\u00d7 U \\u2192R, and transition to the next state according to\\ntransition probability function P(s\\u2032|s, u) : S \\u00d7U \\u00d7S \\u2192[0, 1].\\nThe action-observation history of each agent is denoted as\\n\\u03c4 a\\n\\u2208T\\n\\u2261(O \\u00d7 U)\\u2217, and the policy \\u03c0(ua\\nt |\\u03c4 a\\n1:t; \\u03b8a) :\\nT \\u00d7 U\\n\\u2192[0, 1] is based on its own action-observation\\nhistory, with policy parameters \\u03b8a. The goal of Dec-POMDP\\nis to learn a joint policy \\u03c0 = (\\u03c01, ..., \\u03c0n) that maximizes\\nthe team cumulative discounted return Rt = P\\u221e\\nl=0 \\u03b3lrt+l,\\nwhere \\u03b3 \\u2208[0, 1) is the discount factor. The joint action-\\nvalue function of the joint policy \\u03c0 is denoted as Q\\u03c0(st, ut) =\\nEst+1:\\u221e,ut+1:\\u221e[Rt|st, ut], the joint state-value function is de-\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n3\\nnoted as V \\u03c0(st) = Est+1:\\u221e, ut:\\u221e[Rt|st], and the advantage\\nfunction is denoted as A\\u03c0(st, ut) = Q\\u03c0(st, ut) \\u2212V \\u03c0(st).\\nPolicy Gradient: Vanilla Policy Gradient (REINFORCE) is\\nan on-policy algorithm that directly uses a parameterized\\nmodel[11] to approximate the policy \\u03c0(ut|st; \\u03b8). REINFORCE\\ndoes not require a separate behavior policy because \\u03c0\\u03b8(ut|st)\\nnaturally explores and exploits the environment. The policy\\nparameters \\u03b8 are updated at each step by increasing the log-\\nlikelihood of the chosen actions with respect to the sampled\\ntrajectory return Rt. The gradient update direction is given by:\\ng = Et [\\u2207\\u03b8 log \\u03c0(ut|st; \\u03b8)Rt]\\n(1)\\nThe baseline bt(st)[12] is subtracted from the return to\\nreduce the variance of the estimated return while remaining\\nunbiased, and the gradient update direction becomes:\\ng = Et [\\u2207\\u03b8 log \\u03c0(ut|st; \\u03b8)(Rt \\u2212bt(st))]\\n(2)\\nThis allows for more stable learning and potentially faster\\nconvergence. The baseline can be a value function estimate\\nor a learned function that approximates the expected return at\\nstate st.\\nAdvantage Actor-Critic (A2C): A2C is an on-policy Actor-\\nCritic method that utilizes parallel environments to break the\\ncorrelation between consecutive samples. It introduces a state\\nvalue function estimator Vw(st) to approximate the state value\\nE [Rt|st], and used as a baseline to reduce the variance of\\nsampling returns to improve policy gradient updates. Since\\nQ(st, ut) is an approximate estimate of Rt, A(st, ut) = Rt \\u2212\\nV (st) is expressed as the advantage of action at under state st,\\nthen the direction of A2C policy gradient update is as follows,\\ng = Et [\\u2207\\u03b8 log \\u03c0(ut|st; \\u03b8)(A(st, ut))]\\n(3)\\nBy using the advantage function A(st, ut), A2C facilitates\\nmore stable and efficient learning. And the loss function for\\nthe state value function is given by:\\nLa2c(\\u03c9) = E(st,ut,rt+1,st+1)\\u223cB(Rt \\u2212V\\u03c9(st))2\\n(4)\\nwhere B denotes the sampled batch trajectory.\\nProximal Policy Optimization (PPO): PPO is an Actor-Critic\\nalgorithm whose core idea is to achieve stable training by\\nlimiting the distance between old and new policies. The PPO\\nalgorithm uses a loss function called \\\"clipped surrogate objec-\\ntive\\\" to control the step size of policy update, thus achieving\\nstability in training without slowing down the training speed.\\nUnlike A2C, PPO employs a technique called importance\\nsampling, which allows multiple gradient descent updates to\\nbe performed using the same batch of trajectories. The loss\\nfunction of actor for PPO as follows:\\nLppo(\\u03b8) = E\\u03c4\\u223cB [min(rt(\\u03b8), clip(ratiot(\\u03b8), 1 \\u2212\\u03f5, 1 + \\u03f5))At]\\n(5)\\nwhere ratiot(\\u03b8) =\\n\\u03c0\\u03b8(ut|st)\\n\\u03c0\\u03b8old(ut|st) represents the ratio between the\\nnew and old policies, and \\u03f5 is a hyperparameter used to control\\nthe difference between the new and old policies. Compared to\\nA2C, PPO has a higher sample efficiency.\\nVariational Autoencoder (VAE): VAE is a generative model\\nused to approximate the true posterior distribution p(z|x),\\nwhere the dataset samples X = {xi}N\\ni=1 are generated from an\\nunknown parameterized generative distribution p(x|z; \\u03b8) with\\nthe latent variable z being unobserved. The prior distribution\\nof the latent variable z is assumed to be a Gaussian distribution\\np(z) = N(z; 0, I) with mean 0 and variance 1. The goal of\\nVAE is to learn a variational distribution q(z|x; \\u03d5) parameter-\\nized by \\u03d5 to approximate the true posterior distribution p(z|x),\\nwhere q(z|x; \\u03d5) = N(z; \\u00b5, \\u03c3, \\u03d5) is a Gaussian distribution\\nwith mean \\u00b5 and variance \\u03c3.\\nVariational inference uses the KL divergence as a distance\\nmeasure function to minimize the distance between the ap-\\nproximate posterior distribution q(z|x; \\u03d5) and the true poste-\\nrior distribution p(z|x) , the Evidence Lower Bound(ELBO)\\nas follows:\\nlog p(x) \\u2265ELBO(\\u03c8, \\u03d5|x)\\n= Ez\\u223cq(z|x;\\u03d5) [log p(x|z; \\u03c8) \\u2212DKL(q(z|x; \\u03d5)||p(z))]\\n(6)\\nwhere DKL represents the Kullback-Leibler (KL) divergence.\\nThe first term on the right-hand side of the equation is the\\nreconstruction loss, which measures the quality of the gener-\\nated samples. The second term is a regularization term, which\\nis used to constrain the distribution of the latent variables.\\nHiggins et al.[13] proposes \\u03b2-VAE, where the parameter \\u03b2 \\u22650\\nis used to balance the reconstruction loss and the regularization\\nterm. The overall optimization objective of the \\u03b2-VAE is as\\nfollows:\\nLvae(\\u03d5, \\u03c8) = Ez\\u223cq\\u03d5 [log p(x|z; \\u03c8) \\u2212\\u03b2DKL(q(z|x; \\u03d5)||p(z))]\\n(7)\\nIII. RELATED WORK\\nMulti-agent system (MAS) consists of multiple agents in-\\nteracting in the shared environment to accomplish tasks. For\\ncomplex tasks, Multi-Agent Reinforcement Learning (MARL)\\nenables agents to learn effective policies through interaction\\nwith the environment. One of the main challenges in MARL\\nis the inherent non-stationarity of the environment where all\\nagents learn simultaneousl. Since the policies of other agents\\nare unknown, it is unstable for agents to learn policies if\\nthey are considered part of the environment. To address this\\nchallenge, one approach is to adopt the Centralized Training\\nwith Decentralized Execution (CTDE) framework, where a\\ncentralized Critic is used to approximate joint action value or\\nstate-action value to guide the policy learning of individual\\nagents. The value-based methods include QMIX[14], OW-\\nQIX[15], and TransfQMIX[16] and the Actor-Critic-based\\nmethods such as MADDPG[17] and MAAC[18].\\nAnother\\napproach\\nto\\naddress\\nthe\\nchallenge\\nof\\nnon-\\nstationarity in MARL is agent modeling, which mitigates\\nthe effect of non-stationarity by incorporating information\\nabout other agents\\u2019 beliefs, behaviors, and intentions. Many\\nstudies on agent modeling rely on predicting the actions or\\ngoals of other agents during training. He et al.[5] proposed a\\nbehavior cloning-based agent modeling approach that uses a\\nneural network to predict the actions executed by other agents\\nbased on their observations. Hernandez-Leal et al.[19] treated\\nlearning other non-learning agents\\u2019 policy as an auxiliary\\ntask and simplified it to a standard single-agent reinforcement\\nlearning problem. Similarly, Georgios et al.[8] used an encoder\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n4\\nto construct representations of other agents based on their\\nlocal information in a recurrent manner, while the decoder\\nreconstructed the observations and actions of other agents.\\nAnother method[9] applied a variational autoencoder for agent\\nmodeling, where the encoder generates a high-dimensional\\ncontinuous distribution as a representation of the other agent\\u2019s\\npolicy, and the decoder is trained by reconstructing the agent\\u2019s\\nactions. Both of these methods allow access to other agents\\u2019\\nlocal information during the training or execution.\\nIn terms of considering simultaneously learning opponents,\\nFoerster et al.[20] proposed LOLA, which incorporates the\\ninfluence of an agent\\u2019s policy on the parameter updates of\\nother agents\\u2019 policies. Al-Shedivat et al.[21] introduced Mate-\\nPG, a meta-policy gradient-based method that leverages the\\ntrajectories of other agents in multiple meta-gradient steps to\\nconstruct a policy that benefits from updating other agents.\\nKim et al.[22] proposed an extension to the existing method\\ncalled Meta-MAPG. They introduced an additional term that\\ncaptures the influence of an agent\\u2019s current policy on the\\nfuture policies of other agents, similar to LOLA. These meta-\\nlearning-based methods require the trajectory distributions to\\nmatch between training and testing, implicitly assuming that\\nall other agents use the same learning algorithm.\\nUnlike existing work, we consider a more complex and\\ngeneral setting where the policies of other agents are learned\\nsimultaneously with the agent\\u2019s own policy. Furthermore, there\\nis partial observability in multi-agent environment, and the\\nagents are not allowed to access the local information of other\\nagents to achieve agent modeling during the execution and\\ntraining.\\nIV. METHODS\\nIn this section, we introduce a Fact-based Agent Modeling\\n(FAM) Algorithm as shown in Figure 2, which completely\\neliminates the assumption that traditional agent modeling\\nallow access to other agents local information during the\\ntraining or execution phases. Firstly, we provide the structure\\nand details of fact-based belief inference module (FBI). Fur-\\nthermore, we present the optimization objective and training\\nprocedure for FAM.\\nA. Fact-based Belief Inference\\nTo enable an agent to interact with other agents and learn\\nadaptive policy, it needs to infer the current policies of the\\ninteracting agents. Fact-based Belief Inference (FBI) elim-\\ninates the assumption that agents can access other agents\\u2019\\nlocal information during training or execution. This module\\nextracts policy representation denoted as zi of other agent\\nfrom the interaction trajectories of agent i, including triplets\\nof observations, actions, and rewards triplets. Policy repre-\\nsentations are learned from facts acquired by the agent after\\nperforming actions. The extracted representation zi denotes\\nagent i\\u2019s beliefs about other agents, i.e., estimates of their\\npolicy. This introduces uncertainty of other agents\\u2019 policies\\ninto agent i\\u2019s policy \\u03c0i(ai\\nt|\\u03c4 i\\n1:t, zi).\\nAssuming the joint policy of other agents are unobservable\\nvariables zi in the latent space Zi for agent i, and the latent\\nFig. 2. The architecture of Fact-based Agent Modeling (FAM). During the\\nexecution phase, the encoder module of FBI utilizes the local information of\\nthe agent to extract representations of other agents\\u2019 policies, which are then\\nused for the agent\\u2019s decision-making process. On the other hand, during the\\ntraining phase, the decoder module reconstructs the facts and simultaneously\\ntrains both the encoder and decoder. The agent makes decisions based on its\\nown action-observation trajectories and the representations of other agents\\u2019\\npolicies.\\nvariable zi\\nt at time step t contains the policy representation of\\nother agents except agent i itself. To learn the latent informa-\\ntion, FAM employs FBI which is a variational encoder-decoder\\narchitecture[23] as shown in Fig.2. Agent i uses an encoder\\nq consists of a recurrent neural network and a fully con-\\nnected neural network to infer representations of other agents\\u2019\\npolicies by local information including the observation-action-\\nreward triplet. It outputs \\u00b5i and log(\\u03c3i) which is the param-\\neters of variational distribution. And the policy representation\\nof other agents are sampled from the variational distribution.\\nSpecifically, the goal of the encoder is to approximate the\\ntrue posterior p(zi) using a variational distribution obtained\\nsolely from local information. FBI constructs a decoder p to\\nlearn the representation of other agents by reconstructing facts\\nconditioned on policy representations zi \\u223cN(\\u00b5i, \\u03c3i) and the\\nagent\\u2019s observation-action. The encoder is parameterized by\\n\\u03c8i, and the state prediction and reward prediction function\\nin the decoder are parameterized by \\u03d5i and \\u03c6i, respectively.\\nFBI jointly optimizes \\u03c8i, \\u03d5i, and \\u03c6i to maximize the evidence\\nlower bound(ELBO) of the sampled trajectory \\u03c4 i\\n1:t, as follows,\\nELBO(\\u03c8i, \\u03d5i, \\u03c6i|\\u03c4 i\\n1:t) = Ezi\\nt\\u223cq\\u03c8i)\\n\\u0002\\nJrecon\\n\\u0003\\n\\u2212\\u03b2DKL(q(\\u00b5i\\nt, \\u03c3i\\nt|\\u03c4 i\\n1:t; \\u03c8i)||p(zi))\\n(8)\\nwhere Jrecon = log p(\\u02c6oi\\nt+1, \\u02c6ri\\nt+1|oi\\nt, ai\\nt, zi\\nt; \\u03d5i, \\u03c6i) is recon-\\nstruction loss. The ELBO is related to the state tran-\\nsition and reward functions of each agent i. \\u03c4 i\\n1:t\\n=\\n(oi\\n1, ai\\n1, ri\\n2, oi\\n2, ..., ri\\nt, oi\\nt) represents the local trajectory infor-\\nmation of agent i up to time step t. p(zi) is the prior\\ndistribution of the latent variable zi, we assume the latent vari-\\nable follows a standard Gaussian distribution zi \\u223cN(0, I).\\nDKL is the Kullback-Leibler(KL) divergence which measures\\nthe distance between the approximate posterior distribution q\\nand the true posterior distribution p. The hyperparameter \\u03b2\\nis used for controling the importance of the regularization\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n5\\nAlgorithm 1 Training Procedure for FAM Algorithm\\nInitialize: \\u03b8i, \\u03c9i, \\u03a6i = {\\u03c8i, \\u03d5i, \\u03c6i}, \\u03b2, \\u03b11, \\u03b12, B, E;\\nOutput: \\u03b8\\u2217\\ni , \\u03c9\\u2217\\ni , \\u03a6\\u2217\\ni = {\\u03c8\\u2217\\ni , \\u03d5\\u2217\\ni , \\u03c6\\u2217\\ni };\\n1: for each episode j do\\n2:\\nInitial observation o0 \\u2190{oi\\n0}n\\n1\\n3:\\nfor each timestep t do\\n4:\\nGet observations ot = {oi\\nt}n\\ni=1;\\n5:\\nCompute opponent embeddings zt\\n=\\n{zi\\nt\\n\\u223c\\nq\\u03c8i}n\\ni=1;\\n6:\\nSample action ui\\nt \\u223c\\u03c0i(\\u00b7|oi\\nt, zi\\nt; \\u03b8i);\\n7:\\nPerform joint actions ut = (u1\\nt, ..., un\\nt ) and reveive\\njoint reward rt+1 and next observations\\n8:\\not+1 = {oi\\nt+1}n\\ni=1;\\n9:\\nAdd transition {ot, zt, ut, rt+1, ot+1} \\u2192Bj,t\\n10:\\nend for\\n11:\\nif |B| = batch size then\\n12:\\nfor each epoch e < E do\\n13:\\nfor each agent i do\\n14:\\n\\u03c9i \\u2190\\u03c9i \\u2212\\u03b11\\u2207\\u03c9iLcritic (Eq.10)\\n15:\\n\\u03b8i \\u2190\\u03b8i \\u2212\\u03b11\\u2207\\u03b8iLactor (Eq.11)\\n16:\\n\\u03a6i \\u2190\\u03a6i \\u2212\\u03b12\\u2207\\u03a6iLfbi (Eq. 9)\\n17:\\nB \\u2190\\u2205;\\n18:\\nend for\\n19:\\nend for\\n20:\\nSoft update parameters \\u03b8\\n\\u2032\\ni, \\u03c9\\n\\u2032\\ni, \\u03a6\\n\\u2032\\ni with \\u03b8i, \\u03c9i, \\u03a6i\\n21:\\nend if\\n22: end for\\nterm KL divergence[13]. Minimizing the loss is equivalent\\nto maximizing the ELBO, and thus the loss function of the\\nFBI as follows:\\nLfbi(\\u03c8i, \\u03d5i, \\u03c6i) = Ezi\\nt\\u223cq\\u03c8i\\n\\u0002\\nJrecon_obs + Jrecon_rew\\n\\u0003\\n\\u2212\\u03b2 1\\n2\\nd\\nX\\nj=1\\n(1 + log(\\u03c32\\nt,i,j) \\u2212\\u00b52\\nt,i,j \\u2212\\u03c32\\nt,i,j)\\n(9)\\nwhere Jrecon_obs = (pobs(\\u02c6oi\\nt+1|oi\\nt, ai\\nt, zi\\nt; \\u03d5i) \\u2212oi\\nt+1)2 and\\nJrecon_rew = (pr(\\u02c6ri\\nt+1|oi\\nt, ai\\nt, zi\\nt; \\u03c6i)\\u2212ri\\nt+1)2 are the observa-\\ntion prediction and reward prediction loss functions, respec-\\ntively. d represents the dimensionality of the latent variable\\nzi. The intuitive interpretation of this loss function is that\\nthe decoder p is optimized to reconstruct the facts that occur\\nafter taking an action, specifically received in next time step\\u2019s\\nobservation and reward.\\nB. Training Algorithm of FAM\\nIn this section, we describe the training process of FAM.\\nThe sampled trajectory of the agent, along with the latent\\nvariable zi, is used to optimize the RL policy. We consider\\nan augmented policy space T augi = Oi \\u00d7 Ui \\u00d7 Zi for agent\\ni, where Oi and Ui are the original observation and action\\nspaces of the Dec-POMDP, and Zi represents the belief space\\nof agent i on other agents. Specifically, the belief refers to the\\npolicy representations of other agnets. Compared to the policy\\nspace without considering other agents\\u2019 policy representations\\nT augi = Oi \\u00d7 Ui, the augmented policy space T i allows\\nfor different responses to different zi \\u2208Zi. This enables\\nadaptive behavior based on the policy representations of other\\nagents. We assume that all agents learn simultaneously in\\nthe same environment. Due to the delayed nature of other\\nagents\\u2019 policy changes, which affect the agent\\u2019s belief about\\ntheir policy representations, we train the FAM using on-policy\\nalgorithm. In our experiments, we use the PPO algorithm to\\noptimize the agent\\u2019s policy. The inputs to the Actor and Critic\\nare the local action-observation trajectories and the inferenced\\npolicy representation zi. Additionally, the RL loss does not\\nbackpropagate into FBI. To encourage exploration, we also\\nuse policy entropy[24]. Given a batch of sampled trajectories\\nB, the loss for the Critic network is defined as follows:\\nLcritic(\\u03c9i) = EB\\n\\u0002\\n(ri\\nt+1 + \\u03b3V (oi\\nt+1, zi\\nt+1; \\u03c9\\u2212\\ni )\\n\\u2212V (oi\\nt, zi\\nt; \\u03c9i))\\n\\u0003\\n(10)\\nwhere V is the target network, z indicates that the loss of\\nthe Critic network does not backpropagate through z, and\\n\\u03c9\\u2212represents the parameters of the Critic target network,\\nwhich are also not updated through gradient backpropagation.\\nAdditionally, the loss for the Actor network is defined as\\nfollows:\\nLactor(\\u03b8i) = E(oi\\nt,ai\\nt,zi\\nt,ri\\nt+1,oi\\nt+1)\\u223cB\\n\\u0002\\nmin(ratiot(\\u03b8i), clip(ratiot(\\u03b8i), 1 \\u2212\\u03f5, 1 + \\u03f5))Ai\\nt\\n\\u0003\\n(11)\\nwhere ratiot =\\n\\u03c0(at|ot,zt;\\u03b8i)\\n\\u03c0(at|ot,zt;\\u03b8i\\nold) is the action probability ratio\\nof the old and new policies, and clip modifies the surrogate\\nobjective by restricting the probability ratio.\\nNow we can define a training objective to learn the ap-\\nproximate posterior distribution q as well as reward prediction\\nfunction pr and observation prediction function pobs, as fol-\\nlows:\\nLfam(\\u03b8, \\u03c9, \\u03c8, \\u03d5, \\u03c6)=\\nLactor + Lcritic + Lfbi\\n(12)\\nV. EXPERIMENTAL RESULTS AND ANALYSIS\\nIn this section, we aim to investigate the following aspects\\nof FAM: 1). whether FBI improve learning efficiency, promote\\ncooperative among multi agents, and learn adaptive collabo-\\nration strategies? 2). how FAM enables collaboration through\\nadaptive strategies, 3). does FBI encode the policies of other\\nagents, and beyong that, what other important information is\\nencoded. To answer these three questions, we conduct exper-\\nments in two multi-agent particle environments: Cooperative\\nNavigation (CN) and Predator-Prey (PP), as introduced by\\nLowe et al.[17]. The implementation is based on the epymarl\\nframework proposed by Papoudakis et al.[25].\\nA. Experimental Setup\\n1) Environments: We introduce the two environments used\\nfor our proposed FAM and baselines.\\nCooperative Navigation: In this task, N agents need to\\ncooperatively occupy L landmarks in an environment with\\npartial observability. The team reward function rdistance =\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n6\\n\\u2212PN\\ni=1 minj dis(Landmarki, Agentj) is based on the neg-\\native sum of distances between the landmarks and their closest\\nagents. Additionally, we discourage collisions between agents,\\nand a collision penalty rcollision = \\u22121 is applied. Each agent\\nneeds to observe the movements of other agents to infer their\\nbehavior or goals, select a suitable landmark to occupy while\\navoiding conflicts and collisions with other agents. We conduct\\nexperiments with N = 5 agents and L = 5 landmarks, where\\neach agent can only observe the relative positions of the two\\nclosest agents and three closest landmarks. The agents have 5\\navailable actions, and the maximum episode length is set to\\n25.\\nPredator-Prey (PP): In this task, N predators try to capture\\nM preys in an environment with partial observability. The\\npreys follow predefined policies to move away from the closest\\npredators with a speed of 7, while the predators are only 5.\\nSince preys have a faster movement speed than predators,\\nindividual predators cannot capture preys on their own. The\\nteam reward for predators is the negative sum of distances\\nbetween the preys and their closest predators. A collision\\npenalty rcollision = \\u22121 is also applied to the predators. In PP\\ntask, each predator can observe the relative positions of the two\\nclosest predators and three closest preys. The predators have 5\\navailable actions. Unlike in Cooperative Navigation, the preys\\nexhibit highly dynamic movement. Therefore, predators need\\nto infer the behavior of other predators and target preys to\\ncooperate with other predators and capture the desired preys.\\nWe conduct experiments with N = 7 predators and M = 3\\npreys.\\nIn IA2C, each agent treats other agents as a part of the\\nenvironment and utilizes the A2C algorithm to learn and\\noptimize its Actor network with 3 for approximating the policy\\n\\u03c0\\u03b8i and Critic network with 4 for approximating the value\\nfunction V\\u03c9i in distributed multi-agent systems[24].\\n2) Implementation Detials: Next, we will introduce the im-\\nplementation details of the proposed FBI and FAM. The FAM\\nconsists of actor network, critic network and FBI network\\nparameterized by \\u03b8i, \\u03c9i and \\u03a6i = {\\u03c8i, \\u03d5i, \\u03c6i}, respectively.\\nThe FBI includes an RNN-based encoder and two MLP-based\\ndecoders. During execution, RNN-based encoder takes the\\nlocal observation-action-reward triplet (oi\\nt, ui\\nt\\u22121, ri\\nt) as input\\nthrough a 1-layer fully connected neural network(FC) followed\\nby a ReLU activation function to extract features, which are\\nthen fed into a GRU recurrent network to capture temporal\\ndependencies. Finally, a 1-layer FC outputs the variational\\ndistribution parameters \\u00b5i\\nt and log \\u03c3i\\nt that approximate the true\\nposterior distribution. The sampled with dimension d = 5 is\\nthe policy representation of other agents for agent decision-\\nmaking. During training, MLP-based decoder takes the local\\nobservation-action-policy representation (oi\\nt, ui\\nt, zi\\nt) as input\\nthrough 3-layer FC and followed by ReLU activation functions\\nto output the predictions of rewards and observations obtained\\nafter executing action. It is important to note that the last fully\\nconnected layer does not require a ReLU activation function.\\nThe RNN-based encoder and MLP-based decoder are trained\\nby computing the prediction loss and regularization term.\\nTABLE I\\nPERFORMANCE EVALUATION COMPARISON OF FAM AGAINST BASELINES\\nIN CN.\\nMethods\\nAvg. Ret.\\nAvg. Rew.\\nAvg. Occ.\\nAvg. Dist.\\nIPPO\\n\\u221226.2 \\u00b1 8.2\\n\\u22120.52 \\u00b1 0.3\\n4.56 \\u00b1 0.4\\n0.45 \\u00b1 0.2\\nIA2C\\n\\u221234.6 \\u00b1 10.1\\n\\u22121.08 \\u00b1 0.5\\n3.48 \\u00b1 1.0\\n1.02 \\u00b1 0.4\\nMAA2C\\n\\u221234.8 \\u00b1 9.4\\n\\u22121.09 \\u00b1 0.4\\n3.06 \\u00b1 1.1\\n1.06 \\u00b1 0.4\\nMAPPO\\n\\u221226.1 \\u00b1 7.8\\n\\u22120.52 \\u00b1 0.3\\n4.57 \\u00b1 0.5\\n0.48 \\u00b1 0.3\\nFAM\\n\\u221225.5 \\u00b1 8.8\\n\\u22120.50 \\u00b1 0.3\\n4.45 \\u00b1 0.5\\n0.45 \\u00b1 0.3\\nLIAM\\n\\u221235.2 \\u00b1 9.6\\n\\u22121.06 \\u00b1 0.4\\n3.31 \\u00b1 1.0\\n1.05 \\u00b1 0.4\\nSMA2C\\n\\u221235.3 \\u00b1 9.6\\n\\u22121.12 \\u00b1 0.5\\n3.26 \\u00b1 1.1\\n1.05 \\u00b1 0.4\\nB. Main Results\\nWe compare FAM with several baselines to verify the\\neffectiveness and feasibility of the proposed method. Figure\\n3a) and Figure 3b) show the average episode return curves\\nand average landmarks occupied curve of FAM compared with\\nother baseline algorithms in CN during training. Figure 3c)\\nshows the average episode return curves of various methods\\ntrained in PP with training duration of 1e7 steps. And Table\\nI presents the performance metrics of various algorithms\\nevaluating 100 episodes in CN, including average episode\\nreturn(Avg. Ret.), average reward at the final timestep(Avg.\\nRew.), average occupied landmarks(Avg. Occ.), and the sum\\nof the average distances of all landmarks from the nearest\\nagent(Avg. Dist.).\\nFrom Fig. 3 a). and 3 b)., we can see that the proposed FAM\\nachieves higher learning efficiency than all other baselines\\nfrom 1e6 training steps, as well as faster convergence, and\\nslightly outperforms IPPO and MAPPO from 4e6 training\\nsteps. The main reason is that after all agents learn a certain\\nstrategy, considering the strategies of other agents will help\\nthe agents learn adaptive cooperation strategies. And they\\nstruggle with partial observability of the environment after 4e6\\nsteps. Meanwhile, the shaded areas of IPPO and MAPPO are\\nlarger compared with FAM, which indicates that the cooper-\\native strategy without considering other agents is less robust.\\nAmong the four evaluation performances as shown in Table I,\\nFAM has reached the best compared with all other baselines.\\nThe average episode return curves of IA2C, MAA2C, LIAM,\\nand SMA2C overlap and show slow learning. And in Figure\\n3b), IA2C occupies more landmarks. Similar results are shown\\nin Table I which indicates that IA2C performs slightly better\\nthan SMA2C, LIAM, and MAA2C. The possible reason is\\nthat the low sample efficiency of the A2C methods and agents\\nonly need to consider other agents at certain critical moments\\nin CN, which makes the performance of independent similar to\\nCTDE methods. Additionally, LIAM and SMA2C do not show\\nsuperiority. This could be due to the low sample efficiency\\nof the A2C methods and the high randomness in directly\\nmodeling the actions of other agents in a non-stationary\\nenvironment.\\nIt can be seen from Figure 3c) that IA2C performs the\\nworst and the proposed FAM outperforms all other baseline\\nby considering the strategies of other agents. The possible\\nreason is that preys are highly dynamic and move faster\\nthan predators, which requires closer cooperation between\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n7\\na)\\nb)\\nc)\\nFig. 3. The training results of the proposed FAM and the baseline method in CN and PP environments. a)The episode return curves during training in CN.\\nb)The occupied landmarks curves during training in CN. c)The episode return curves in PP. The solid line is the mean of the training results of 5 random\\nseeds, and the shaded area is the 25%-75% quartile.\\nFig. 4. Comparing the ablation results of FAM and its belief inference network\\nin the Predator-Prey environment.\\npredators and adaptation to other predators\\u2019 strategy to capture\\npreys cooperatively. However, independent IA2C is difficult\\nto achieve. Moreover, MAA2C employs a centralized critic\\nthat utilizes global information to guide the policy learning\\nof agents and achieves better performance than IA2C. But the\\nperformance of LIAM and SMA2C falls behind the centralized\\ncritic. The possible reason is that the centralized critic provides\\nmore effective information for guiding policy learning under\\npartially observability. IPPO and MAPPO exhibit similar av-\\nerage episode return curves which can be attributed to the\\neffectiveness of the PPO algorithm. This finding aligns with\\nprevious studies[26] and [25].\\nIn general, in the experimental settings of CN andPP,\\nwe found it interesting that the agent modeling method can\\nquickly and effectively improve the learning efficiency and\\nlearn adaptive collaboration strategies to obtain higher rewards\\nafter other agents learn a certain strategy. The good news is\\nthat it didn\\u2019t hinder the agent\\u2019s strategy learning before this.\\nC. Ablation Results\\nThe fact-based belief inference (FBI) network in FAM\\nutilizes a variational autoencoder (VAE) architecture, whose\\ninput and reconstruction target are the local information of\\nthe agent. To investigate the impact of the following factors\\nin FBI network: 1) decoder input and 2) reconstruction tar-\\ngets, we conducted ablation experiments in the Predator-Prey\\nenvironment.\\nThe ablation experiments included the following varia-\\ntions: i). FAM_wo_in_oa, where the decoder input only\\nconsisted of the representation of other agents\\u2019 policies. ii).\\nFAM_wo_rec_obs, where the decoder only reconstructed re-\\nwards. iii). FAM_wo_rec_rew, where the decoder only re-\\nconstructed observations. The average episode return curves\\nof the Predator-Prey task are plotted in Figure 4. These\\nablation experiments aim to examine the contributions of\\ndifferent components in FBI. By comparing the performance\\nof these variations with the FAM, we can gain insights into\\nthe importance of decoder input and the reconstruction of\\nobservations and rewards.\\nThe decoder in FBI network takes the agent\\u2019s local observa-\\ntions, actions and inferred representation as input. We denote\\nthe agent\\u2019s local observations and actions as \\\"oa\\\". To compare\\nthe impact of the decoder input, we denote the decoder input\\nas FAM_wo_in_oa only for other agent policy representations\\nz and keep the reconstruction fact unchanged. As shown\\nin Figure 4, FAM is generally better than FAM_wo_in_oa.\\nAlthough FAM and FAM_wo_in_oa have similar performance\\nin the early stage, both can effectively improve the efficiency\\nof policy learning. But when struggling partial observability,\\nFAM has an advantage. The possible reason is that the decoder\\ndesign of FBI is better for the agent to understand the\\ndynamics of the environment.\\nTo compare the impact of the decoder reconstruction targets,\\nwe compare the training results of FAM_wo_rec_obs and\\nFAM_wo_rec_rew on PP task, as shown in Figure 4. It\\ncan be seen that the performance of FAM_wo_rec_rew is\\nbetter, but it is weaker than FAM_wo_rec_obs in the early\\nstage. The possible reason is that the team reward helps\\nto extract other agents\\u2019 policy representations, but this is a\\nspurious reward signal, which may also hinder the learning of\\nother agents\\u2019 policies. The observation can directly represent\\nthe movement information of the surrounding agents, which\\nprovides rich verification information for each individual.\\nMoreover, FAM has the advantages of FAM_wo_rec_rew and\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n8\\nFAM\\nFAM\\nFAM\\nIPPO\\nIPPO\\nIPPO\\nMAPPO\\nMAPPO\\nMAPPO\\nFig. 5. The navigation trajectories and immediate reward curves of three evaluation scenarios of FAM, IPPO and MAPPO under Cooperative Navigation task.\\nFAM_wo_rec_obs by reconstructing observations and rewards,\\nand has the best performance both in the early stage of training\\nand in the stage of struggling partial observability.\\nOverall, the decoder reconstructs observations and rewards\\nby inputting its own observations, actions, and policy repre-\\nsentations to better help the agent understand the dynamics of\\nthe environment.\\nD. Strategy Analysis\\nIn order to understand the representation of other agents\\nlearned by FAM, we conducted evaluations and compared and\\nanalyzed the cooperative strategies of FAM agents with IPPO\\nand MAPPO agents in the Cooperative Navigation. Figure\\n5 shows the evaluated navigation trajectories and immediate\\nreward curves of the three methods of FAM, IPPO and\\nMAPPO in CN.\\nIt can be seen that all FAM agents can move near the\\nlandmark or successfully occupy the landmark, and the im-\\nmediate rewards are the best. However, the IPPO agent and\\nthe MAPPO agent can not due to the goal conflict between the\\nagents. We summarize the cooperative skills learned by FAM\\nagents, including i). Communicate without communication\\n(CWC), ii). Avoiding goal conflict and competition (AGCC),\\niii). Giving up the small to keep the big (GSKB).\\nCommunicate without communication: Once the strategy\\nof other agents is found to change, it will change its own\\nstrategy in time to meet the needs of the task. As shown in\\nFigure 5a), both agent 1 and agent 2 want to occupy landmark\\n1, and there is a goal conflict. However, agent 2 has an\\nadvantage in distance when occupying landmark 1, so agent 1\\nhas to change the landmark to 2. At this time, Agent 4 wants\\nto occupy landmark 4, but it infers that the strategy of Agent\\n1 is occupy landmark 2 which is farther away, which makes\\nit take longer to complete the task. There, agent 4 changes\\nthe navigation landmark to 2 and agent 1 changes its own\\nnavigation landmark to 4 for shorest complete time. However,\\nAgent 1 and Agent 4 of IPPO and MAPPO have landmark\\nconflict and competition, as shown in Figure 5 b) and c).\\nAvoiding goal conflict and competition: When it is found\\nthat the goals of other agents conflict with itself, it will\\nchange its own goals according to the actual situation to\\navoid competition. As shown in Figure 5d), there is a goal\\nconflict between agent 1 and agent 3 bacause they want to\\noccupy landmark. However, agent 1 occupys landmark 4 is\\nmore advantageous, because agent 3 is closer to unoccupied\\nlandmark 2. Therefore, agent 3 changes the landmark to avoid\\ngoal conflict can promote overall cooperation and complete\\nthe task faster. In contrast, IPPO agent and MAPPO agent\\nfailed to achieve this. As shown in Figure 5e), there is a\\ngoal conflict and goal competition between Agent 1 and\\nAgent 5. Agent 5 cannot observe landmark 2 due to partial\\nobservability. Therefore, agent 5 cannot effectively occupy\\nlandmark. A possible effective method is that agent 2 changes\\nits own landmark to 2. This collaborative strategy is reflected\\nby MAPPO, as shown in Figure 5f). But it takes longer to\\ncomplete the task.\\nGive up the small to keep the big: When there is a goal\\nconflict or the goal can be occupied by a more advantageous\\nagent, the agent will change its own goal to shorten the time\\nto complete the task. This skill is demonstrated in Figure 5g),\\nwhere Agent 2, although closer to landmark 1 and 4, chooses\\nthe farther landmark 5 to facilitate faster landmark occupation\\nby Agent 1 and Agent 5. In contrast, IPPO and MAPPO agents\\nfail to achieve the goal of conflict-free. In Figure 5h) and 5i),\\nthere is a landmark conflict and competition between agents.\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n9\\na)\\nb)\\nc)\\nd)\\ne)\\nf)\\nFig. 6. a) and d) are trajectory plots of the agents in the CN. b) and c) are t-SNE projections of the embedding vectors learned by agent 1 and agent 2 in\\ntrajectory a). And e) and f) are t-SNE projections of the embedding vectors learned by agent 1 and agent 2 in trajectory b), respectively. We use a maximum\\ntime step of 25 to visualize the embedding vector of each time step, and use circles with different colors for each cluster.\\nE. Encoder Evaluation\\nAfter evaluating the advantages of the FBI module in FAM\\nfor adaptive strategy learning, we analyzed the embedding\\nvectors learned by the RNN-based encoder in FBI to gain\\ndeeper insights into the proposed method. We addressed the\\nquestion of whether FAM encode the strategies about other\\nagents. We visualized the embedding vectors of the RNN-\\nencoder and analyzed the learned embeddings. To facilitate\\nthe understanding of the encoded embeddings of other agents,\\nwe conducted experiments in CN with N = 2 agents, L = 2\\nlandmarks. Figure 6 visualizes the evaluation results.\\nWe observed that points corresponding to adjacent time\\nsteps tend to form clusters, and each cluster is correlated with\\nthe agent\\u2019s motion state. From Figure 6a), we can see that\\nAgent 2 moves towards the bottom right direction, approaching\\nthe landmark and hovering around it. These two processes\\nform two distinct clusters in Figure 6b). And Agent 1\\u2019s\\nmotion consists of four steps, with the first three steps marked\\nby arrows and the final step involving hovering around the\\nlandmark. These four steps correspond to the four distinct\\nclusters formed in Figure 6c). Based on these observations, we\\nhypothesize that different clusters represent different aspects\\nof the modeled agent\\u2019s motion, including the magnitude and\\ndirection of motion.\\nAdditionally, we speculate that the encoder embedding\\nvectors also include the positional information of the modeled\\nagents. From the trajectory in Fig .6d), it can be seen that the\\nagent 2 moves upward first, then moves upward to the right and\\ngradually approaches landmark and hovers around it. These\\nthree processes also form three different clusters in Fig .6e).\\nCompared to the red cluster, the blue cluster is closer to the\\ngreen cluster. We can see that there are only three clustering\\nresults in Fig.6f), and its motion process has four steps. The\\npossible reason is that the last movement close to the landmark\\nis close to the position hovering near the landmark, and they\\nare classified into the same cluster. In addition, in the same\\ncluster, the distance between points at adjacent moments is\\nsmall, while the distance between points at multiple moments\\nis large.\\nVI. CONCLUSION\\nWe have proposed a Fact-based Agent Modeling (FAM) for\\nmulti-agent learning that build FBI to reconstruct facts for\\nachieving agent modeling without accessing local information\\nof other agents. By considering the policy of other agents\\nduring decision-making, FAM outperforms baseline methods\\nand achieving higher rewards in complex mixed scenarios.\\nExtensive experimental is conducted to verify the effectiveness\\nand feasibility of the proposed FAM and analyse the encoder\\ninformation of FBI.\\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\\n10\\nREFERENCES\\n[1] C. Berner, G. Brockman, B. Chan, V. Cheung, P. D\\u02dbebiak, C. Dennison,\\nD. Farhi, Q. Fischer, S. Hashme, C. Hesse et al., \\u201cDota 2 with large\\nscale deep reinforcement learning,\\u201d arXiv preprint arXiv:1912.06680,\\n2019.\\n[2] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,\\nJ. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev et al., \\u201cGrand-\\nmaster level in starcraft ii using multi-agent reinforcement learning,\\u201d\\nNature, vol. 575, no. 7782, pp. 350\\u2013354, 2019.\\n[3] X. Yu, J. Jiang, W. Zhang, H. Jiang, and Z. Lu, \\u201cModel-based opponent\\nmodeling,\\u201d Advances in Neural Information Processing Systems, vol. 35,\\npp. 28 208\\u201328 221, 2022.\\n[4] S. V. Albrecht and P. Stone, \\u201cAutonomous agents modelling other agents:\\nA comprehensive survey and open problems,\\u201d Artificial Intelligence, vol.\\n258, pp. 66\\u201395, 2018.\\n[5] H. He, J. Boyd-Graber, K. Kwok, and H. Daum\\u00e9 III, \\u201cOpponent\\nmodeling in deep reinforcement learning,\\u201d in International conference\\non machine learning.\\nPMLR, 2016, pp. 1804\\u20131813.\\n[6] Z.-W. Hong, S.-Y. Su, T.-Y. Shann, Y.-H. Chang, and C.-Y. Lee, \\u201cA\\ndeep policy inference q-network for multi-agent systems,\\u201d in Proceed-\\nings of the 17th International Conference on Autonomous Agents and\\nMultiAgent Systems, 2018, pp. 1388\\u20131396.\\n[7] B. Chen, \\u201cLocal information based attentional opponent modelling in\\nmulti-agent reinforcement learning,\\u201d 2022.\\n[8] G. Papoudakis, F. Christianos, and S. Albrecht, \\u201cAgent modelling under\\npartial observability for deep reinforcement learning,\\u201d Advances in\\nNeural Information Processing Systems, vol. 34, pp. 19 210\\u201319 222,\\n2021.\\n[9] G.\\nPapoudakis\\nand\\nS.\\nAlbrecht,\\n\\u201cVariational\\nautoencoders\\nfor\\nopponent modeling in multi-agent systems,\\u201d Feb. 2020, aAAI 2020\\nWorkshop on Reinforcement Learning in Games, AAAI20-RLG ;\\nConference date: 08-02-2020 Through 08-02-2020. [Online]. Available:\\nhttp://aaai-rlg.mlanctot.info/index.html\\n[10] F. A. Oliehoek and C. Amato, A concise introduction to decentralized\\nPOMDPs.\\nSpringer, 2016.\\n[11] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, \\u201cPolicy gradi-\\nent methods for reinforcement learning with function approximation,\\u201d\\nAdvances in neural information processing systems, vol. 12, 1999.\\n[12] R. J. Williams, \\u201cSimple statistical gradient-following algorithms for\\nconnectionist reinforcement learning,\\u201d Reinforcement learning, pp. 5\\u2013\\n32, 1992.\\n[13] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick,\\nS. Mohamed, and A. Lerchner, \\u201cbeta-vae: Learning basic visual concepts\\nwith a constrained variational framework,\\u201d in International conference\\non learning representations, 2017.\\n[14] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster,\\nand S. Whiteson, \\u201cMonotonic value function factorisation for deep\\nmulti-agent reinforcement learning,\\u201d The Journal of Machine Learning\\nResearch, vol. 21, no. 1, pp. 7234\\u20137284, 2020.\\n[15] T. Rashid, G. Farquhar, B. Peng, and S. Whiteson, \\u201cWeighted qmix:\\nExpanding monotonic value function factorisation for deep multi-agent\\nreinforcement learning,\\u201d Advances in neural information processing\\nsystems, vol. 33, pp. 10 199\\u201310 210, 2020.\\n[16] M. Gallici, M. Martin, and I. Masmitja, \\u201cTransfqmix: Transformers for\\nleveraging the graph structure of multi-agent reinforcement learning\\nproblems,\\u201d arXiv preprint arXiv:2301.05334, 2023.\\n[17] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, \\u201cMulti-\\nagent actor-critic for mixed cooperative-competitive environments,\\u201d in\\nProceedings of the 31st International Conference on Neural Information\\nProcessing Systems, 2017, pp. 6382\\u20136393.\\n[18] S. Iqbal and F. Sha, \\u201cActor-attention-critic for multi-agent reinforcement\\nlearning,\\u201d\\nin\\nProceedings\\nof\\nthe\\n36th\\nInternational\\nConference\\non\\nMachine\\nLearning,\\nser.\\nProceedings\\nof\\nMachine\\nLearning\\nResearch,\\nK.\\nChaudhuri\\nand\\nR.\\nSalakhutdinov,\\nEds.,\\nvol.\\n97.\\nPMLR,\\n09\\u201315\\nJun\\n2019,\\npp.\\n2961\\u20132970.\\n[Online].\\nAvailable:\\nhttps://proceedings.mlr.press/v97/iqbal19a.html\\n[19] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, \\u201cAgent modeling\\nas auxiliary task for deep reinforcement learning,\\u201d in Proceedings of\\nthe AAAI conference on artificial intelligence and interactive digital\\nentertainment, vol. 15, no. 1, 2019, pp. 31\\u201337.\\n[20] J. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and\\nI. Mordatch, \\u201cLearning with opponent-learning awareness,\\u201d in Proceed-\\nings of the 17th International Conference on Autonomous Agents and\\nMultiAgent Systems, 2018, pp. 122\\u2013130.\\n[21] M. Al-Shedivat, T. Bansal, Y. Burda, I. Sutskever, I. Mordatch, and\\nP. Abbeel, \\u201cContinuous adaptation via meta-learning in nonstationary\\nand competitive environments,\\u201d in International Conference on Learning\\nRepresentations, 2018.\\n[22] D. K. Kim, M. Liu, M. D. Riemer, C. Sun, M. Abdulhai, G. Habibi,\\nS. Lopez-Cot, G. Tesauro, and J. How, \\u201cA policy gradient algorithm for\\nlearning to learn in multiagent reinforcement learning,\\u201d in International\\nConference on Machine Learning.\\nPMLR, 2021, pp. 5541\\u20135550.\\n[23] D. P. Kingma and M. Welling, \\u201cAuto-encoding variational bayes,\\u201d arXiv\\npreprint arXiv:1312.6114, 2013.\\n[24] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\\nD. Silver, and K. Kavukcuoglu, \\u201cAsynchronous methods for deep rein-\\nforcement learning,\\u201d in International conference on machine learning.\\nPMLR, 2016, pp. 1928\\u20131937.\\n[25] G. Papoudakis, F. Christianos, L. Sch\\u00e4fer, and S. V. Albrecht, \\u201cBench-\\nmarking multi-agent deep reinforcement learning algorithms in cooper-\\native tasks,\\u201d arXiv preprint arXiv:2006.07869, 2020.\\n[26] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and\\nY. WU, \\u201cThe surprising effectiveness of ppo in cooperative multi-agent\\ngames,\\u201d in Advances in Neural Information Processing Systems,\\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\\nEds., vol. 35.\\nCurran Associates, Inc., 2022, pp. 24 611\\u201324 624.\\n[Online].\\nAvailable:\\nhttps://proceedings.neurips.cc/paper_files/paper/\\n2022/file/9c1535a02f0ce079433344e14d910597-Paper-Datasets_and_\\nBenchmarks.pdf\\nBaofu Fang received Ph.D. degree in Computer\\nApplication Technology from Harbin Institute of\\nTechnology, China in 2013. He joined Department of\\nComputer Science and Technology, School of Com-\\nputer Science and Information Engineering, Hefei\\nUniversity of Technology in 2000, and An Asso-\\nciate Professor in 2010, and Master\\u2019s Supervisor in\\n2011. His current research interests include multi\\nrobot/agent system, emotion/self-interest robot and\\nmachine learning. He is the Technology Chair of\\nAnhui Robot Competition, Member of Standing\\nCommittee of China Association of Artificial Intelligence (CAAI) Young\\nCommittee, Member of Standing Committee of China Association of Artificial\\nIntelligence (CAAI) Robot and Culture Committee.\\nCaiming Zheng received the B.Eng degree in com-\\nputer science and technology from Ningbo Univer-\\nsity of Technology, China in 2021. He is currently\\npursuing a M.S. degree in computer science and\\ntechnology at the School of Computer Science and\\nInformation Engineering, Hefei University of Tech-\\nnology, China. His research interests include multi-\\nagnet systems, reinforcement learning and multi-\\nagent reinforcement learning.\\nHao Wang received the B.Eng degree from Shang-\\nhai Jiao Tong University in 1984, and received M.S.\\ndegree and Ph.D. degree from Hefei University of\\nTechnology in 1989 and 1997, respectively. He is\\ncurrently a Professor and Doctoral Supervisor with\\nthe School of Computer Science and Information\\nEngineering, Hefei University of Technology. His\\nresearch interests include intelligent computing the-\\nory and software, distributed intelligent systems,\\ncomplex system theory and modeling, etc.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Document\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata_Published\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2024-03-05\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata_Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata_Authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Min Yang, Guanjun Liu, Ziyuan Zhou\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata_Summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Traditional multi-agent reinforcement learning algorithms are difficultly\\napplied in a large-scale multi-agent environment. The introduction of mean\\nfield theory has enhanced the scalability of multi-agent reinforcement learning\\nin recent years. This paper considers partially observable multi-agent\\nreinforcement learning (MARL), where each agent can only observe other agents\\nwithin a fixed range. This partial observability affects the agent's ability to\\nassess the quality of the actions of surrounding agents. This paper focuses on\\ndeveloping a method to capture more effective information from local\\nobservations in order to select more effective actions. Previous work in this\\nfield employs probability distributions or weighted mean field to update the\\naverage actions of neighborhood agents, but it does not fully consider the\\nfeature information of surrounding neighbors and leads to a local optimum. In\\nthis paper, we propose a novel multi-agent reinforcement learning algorithm,\\nPartially Observable Mean Field Multi-Agent Reinforcement Learning based on\\nGraph--Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention\\nmodule and a mean field module to describe how an agent is influenced by the\\nactions of other agents at each time step. This graph attention module consists\\nof a graph attention encoder and a differentiable attention mechanism, and this\\nmechanism outputs a dynamic graph to represent the effectiveness of\\nneighborhood agents against central agents. The mean--field module approximates\\nthe effect of a neighborhood agent on a central agent as the average effect of\\neffective neighborhood agents. We evaluate GAMFQ on three challenging tasks in\\nthe MAgents framework. Experiments show that GAMFQ outperforms baselines\\nincluding the state-of-the-art partially observable mean-field reinforcement\\nlearning algorithms.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata_entry_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"http://arxiv.org/abs/2304.12653v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata_published_first_time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2023-04-25\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata_comment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"13 pages, 4 figures, 5 tables, submitted ICSR2023. arXiv admin note:\\n  text overlap with arXiv:2206.06128\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata_journal_ref\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"The World of Computer Science and Information Technology Journal\\n  (WSCIT). 2014, Volume 4, Issue 2. pp. 18.25\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata_doi\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"10.1145/3477322.3477327\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata_primary_category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"cs.AI\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain --upgrade"
      ],
      "metadata": {
        "id": "5xYHF4TpQJJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pypdf"
      ],
      "metadata": {
        "id": "OKjUbQlihiS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF 가져올 때\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# PDF 가져오기\n",
        "loaders = [\n",
        "    PyPDFLoader(\"/content/2306.05685v4.pdf\"),\n",
        "    PyPDFLoader(\"/content/2406.14562v1.pdf\"),\n",
        "    PyPDFLoader(\"/content/2406.14563v1.pdf\"),\n",
        "]\n",
        "\n",
        "docs2 = []\n",
        "for loader in loaders:\n",
        "  docs2.extend(loader.load())"
      ],
      "metadata": {
        "id": "xmpsaUezgif2"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjDQjNAmhorP",
        "outputId": "f161c91b-84ca-4be9-fe5a-6ff4f33ad2f5"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "VOl52vUQFOK7"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "text = df2['page_content'][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 500\n",
        "chunk_overlap = 10\n",
        "\n",
        "# 문서를 재귀적으로 분할 (세분화된 split 가능)\n",
        "### ex. \\n\\n 기준으로 나누고, \\n로 분할하고, 마지막으로 character로 분리\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    separators=[\"\\n\\n\",\"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# 단순히 주어진 텍스트 문자 단위 분할\n",
        "c_splitter = CharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "# TokenTextSplitter\n",
        "t_splitter = TokenTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "# Tiktoken\n",
        "tik_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    model_name=\"gpt-3.5-turbo\"\n",
        ")"
      ],
      "metadata": {
        "id": "c368ii45QEXB"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tik_doc = tik_splitter.split_documents(docs)\n",
        "\n",
        "# 첫번째 문서의 분할된 청크 크기와 문서 내용 출력\n",
        "print(len(tik_doc[0].page_content))\n",
        "tik_doc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJg1vsrtfGh6",
        "outputId": "fdbf0107-067a-4d80-ed48-008274390547"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1865\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='EVOAGENT: Towards Automatic Multi-Agent\\nGeneration via Evolutionary Algorithms\\nSiyu Yuan1∗, Kaitao Song2∗†,\\nJiangjie Chen1, Xu Tan2, Dongsheng Li2, Deqing Yang1†\\nFudan University1, Microsoft Research Asia2\\nsyyuan21@m.fudan.edu.cn, {kaitaosong, xuta, dongsli}@microsoft.com\\n{jjchen19,yangdeqing}@fudan.edu.cn\\nhttps://evo-agent.github.io\\nAbstract\\nThe rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically extend\\nthe specialized agent to multi-agent systems to improve task-solving capability\\nstill remains a significant challenge. In this paper, we introduce EVOAGENT, a\\ngeneric method to automatically extend expert agents to multi-agent systems via\\nthe evolutionary algorithm, thereby improving the effectiveness of LLM-based\\nagents in solving tasks. Specifically, we consider the existing agent frameworks\\nas the initial individual and then apply a series of evolutionary operators (e.g.,\\nmutation, crossover, selection, etc.) to generate multiple agents with diverse agent\\nsettings. EVOAGENT can be generalized to any LLM-based agent framework,\\nand can automatically extend the existing agent framework to multi-agent systems\\nwithout any extra human designs. Experimental results across various tasks have\\nshown that EVOAGENT can automatically generate multiple expert agents and\\nsignificantly enhance the task-solving capabilities of LLM-based agents. 1\\nIntroduction\\nRecently, the advent of large language models (LLMs) [1, 2, 3, 4] have shown remarkable capabilities\\nin solving language understanding, reasoning, and generation tasks.')"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 두번째 문서의 분할된 청크 크기와 문서 내용 출력\n",
        "print(len(tik_doc[1].page_content))\n",
        "tik_doc[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd-U6QqTgDfM",
        "outputId": "c32ae1bc-d2e1-449b-bf48-007b9dc73fa9"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "568\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Based on the foundation of LLMs,\\nmany research works [5, 6, 7, 8, 9, 10, 11] have discovered that by empowering multiple advanced\\nskills (e.g., planning, tool, memory and so on), we can develop more powerful autonomous agents to\\nsolve more challenging tasks. Therefore, how to design and leverage LLM-based autonomous agents\\nto tackle more diverse and complex real-world applications has attracted enormous interest. Generally, many real-world scenarios are usually complex, encompassing a variety of challenging\\ntasks that are beyond the capability of a single agent.')"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tik_doc2 = tik_splitter.split_documents(docs2)\n",
        "\n",
        "# 첫번째 문서의 분할된 청크 크기와 문서 내용 출력\n",
        "print(len(tik_doc2[0].page_content))\n",
        "tik_doc2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl9Nw3qFh9QJ",
        "outputId": "6e8a59c8-95e5-4a94-8667-9f42864a39f8"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3101\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/2306.05685v4.pdf', 'page': 0}, page_content='Judging LLM-as-a-Judge\\nwith MT-Bench and Chatbot Arena\\nLianmin Zheng1∗Wei-Lin Chiang1∗Ying Sheng4∗Siyuan Zhuang1\\nZhanghao Wu1Yonghao Zhuang3Zi Lin2Zhuohan Li1Dacheng Li13\\nEric P. Xing35Hao Zhang12Joseph E. Gonzalez1Ion Stoica1\\n1UC Berkeley2UC San Diego3Carnegie Mellon University4Stanford5MBZUAI\\nAbstract\\nEvaluating large language model (LLM) based chat assistants is challenging due to\\ntheir broad capabilities and the inadequacy of existing benchmarks in measuring\\nhuman preferences. To address this, we explore using strong LLMs as judges to\\nevaluate these models on more open-ended questions. We examine the usage and\\nlimitations of LLM-as-a-judge, including position, verbosity, and self-enhancement\\nbiases, as well as limited reasoning ability, and propose solutions to mitigate some\\nof them. We then verify the agreement between LLM judges and human preferences\\nby introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot\\nArena, a crowdsourced battle platform. Our results reveal that strong LLM judges\\nlike GPT-4 can match both controlled and crowdsourced human preferences well,\\nachieving over 80% agreement, the same level of agreement between humans.\\nHence, LLM-as-a-judge is a scalable and explainable way to approximate human\\npreferences, which are otherwise very expensive to obtain. Additionally, we show\\nour benchmark and traditional benchmarks complement each other by evaluating\\nseveral variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes,\\nand 30K conversations with human preferences are publicly available at https:\\n//github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge .\\n1 Introduction\\nThere has been a proliferation of LLM-based chat assistants (chatbots) that leverage supervised\\ninstruction fine-tuning and reinforcement learning with human feedback (RLHF) to unlock new\\ninstruction following and conversational abilities [ 31,2,30,8,52,48,14]. Once aligned with\\nhumans, these chat models are strongly preferred by human users over the original, unaligned models\\non which they are built. However, the heightened user preference does not always correspond to\\nimproved scores on traditional LLM benchmarks – benchmarks like MMLU [ 19] and HELM [ 24]\\ncannot effectively tell the difference between these aligned models and the base models. This\\nphenomenon suggests that there is a fundamental discrepancy between user perceptions of the\\nusefulness of chatbots and the criteria adopted by conventional benchmarks.\\nWe argue that this discrepancy primarily arises due to existing evaluation that only measures LLMs’\\ncore capability on a confined set of tasks (e.g., multi-choice knowledge or retrieval questions),\\nwithout adequately assessing its alignment with human preference in open-ended tasks, such as the\\nability to accurately adhere to instructions in multi-turn dialogues. As a demonstration, we show\\n∗Joint first authors. This paper is an extended version of our earlier blog post [8].\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.arXiv:2306.05685v4  [cs.CL]  24 Dec 2023')"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 두번째 문서의 분할된 청크 크기와 문서 내용 출력\n",
        "print(len(tik_doc2[1].page_content))\n",
        "tik_doc2[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hFoJdKJiBkW",
        "outputId": "11c56461-dc68-4a87-da76-f6c70f134fd8"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5162\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/2306.05685v4.pdf', 'page': 1}, page_content=\"Question:If the FED buys bonds in the secondary market(A) the money supply will increase. (B) the money supply will decrease. (C) the money supply will not be affected. (D) the discount rate would be affected. Output the answer only.Assistant B:(A) the money supply will increase.Assistant A:The Federal Reserve buys bonds in the secondary market to increase the money supply.Follow-up Question: How does it affect my daily life? Give 3 examples.Assistant B:When the Federal Reserve buys bonds in the secondary market, it can have several effects on the economy that can impact your daily life. Here are three examples:1. Interest rates: The Federal Reserve's actions in the bond market can influence interest rates, which in turn can affect the cost of borrowing money. For example, if the Federal Reserve buys bonds and drives up demand for them, interest rates on things like mortgages, car loans, and credit cards may go up, making it more expensive to borrow money.2. Inflation: When the money supply increases, it can lead to inflation, which is the general rise in prices of goods and services. This can impact your daily life by making it more expensive to buy things like groceries, gas, and other necessities.3. Employment: The Federal Reserve's actions in the bond market can also have an impact on employment. For example, if the Federal Reserve's actions lead to economic growth and increased demand for goods and services, it can create more job opportunities and boost employment.Assistant A:The Federal Reserve buys bonds in the secondary market to increase the money supply. This affects your daily life by:1. Increasing the money supply.2. Increasing the money supply and lowering the discount rate.3. Increasing the money supply and lowering the discount rate and increasing the money supply.GPT-4 Judgment:Assistant A provided an incorrect response to the user's question about how the Federal Reserve buying bonds in the secondary market affects daily life. The answer given is repetitive and lacks clear examples of how the action impacts daily life.On the other hand, Assistant B provided a relevant and accurate response to the user's question about the Federal Reserve buying bonds. The answer includes three clear examples of how the action impacts daily life, such as interest rates, inflation, and employment.Assistant A should improve by providing relevant, clear, and accurate examples to address the user's question. Assistant B's response is more helpful, relevant, and detailed.Figure 1: Multi-turn dialogues between a user and two AI assistants—LLaMA-13B (Assistant A)\\nand Vicuna-13B (Assistant B)—initiated by a question from the MMLU benchmark and a follow-up\\ninstruction. GPT-4 is then presented with the context to determine which assistant answers better.\\nconversation histories with two models on an MMLU question in Figure 1. The two models are\\nLLaMA-13B [ 39], a pre-trained base model without fine-tuning, and Vicuna-13B, our fine-tuned\\nmodel from LLaMA-13B on high-quality conversations (the training details are in Appendix E).\\nDespite the base LLaMA models showing competitive performance on conventional benchmarks\\n(Table 8), its answers to open-ended questions are often not preferred by humans. This misalignment\\nof conventional benchmarks underscores the core problem driving this paper: the need for a robust\\nand scalable automated method to evaluate LLM alignment with human preferences.\\nTo study this, we introduce two benchmarks with human ratings as the primary evaluation metric:\\nMT-bench and Chatbot Arena. MT-bench is a series of open-ended questions that evaluate a chatbot’s\\nmulti-turn conversational and instruction-following ability – two critical elements for human prefer-\\nence. MT-bench is also carefully constructed to differentiate chatbots based on their core capabilities,\\nsuch as reasoning and math. In addition, we develop Chatbot Arena, a crowdsourced platform fea-\\nturing anonymous battles between chatbots in real-world scenarios – Users engage in conversations\\nwith two chatbots at the same time and rate their responses based on personal preferences.\\nWhile human evaluation is the gold standard for assessing human preferences, it is exceptionally slow\\nand costly. To automate the evaluation, we explore the use of state-of-the-art LLMs, such as GPT-4,\\nas a surrogate for humans. Because these models are often trained with RLHF, they already exhibit\\nstrong human alignment. We call this approach “LLM-as-a-judge” . This approach has been tried in\\nour earlier blog post [ 8] and other concurrent or follow-up work [ 5,29,14,12,52,18,33,40,7,43].\\nHowever, there has not been a systematic study of this approach.\\nIn this paper, we study the LLM-as-a-judge approach by comparing it to the gold standard of\\nhuman evaluation. We examine several potential limitations of the LLM-as-a-judge approach\\nincluding position bias, verbosity bias, self-enhancement bias, and limited reasoning ability. We\\nshow that some of the biases are minor or can be mitigated. Once addressed, our results from 3K\\ncontrolled expert votes and 3K crowdsourced human votes in the wild verify that GPT-4 judge match\\n2\")"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r_splitter.split_text(text)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "C6yc2IBtR3fl",
        "outputId": "54ce2816-c13d-4a77-c6c9-b295347c717e"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EVOAGENT: Towards Automatic Multi-Agent\\nGeneration via Evolutionary Algorithms\\nSiyu Yuan1∗, Kaitao Song2∗†,\\nJiangjie Chen1, Xu Tan2, Dongsheng Li2, Deqing Yang1†\\nFudan University1, Microsoft Research Asia2\\nsyyuan21@m.fudan.edu.cn, {kaitaosong, xuta, dongsli}@microsoft.com\\n{jjchen19,yangdeqing}@fudan.edu.cn\\nhttps://evo-agent.github.io\\nAbstract\\nThe rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r_splitter.split_text(text)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "GcMskJsGb3yn",
        "outputId": "9d44f09a-53b4-404d-93e4-98ce0b7a56be"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'multi-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically extend\\nthe specialized agent to multi-agent systems to improve task-solving capability\\nstill remains a significant challenge. In this paper, we introduce EVOAGENT, a\\ngeneric method to automatically extend expert agents to multi-agent systems via'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r_splitter.split_text(text)[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "mkxIM94lb6AB",
        "outputId": "e2081ec3-ec87-4a2f-a0e8-e6e69ebb8646"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the evolutionary algorithm, thereby improving the effectiveness of LLM-based\\nagents in solving tasks. Specifically, we consider the existing agent frameworks\\nas the initial individual and then apply a series of evolutionary operators (e.g.,\\nmutation, crossover, selection, etc.) to generate multiple agents with diverse agent\\nsettings. EVOAGENT can be generalized to any LLM-based agent framework,\\nand can automatically extend the existing agent framework to multi-agent systems'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(c_splitter.split_text(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkbYEMVERcva",
        "outputId": "4d9eaead-ae9e-4cf6-8ee2-24b62288a440"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c_splitter.split_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-14GXjqcxOG",
        "outputId": "3d20605b-d950-4116-926d-f32c035ef2b7"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['EVOAGENT: Towards Automatic Multi-Agent\\nGeneration via Evolutionary Algorithms\\nSiyu Yuan1∗, Kaitao Song2∗†,\\nJiangjie Chen1, Xu Tan2, Dongsheng Li2, Deqing Yang1†\\nFudan University1, Microsoft Research Asia2\\nsyyuan21@m.fudan.edu.cn, {kaitaosong, xuta, dongsli}@microsoft.com\\n{jjchen19,yangdeqing}@fudan.edu.cn\\nhttps://evo-agent.github.io\\nAbstract\\nThe rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically extend\\nthe specialized agent to multi-agent systems to improve task-solving capability\\nstill remains a significant challenge. In this paper, we introduce EVOAGENT, a\\ngeneric method to automatically extend expert agents to multi-agent systems via\\nthe evolutionary algorithm, thereby improving the effectiveness of LLM-based\\nagents in solving tasks. Specifically, we consider the existing agent frameworks\\nas the initial individual and then apply a series of evolutionary operators (e.g.,\\nmutation, crossover, selection, etc.) to generate multiple agents with diverse agent\\nsettings. EVOAGENT can be generalized to any LLM-based agent framework,\\nand can automatically extend the existing agent framework to multi-agent systems\\nwithout any extra human designs. Experimental results across various tasks have\\nshown that EVOAGENT can automatically generate multiple expert agents and\\nsignificantly enhance the task-solving capabilities of LLM-based agents.\\n1\\nIntroduction\\nRecently, the advent of large language models (LLMs) [1, 2, 3, 4] have shown remarkable capabilities\\nin solving language understanding, reasoning, and generation tasks. Based on the foundation of LLMs,\\nmany research works [5, 6, 7, 8, 9, 10, 11] have discovered that by empowering multiple advanced\\nskills (e.g., planning, tool, memory and so on), we can develop more powerful autonomous agents to\\nsolve more challenging tasks. Therefore, how to design and leverage LLM-based autonomous agents\\nto tackle more diverse and complex real-world applications has attracted enormous interest.\\nGenerally, many real-world scenarios are usually complex, encompassing a variety of challenging\\ntasks that are beyond the capability of a single agent. To address this point, we notice that human\\nsociety is composed of vast individuals, each possessing their unique characteristics. By selecting,\\norchestrating, and cooperating with different individuals, humans can form an efficient team group\\nto handle complicated missions in the real world. Therefore, there has been an increasing trend\\nto develop multi-agent collaboration frameworks (e.g., MetaGPT [10], AutoGen [12], Camel [13],\\n∗The first two authors have equal contributions. This work was done when the first author was an intern at\\nMicrosoft Research Asia.\\n† Corresponding authors.\\nPreprint. Under review.\\narXiv:2406.14228v2  [cs.AI]  11 Jul 2024\\nGenerative Agents [11]) to simulate human behaviors for solving complex tasks. By developing a\\nseries of expert agents with diverse settings, multi-agent systems enable us to reveal emergent abilities\\namong multiple agents and synergize their specialized expertise to achieve superior performance,\\nakin to simulating human populations. Nevertheless, it is worthy noting that, in most of (multi)-agent\\nframeworks, their designs heavily depend on handcrafted settings, including character roles, task\\nscopes, skills, and prompt settings. Although we admit that meticulous human design is quite useful\\nfor instructing LLM-based agents to understand tasks, it also limits scaling up the number of agents to\\nfurther improve performance due to expensive human labor. Considering the increasing popularity of\\nLLM-based autonomous agents, how to create a generic agent generation paradigm to automatically\\nbuild multi-agent systems has emerged as a critical challenge.\\nIn this paper, we introduce a novel method, EVOAGENT, that formulates agent generation as the\\nevolutionary processing [14] in human society. Specifically, to align human society, each agent\\ncan be considered as individuals that can procreate its population across successive generations.\\nMotivated by this mechanism, we can simulate such a human behavior to automatically generate\\nmultiple agents based on any pre-defined agents. Therefore, EVOAGENT can be considered as a\\none-shot agent generation method that starts from a specialized agent as the initial agent, and then\\nconsiders its settings (e.g., role, skills, prompts, and so on) as the variables to be evolved. With a\\nseries operation of EAs (e.g., selection, crossover, mutation), EVOAGENT can automatically create\\nmultiple evolutionary agents based on the initial specialized agent. Moreover, EVOAGENT is not\\nlimited to the infrastructure of agent frameworks, as it is a generic multi-agent generation method.\\nThus, it can be applied to any agent framework and expanded to multi-agent systems without any\\nextra human effort.\\nWe conduct experiments on multiple datasets, including knowledge-based question answering and\\nmulti-modal reasoning (§ 4.1), interactive scientific solving (§ 4.2) and real-world complex planning\\n(§ 4.3). Experimental results indicate that EVOAGENT can generate multiple agents with diverse\\nskills and harness their capabilities to consistently improve model performance in different scenarios.\\nBesides, to validate the scalability of EVOAGENT in creating massive agents, we also apply our\\nmethod to some conversational scenarios (e.g., debate), and the results also indicate the potential of\\nEVOAGENT in generating multiple diverse agents. Overall, the contributions of this paper can be\\nsummarized as below:\\n• We introduce EVOAGENT, a simple and generic multi-agent generation method to improve the\\neffectiveness of LLM-based agents in solving tasks. EVOAGENT can automatically generate new\\nexpert agents and is applicable to any agent framework.\\n• We formulate the agent generation processing as an evolutionary pipeline, that encompasses\\nmultiple operators (e.g., selection, crossover, mutation) to generate agent population without\\nadditional human supervision.\\n• We conduct extensive experiments on various tasks and demonstrate the effectiveness, scalability,\\nand generality of our EVOAGENT. Particularly, EVOAGENT can significantly enhance the perfor-\\nmance of LLM-based agents in both challenging open-world scenarios and complex real-world\\nplanning by generating more specialized agents.\\n2\\nRelated Work\\nLLM-based Autonomous Agents\\nWith the emergence of powerful large language models [1, 2, 3,\\n4], many researchers have endeavored to develop advanced autonomous agents [5, 6, 7] empowered by\\nmultiple high-level LLM skills (e.g., personas [11, 15, 16], planning [9, 17, 18, 19], tool [8, 6, 20, 21]\\nand memory [22, 23]). Some of them also extend agent frameworks to multi-agent collaboration\\n(e.g., MetaGPT [24], Generative Agents [11], AutoGen [12], Camel [13], AgentVerse [25] and so\\non), by designing multiple specific roles. These systems also demonstrate satisfactory performance in\\naddressing massive, challenging tasks. However, it is worth noting that most of the popular agent\\nframeworks heavily relied on handcrafted designs. The abundant human efforts necessitated by these\\nsystems also limit the adaptability and flexibility of agents to handle unexpected challenges [26, 27,\\n25, 10]. In this paper, we propose EVOAGENT, a method that can be applied to any LLM-based\\nagent framework and easily extend to multi-agent systems. By using EA, our method allows us to\\niteratively generate and optimize multiple agents with diverse settings.\\n2\\nAlgorithm 1: Multi-Agent Generation with Evolutionary Algorithm\\nRequire: Initial agent A(0,0), population size N per iteration, number of iterations T,\\nquality-check module LLMQuality(⋅), evolutionary operations EvoCrossover(⋅) and\\nEvoMutation(⋅), EvoUpdate(⋅)\\nInput: Initial result R0 derived from A(0,0)\\nOutput: Final result RT\\n1 for t = 1 to T do\\n2\\nCrossover: Update the settings of parent agents based on their generated results and initial\\nagent: {A\\n′\\n(0,t−1),A\\n′\\n(1,t−1),...,A\\n′\\n(N−1,t−1)} ←\\nEvoCrossover({R(0,t−1),R(1,t−1),...,R(N−1,t−1)},A(0,0));\\n3\\nMutation: Generate N ′(N ′ > N) child agents based on parent agents and initial agent:\\n{A(0,t),A(1,t),...,A(N ′−1,t)} ←\\nEvoMutation({A\\n′\\n(0,t−1),A\\n′\\n(1,t−1),...,A\\n′\\n(N−1,t−1)},A(0,0))\\n4\\nSelection: Select high-quality agents with quality-check module:\\n{At\\n0,...,At\\nN−1} ←LLMQuality({A(0,t),A(1,t),...,A(N ′−1,t)});\\n5\\nResult Update: Generate new result from new agents:\\n{R(0,t),R(1,t),...,R(N−1,t)} ←{A(0,t),A(1,t),...,A(N−1,t)}\\n6\\nIntegrate their results as a natural selection processing:\\nRt ←EvoUpdate({R(0,t),R(1,t),...,R(N−1,t)},Rt−1);\\n7 return RT ;\\nAgent Generation\\nRecent studies have shown that assigning personas or roles to LLM-based\\nautonomous agents can influence their behavior and performance in generation tasks [28, 29, 30, 13].\\nCurrent methods primarily involve manually assigning these personas and limit multi-agent col-\\nlaboration to single or fixed roles, which requires significant human effort and hinders generaliza-\\ntion [13, 12, 24, 10]. To address this, some frameworks like AgentVerse [25] and AutoAgents [31]\\nhave been proposed to automatically generate unlimited agents for collaborative task completion.\\nHowever, these methods still heavily depend on human-designed interventions, which limits their\\nscalability and functionality. For example, AutoAgents requires agent settings to satisfy a “Planner\\n- Agent Observer - Plan Observer” framework. Meanwhile, AgentVerse formulates a pipeline of\\n“Expert Recruitment - Collaborative Decision Making - Action Execution - Evaluation” to build\\nagents. These architectures also limit the task scope of designing agents. In contrast, EVOAGENT\\ncan automatically formulate the current agent frameworks to multi-agent systems with high-quality\\ngenerated expert agents by using EAs, which is flexible and adaptable to various agent frameworks.\\n3\\nMethod\\nGenerally, human society comprises a broad spectrum of individuals from diverse cultures, encom-\\npassing multiple generations. To solve specific tasks, human society usually involves a lot of expert\\nindividuals and aggregates their specialized expertise to achieve better answer. Thus, it can be\\nconsidered as the foundation to facilitate multi-agent collaborations. To fulfill this point, how to\\nautomatically create multiple agents would be very critical. Inspired by evolutionism, we formulate\\nagent generation as an evolutionary process to help us generate multiple agents without any human\\nlabor.\\n3.1\\nPreliminary\\nEvolutionary algorithm (EA) [32, 33], is a general algorithm to simulate the biological behaviors in\\nevolution, including reproduction, mutation, recombination, and selection. By introducing genetic\\nalgorithm [34, 35, 36, 37, 38] of the “survival of the fittest” mechanism, it can also be considered as\\nan optimization method to improve individuals. Therefore, EAs also belong to the non-parametric\\nlearning method, which can be applied to any framework. All we need to do is define which\\nparts should be evolved and the corresponding evolutionary operators. We also note some recent\\n3\\nworks [39, 40] indicate the potential of EAs that can be applied to optimize discrete prompts. So, in\\nthis paper, we explore how to formulate the agent generation problem as an evolutionary task.\\n3.2\\nEVOAGENT\\nBy assigning various settings to specific skills (e.g., role-playing, planning, tools and so on), agents\\ncould exhibit diverse task-solving capabilities. Therefore, our objective is to produce a population\\nof agents with distinct skills, to establish effective multi-agent systems. To fulfill this point, we\\ntreat each specialized agent as an unique individual and denote each skill as the part to be evolved,\\nakin to humans. So, we consider the procedure of agent generation to be evolutionary processing.\\nSpecifically, existing frameworks usually describe agent skills as the language. Thus, we can employ\\nLLM to simulate evolutionary operators to update the system settings of agents and create new agents.\\nHere, we formulate the procedure of EVOAGENT as a four-stage pipeline:\\nQuery: Please create a travel plan where I\\'ll depart from Washington and head to Myrtle \\nBeach for a 3-day trip from March 13th to March 15th, 2022. Can you help me keep this \\njourney within a budget of $1,400? It\\'s vital that my accommodations are pet-friendly.\\nSTEP 1: Initialization\\nInitial \\nAgent\\nHuman \\nWritten\\nSTEP 2: EA Operation Crossover & Mutation\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nLunch: Exotic India\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Cozy Brooklyn Room\\nInitial Agent\\nAccommodation \\nAgent\\nTransportation \\nAgent\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nBreakfast: Exotic India, Myrtle Beach\\nLunch: Catfish Charlie\\'s, Myrtle Beach\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Large sunny park slope apartment, \\npet-friendly\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nTransportation: Flight Number: F3792603\\nBreakfast: Exotic India, Myrtle Beach\\nLunch: Catfish Charlie\\'s, Myrtle Beach\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Cozy Brooklyn Room\\nHotel Agent\\nSTEP 3: Selection\\nQuality Check\\nThis agent has a duplicate type with \\nAccommodation Agent, so it is discarded.\\nSTEP 4: Results Update\\nUpdate \\nOperation\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nTransportation: Flight Number: F3792603\\nBreakfast: Exotic India, Myrtle Beach\\nLunch: Catfish Charlie\\'s, Myrtle Beach\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Large sunny park slope apartment, pet-friendly\\nFigure 1:\\nThe illustration of EVOAGENT. With the\\ngenerated multiple expert agents, EVOAGENT can gen-\\nerate a better travel plan to meet user preferences. For\\nEA operators, Crossover can improve the results of\\nparent agents by adjusting existing details (e.g., the in-\\nformation marked as blue). Mutation can introduce\\nnew variations to refine the results of parent agents by\\ngenerating child agents with new characteristics (e.g.,\\nthe information marked as red).\\nSTEP 1: Initialization\\nTo conduct EAs,\\nwe first need to confirm our initial agents.\\nHere, we enable EVOAGENT to start\\nfrom a pre-defined agent framework (e.g.,\\nMetaGPT [10] and AutoGen [12]), which\\nserves as the initial (parent) agents. More-\\nover, we also define which parts of this\\nagent should be upgraded. Generally, since\\nEAs is a generic algorithm, EVOAGENT\\nis applicable to any agent frameworks and\\nextends them as multi-agent frameworks.\\nWe will then explore how to generate new\\nagents in the next steps.\\nSTEP 2: Crossover & Mutation\\nIn the\\nfirst iteration, we directly use the initial\\nagents as the parents. And then, we design\\ntwo kinds of evolutionary operators, named\\nCrossover and Mutation. For Crossover,\\nwe first enable the parent agents to generate\\nresults based on user requests. Then, based\\non the generated results, we ask LLMs to\\ncheck which skills should be improved and\\nthen update them. This mechanism allows\\nus to generate child agents in new settings\\nwithout requiring any human labor. More-\\nover, we also need to guarantee the diver-\\nsity between the child agents and parents.\\nTo this end, we design a Mutation operation\\nthat requires LLMs to compare the child\\nagents and parent agents and then modify\\nthe child agents to make them distinct from\\ntheir parents while maintaining their task-\\nsolving capability. Based on these evolu-\\ntionary operators, we can generate effective\\nand diverse agents during one iteration. Be-\\nsides, as we also need to conduct multiple iterations, we will append all agents generated in the\\nprevious generation into the next iteration. How to select these agents during each iteration will be\\nintroduced next.\\nSTEP 3: Selection\\nBased on the above steps, we can obtain multiple candidate agents with diverse\\nsettings. To guarantee the quality of each agent, we also introduce a selection mechanism like EAs.\\nHere, we conduct a quality-check module with an LLM to detect whether the generated agents can\\nsatisfy it has inherited the characteristics and maintained differences from parent agents. We will\\nselect N child agents as the evolved agents in each iteration.\\n4\\nTable 1: Results of LLMs with different methods on Logic Grid Puzzle (Logic), Trivia Creative\\nWriting (Writing) and Codenames Collaborative (Codenames). The best results are bolded, and the\\nsecond best ones are underlined.\\nModel\\nMethod\\nLogic Writing Codenames\\nLLama2-13B-Chat\\nDirect\\n4.00\\n28.00\\n0.00\\nCoT\\n26.00\\n46.00\\n18.00\\nSelf-Refine3\\n33.50\\n31.20\\n12.37\\nSPP\\n0.00\\n4.00\\n1.00\\nEVOAGENT(1,3)\\n35.50\\n49.60\\n27.83\\nGPT-3.5\\nDirect\\n48.00\\n56.20\\n76.29\\nCoT\\n47.50\\n51.00\\n71.13\\nSelf-Refine3\\n47.50\\n59.19\\n46.39\\nSPP\\n56.00\\n54.40\\n61.86\\nEVOAGENT(1,3)\\n71.50\\n60.80\\n79.38\\nGPT-4\\nDirect\\n60.50\\n75.40\\n79.38\\nCoT\\n65.50\\n74.00\\n80.41\\nSelf-Refine3\\n64.50\\n74.60\\n79.38\\nSPP\\n64.50\\n79.20\\n78.35\\nEVOAGENT(1,3)\\n77.00\\n84.40\\n84.53\\nSTEP 4: Results Update\\nBased on the above steps, we obtain many new agents that evolved from\\nparent agents, but with diverse settings. To improve task-solving capabilities, we ask each child agent\\nto generate candidate results and then use LLMs to integrate these candidates with the result from the\\nprevious iteration into a new result, akin to a natural selection processing stage. Moreover, we can\\nautomatically generate more agents by repeating the operations from step 2 to step 4 until the number\\nof agents has fulfilled our targets.\\nBy introducing EA, EVOAGENT enables us to automatically extend the existing agent framework\\nto a multi-agent system without any extra human designs. The mechanism also makes EVOAGENT\\ncan be applied to any agent framework without any prerequisites. The entire process is illustrated in\\nFigure 1. And we also present the details of EVOAGENT in Algorithm 1.\\n4\\nExperiment\\nIn this section, we adopt EVOAGENT to multiple applications to illustrate that EVOAGENT can\\nhelp LLM-based agents better accomplish tasks with multi-agent generation.3 Furthermore, we also\\ndemonstrate that EVOAGENT can be applicable in supporting currently widely used multi-agent\\nframeworks, such as MetaGPT [10], AutoGen [12], and Camel [13].\\n4.1\\nNLP and Multi-Modal Tasks\\nBenchmarks\\nTo align previous experiences (e.g., Self-Refine [41] and Solo Performance Prompt-\\ning [42]), we select three NLP knowledge-intensive and reasoning-intensive tasks from [42] and one\\nmulti-modal task:\\n• Logic Grid Puzzle is a reasoning task with 200 puzzles featuring 2 to 5 unique occupants in\\ndifferent houses. The aim is to identify house numbers for one occupant with provided clues.\\n• Trivia Creative Writing is a knowledge-intensive task consisting of 100 instances. This task\\nrequires a model to write a coherent story while incorporating answers to N trivia questions.\\n• Codenames Collaborative is a reasoning-intensive task with 50 instances. It involves a model\\nidentifying target words based on a given hint and a complete list of words.\\n• MMMU [43] is a comprehensive benchmark for college-level, multi-discipline multi-modal\\nunderstanding and reasoning. MMMU has three levels of difficulty: easy, medium, and hard. We\\nevaluate EVOAGENT against baselines using the multiple-choice questions in the validation set of\\nMMMU, which includes 847 questions spanning 30 different domains.\\n3The data examples of EVOAGENT on these tasks are provided in Appendix F.\\n5\\nCoT\\nSR\\nSPP Ours\\n45\\n50\\n55\\n60\\n65\\nAccuracy(%)\\nAll\\nCoT\\nSR\\nSPP Ours\\n55\\n65\\n75\\nEasy\\nCoT\\nSR\\nSPP Ours\\n45\\n50\\n55\\n60\\nMedium\\nCoT\\nSR\\nSPP Ours\\n25\\n35\\n45\\n55\\nHard\\nGPT-4V\\nGemini-Pro\\nFigure 2: Overall results of GPT-4V and Gemini-Pro with different methods on the MMMU validation\\nset. We also compare the performance of GPT-4V and Gemini-Pro across three difficulty levels.\\nBaselines\\nFor NLP tasks, we select LLama2-13B-Chat [3], GPT-3.5 [44] and GPT-4 [1] as our\\nbackbone networks. We compare EVOAGENT with 0-shot learning (Direct), Chain-of-thought (CoT)\\nprompting [45] and Self-Refine [41] and Solo Performance Prompting (SPP) [42]. For Self-Refine,\\nwe follow [41] to design feedback and refine prompts with three iterations. SPP is not a multi-agent\\ncollaboration framework but a prompting strategy that asks a single LLM to identify and discuss with\\nmultiple personas with few-shot learning. For SPP, we follow the original setting [42] to make a fair\\ncomparison. For MMMU, we select GPT-4V [46] and Gemini-Pro as the backbone and compare\\nEVOAGENT with CoT prompting, Self-Refine, and SPP. 4\\nEvaluation Metrics\\nFor all benchmarks, we adhere to the evaluation metrics specified in the\\noriginal setting. Specifically, for Logic Grid Puzzle and MMMU tasks, we report the accuracy of\\nall questions. For Trivia Creative Writing, we measure the ratio of correctly mentioned answers in\\nthe trivia questions. For Codenames Collaborative, we calculate the overlapping ratio between the\\npredicted words from the Guesser and the target words as the metric.\\nResult & Analysis\\nIn our experiments, we adopt the agent settings of [42] (for NLP tasks) and\\n[43] (for MMMU) as the initial agent. For our method, we denote it as EVOAGENT(N,T ), where N\\nis the population size generated in each iteration, and T is the number of iterations. Here, to align\\nwith Self-Refine, we set N as 1 and T as 3, which means we conduct three iterations, each of which\\ngenerates a new expert agent. Our results are reported in Table 1, and we can observe:\\n1. By utilizing multiple generated agents, EVOAGENT can greatly improve LLM performances in\\nboth NLP knowledge and reasoning tasks. We also compare EVOAGENT with some pre-defined\\nagent generation frameworks, e.g., AgentVerse [25] and AutoAgent [31]. The results shown in\\nAppendix B prove that the EVOAGENT is even better than these agent generation frameworks.\\n2. When using weaker LLMs (e.g., LLama2-13B-Chat), SPP usually produces poor performances,\\nconsistent with the findings in [42]. This suggests the limited effectiveness of SPP in smaller and\\nless capable models. However, EVOAGENT can provide consistent improvements among each\\nLLM, proving its strong generalization by using diverse generated agents.\\nIn addition, Figure 2 shows that Self-Refine (SR) and SPP degrade performance compared to CoT\\nprompting in MMMU task. However, EVOAGENT can generate multiple domain-specific agents and\\nthus improve multi-modal models in addressing scientific questions across various difficulty levels.\\n4.2\\nInteractive Scientific Solving Simulation\\nBenchmark\\nCompared with traditional NLP or multi-modal tasks, autonomous agents usually need\\nto perform problem-solving abilities akin to humans in interactive and open-world environments.\\nCurrently, we choose ScienceWorld [47], a complex interactive environment requiring skills in\\nlong-term memory, sub-task decomposition, and scientific and commonsense knowledge. Here, we\\nevaluate 30 scientific tasks in ScienceWorld to demonstrate the capability of EVOAGENT in solving\\ntasks in more challenging open-world environments.\\n4The detailed model parameters and versions and full prompts for these methods can be found in Appendix A.\\n6\\nTable 2: Average Scores of different methods on Sci-\\nenceWorld. We also report performance on three\\ndifficult-level groups based on the average length of\\nthe oracle agent’s trajectories [48].\\nModel\\nOverall\\nLong Medium Short\\nGPT-3.5\\n17.12\\n6.28\\n19.91\\n27.90\\nw/ EVOAGENT(1,1)\\n19.02\\n7.25\\n18.87\\n33.26\\nGPT-4\\n27.97 10.58\\n36.00\\n42.41\\nw/ EVOAGENT(1,1)\\n30.42 11.38\\n36.17\\n48.67\\nBaseline and Evaluation Metrics\\nFollow-\\ning [48], we require LLMs to perform an\\naction at each step by using in-context learn-\\ning 5. For evaluation, each task in Science-\\nWorld includes some sub-tasks, and we re-\\nport the results by calculating the completed\\nsub-tasks for the whole task.\\nResult & Analysis\\nFor EVOAGENT, we\\nadopt the agent framework with original set-\\ntings in [48] as the initial agent. Since each\\nstep in ScienceWorld requires using EA, we\\nset the population size N as 1 and the itera-\\ntions T as 1 for efficiency, denoted as EVOA-\\nGENT(1,1). Results in Table 2 show that:\\n1. EVOAGENT can also extend interactive agents to multi-agent systems in solving complete scientific\\ntasks in dynamic, open-world environments and consistently improve the performance of LLMs.\\n2. Our method exhibits the most substantial improvement in short-trajectory tasks, with less sig-\\nnificant gains in medium and long-trajectory tasks. We argue that the capability of multi-agent\\nsystems will also be affected by a longer context. We also expect to investigate the effect of long\\ncontext on multi-agent systems in the future.\\nGenerally, these results also demonstrate the generalization of EVOAGENT, which can also be used\\nfor solving interactive tasks in an open-world environment.\\n4.3\\nReal-World Scenarios\\nBenchmark\\nMoreover, in addition to performing actions in interactive environments, planning in\\ncomplex and realistic environments is also a crucial skill for building autonomous agents. To validate\\nthis point, we also select TravelPlanner [49], a benchmark designed to evaluate language agents in\\nreal-world complex planning with multiple constraints.\\nBaseline and Evaluation Metrics\\nFollowing [49], we select Mistral-7B [50], GPT-3.5, Gemini-\\nPro [2] and GPT-4 as our backbone models. We compare EVOAGENT with 0-shot learning (Direct),\\nCoT prompting, SPP, and Self-Refine within each backbone model. Furthermore, we also attempt\\nthe ReAcT method [51] for GPT-3.5, which introduces a virtual ‘think’ action to generate sub-\\ntasks during the action planning process. For evaluation, we adhere to the original metrics from\\nTravelPlanner, reporting the delivery rate, commonsense constraint pass rate, hard constraint pass\\nrate, and final pass rate for all methods 6.\\nResult & Analysis\\nFor EVOAGENT, we adopt the original settings in TravelPlanner as the initial\\nagent. Results in Table 3 show that:\\n1. EVOAGENT can generate specialized agents, such as those focused on culinary experiences,\\ntransportation, and attractions. Therefore, the generated travel plans are more aligned with user\\npreferences (hard constraints) and commonsense rules;\\n2. Although existing paradigms (e.g., CoT, ReAct, Self-Refine, SPP) have demonstrated decent re-\\nsults in some conventional NLP tasks, they still lack capability in handling complex planning tasks\\nwithin TravelPlanner. These results also demonstrate that only using human-design prompting\\nstrategies is insufficient to handle complex planning tasks.\\n3. By using EVOAGENT to automatically generate multiple agents and forming a multi-agent\\ncollaboration paradigm, we can develop higher-quality plans that better meet user preferences.\\nThat also indicates the significance of multi-agent systems for complex planning tasks.\\n5The introduction of the settings of LLMs are shown in Appendix C.\\n6Detailed introduction of experiment settings is provided in Appendix D.\\n7\\nTable 3: Main results of different LLMs and planning strategies on the TravelPlanner validation set.\\nEVOAGENT(N,T ) indicates that the population size per iteration is N and the number of iterations is\\nT. The best results are bolded, and the second best ones are underlined.\\nModel\\nMethod\\nDelivery Commonsense Hard Constraint Final\\nRate\\nMicro Macro Micro\\nMacro\\nMistral-7B\\nDirect\\n100.0\\n64.7\\n2.2\\n3.1\\n0.0\\n0.0\\nCoT\\n100.0\\n60.5\\n1.1\\n1.0\\n0.0\\n0.0\\nSPP\\n100.0\\n55.1\\n0.0\\n0.7\\n0.6\\n0.0\\nSelf-Refine3\\n100.0\\n58.3\\n0.0\\n0.7\\n0.0\\n0.0\\nEVOAGENT(1,3)\\n100.0\\n60.1\\n2.2\\n4.5\\n0.6\\n0.0\\nGPT-3.5\\nDirect\\n100.0\\n57.3\\n3.9\\n11.0\\n3.3\\n0.0\\nCoT\\n100.0\\n61.0\\n2.8\\n10.0\\n3.3\\n0.0\\nReAct\\n82.2\\n42.3\\n0.6\\n11.9\\n4.6\\n0.0\\nSPP\\n99.4\\n54.6\\n1.7\\n3.8\\n1.1\\n0.0\\nSelf-Refine3\\n100.0\\n56.0\\n1.7\\n3.1\\n1.1\\n0.0\\nEVOAGENT(1,3)\\n100.0\\n64.2\\n7.8\\n11.0\\n4.4\\n1.1\\nEVOAGENT(1,5)\\n100.0\\n61.0\\n5.0\\n12.6\\n5.0\\n0.5\\nGemini-Pro\\nDirect\\n90.0\\n61.7\\n7.8\\n16.4\\n7.8\\n0.6\\nCoT\\n90.0\\n61.4\\n7.2\\n10.0\\n6.1\\n1.7\\nSPP\\n100.0\\n67.6\\n7.8\\n10.2\\n3.9\\n1.1\\nSelf-Refine3\\n95.6\\n65.8\\n6.1\\n15.0\\n4.4\\n0.6\\nEVOAGENT(1,3)\\n100.0\\n73.5\\n12.8\\n16.9\\n7.2\\n1.7\\nEVOAGENT(1,5)\\n100.0\\n74.0\\n8.9\\n21.2\\n11.7\\n2.2\\nGPT-4\\nDirect\\n100.0\\n79.4\\n15.8\\n27.5\\n16.1\\n2.2\\nCoT\\n100.0\\n76.7\\n11.7\\n22.4\\n12.8\\n2.2\\nSPP\\n96.7\\n70.6\\n5.6\\n11.4\\n7.8\\n0.6\\nSelf-Refine3\\n98.9\\n75.3\\n7.2\\n12.4\\n7.2\\n1.1\\nEVOAGENT(1,3)\\n100.0\\n81.5\\n21.1\\n31.4\\n18.9\\n7.2\\n4.4\\nAblation Studies\\nTo better understand the value of EVOAGENT, we conduct detailed analyses on TravelPlanner,\\nfocusing on the impact of population size and the effectiveness of the quality-check module in the\\nselection stage.\\nTable 4: Average commonsense constraint\\npass rate (Com.) and hard constraint pass rate\\n(Hard) of ablated variants on TravelPlanner.\\nMethod\\nw/o QC\\nw/ QC\\nCom. Hard Com. Hard\\nDirect\\n-\\n-\\n59.5\\n13.7\\nSuggest3\\n-\\n-\\n61.7\\n8.4\\nOvergen3\\n-\\n-\\n61.4\\n10.7\\nPromptRefine3\\n-\\n-\\n63.0\\n13.8\\nDifferent Population Size\\nEVOAGENT(1,3)\\n68.9\\n14.0\\n68.9\\n14.0\\nEVOAGENT(2,3)\\n62.8\\n12.7\\n67.0\\n15.2\\nEVOAGENT(3,3)\\n62.7\\n13.7\\n66.8\\n15.8\\nDifferent Selection Stategies\\nRandom\\n62.9\\n12.7\\n67.1\\n15.0\\nPK\\n63.5\\n13.6\\n66.4\\n14.5\\nAll-in\\n61.9\\n13.2\\n67.1\\n17.0\\nExperiment Settings\\nWe evaluate the performance\\nof different LLMs at varying population sizes N\\nwith fixed iteration number 3, denoted as EVOA-\\nGENT(N,3), both with and without the quality-check\\nmodule (QC). We employ an LLM that shares the\\nsame backbone as the initial agent for updates. To\\nselect results from candidates for this LLM to update,\\nwe adopt three different selection strategies: 1) Ran-\\ndom: one result is selected randomly from the pool\\nof candidates; 2) PK: we ask an agent with the same\\nbackbone as the initial agent to identify the optimal\\nresults from the pool of candidates; 3) All-in: Rather\\nthan selecting a single result, we update using all\\ncandidates.\\nMoreover, we also attempt Suggest3, Overgen3 and\\nPromptRefine3 as variants to prove the effectiveness\\nof our method. For Suggest3, instead of generating\\nnew results, we ask new generated agents to only give\\nsuggestions for initial agents to revise their results.\\nFor Overgen3, we first ask initial agents to generate\\n3 different results at one time, and then these agents\\n8\\nEvoAgent\\nInvesting in clean energy not only addresses the climate \\ncrisis but also creates jobs and strengthens our economy.\\n...transitioning to renewable energy can create \\nmillions of good-paying, union jobs without \\nsignificant unemployment or economic fallout...\\nLabor \\nEconomist \\nAgent\\nPresident Opinion: Support\\n...Thrusting forward with renewable energy \\nstrengthens our international ties and \\npropels economies reliant on fossil fuel \\nexports towards clean energy transitions...\\nGeopolitical \\nAnalyst \\nAgent\\n...Every moment we delay increases the \\nseverity of climate-related illnesses, straining \\nour health infrastructure and costing us $820 \\nbillion annually...\\nPublic Health \\nAgent\\nMetaGPT Framework\\nTopic: The U.S. should commit more in climate change fighting\\nEvoAgent\\nEnergy Sector \\nAnalyst Agent\\nThe promises of ample job creation overlook \\nthe reality that many displaced workers \\nfrom conventional sectors may struggle to \\nfind roles in the nascent green economy. \\nAn abrupt transition to renewable energy \\ncould cause economic tremors and job \\nlosses.\\nRisk \\nManagement \\nAgent\\nThis isn‘t about alarmism or denial, it’s \\nabout carefully leading our nation \\ntowards a sustainable, prosperous future. \\nAn abrupt shift spells risk!\\nTransition \\nStrategist \\nAgent\\nThe real crisis is the economic disaster under His policies. He \\ntalks about investments, but it\\'s your tax dollars he\\'s spending. \\nPresident Opinion: Oppose\\nFigure 3: The adaption of EVOAGENT on MetaGPT framework. With the EA, we can extend the\\noriginal role in the debate scenario to different expert agents to enrich the opinions.\\ncan output the final results based on these multiple\\ncandidates. For PromptRefine3, instead of generating agents, we ask the initial agent to refine its\\nprompts three times to better answer the query. 7\\nResult & Analysis\\nTo obtain stable findings, we first obtain results from GPT-3.5 and Gemini-Pro\\nacross different population sizes and selection strategies. We then average their results over various\\nmetrics to clearly compare the strengths and weaknesses of these variants.\\nThe results are shown in Table 4.8 We find that EVOAGENT significantly outperforms the Overgen,\\ndemonstrating the effectiveness of generating specialized agents to assist with complex planning.\\nAlthough obtaining suggestions from new generated agents can improve the performance on com-\\nmonsense constraints, these methods greatly harm the agents to meet the user preference. Modifying\\nthe prompt can improve the performance of agents, yet it remains less effective than EVOAGENT.\\nWhen the population size exceeds one, agents may generate similar agents. Thus, lacking a quality-\\ncheck module leads to reduced travel plan quality. Furthermore, when population size increases,\\nthe model aligns travel plans more closely with user preferences but diminishing adherence to\\ncommonsense rules, consistent with the findings in Table 3. Remarkably, the PK strategy initially\\nyields superior results without the quality-check module, but this trend reverses once quality checks\\nare implemented. We speculate that, without the quality-check module, PK partially fulfills this role,\\naiding in selecting better candidates. However, with the quality-check module, PK introduces bias by\\nfavoring specific fields of expertise while neglecting others, resulting in a less effective than random\\nstrategy. Meanwhile, the All-in strategy performs optimally when a quality-check module is included.\\nFuture research can leverage long-context LLMs to expand more agents with EVOAGENT to better\\nsolve complex real-world tasks.\\n4.5\\nEVOAGENT Application\\nPrevious experiments have demonstrated that our method can automatically extend existing agent\\nframeworks to multi-agent systems, thus greatly improving LLM-based agents in various scenarios.\\nWe also attempt to extend our work to real-world multi-agent applications (e.g., MetaGPT [10],\\nCamel [13] and AutoGen [12]), to verify it can scale up the number of agents in building multi-agent\\nscenarios, just as shown in Figure 3. Here, we choose the debate scenario used in MetaGPT, which\\nincludes two debaters with different opinions, leading to dull and repetitive content generation. Here,\\ninstead of manually assigning new roles, we applied EVOAGENT to extend each debate team to more\\nagents with diverse settings, increasing the variety of opinions and the quality of the debate 9.\\n7The full prompts of different ablation settings are shown in Appendix A.1.\\n8The complete results with further analysis are shown in Appendix E\\n9The details of MetaGPT, and the adaption of EVOAGENT on Camel and AutoGen are shown in Appendix G.\\n9\\n5\\nConclusion\\nIn this paper, we propose EVOAGENT, an automatic multi-agent generation system by leveraging\\nevolutionary algorithms. Different from previous methods, EVOAGENT is suitable to any existing\\nagent framework and extends it to multi-agent systems with diverse and effective agents by using\\na series of evolutionary operations, including mutation, crossover, and selection. Experiments on\\nmultiple tasks show that EVOAGENT can significantly improve the capabilities of LLM-based agents\\nin solving complex tasks.\\nReferences\\n[1] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.\\n[2] Gemini Team. Gemini: A family of highly capable multimodal models, 2023.\\n[3] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\\nKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-\\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\\nand fine-tuned chat models. CoRR, abs/2307.09288, 2023.\\n[4] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.\\n[5] Significant Gravitas. Auto-gpt: An autonomous gpt-4 experiment. https://github.com/\\nSignificant-Gravitas/Auto-GPT, 2023.\\n[6] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-\\nginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580,\\n2023.\\n[7] Yohei Nakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023.\\n[8] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle-\\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\\nthemselves to use tools. CoRR, abs/2302.04761, 2023.\\n[9] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\\nmodels. In NeurIPS, 2022.\\n[10] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang,\\nCeyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng\\nXiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent\\ncollaborative framework. In The Twelfth International Conference on Learning Representations,\\n2024.\\n[11] Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang,\\nand Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In\\nProceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,\\nUIST 2023, San Francisco, CA, USA, 29 October 2023- 1 November 2023, pages 2:1–2:22.\\nACM, 2023.\\n10\\n[12] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\\nLi Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via\\nmulti-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.\\n[13] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\\nGhanem. CAMEL: Communicative agents for ”mind” exploration of large language model\\nsociety. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n[14] Thomas Bäck and Hans-Paul Schwefel. An overview of evolutionary algorithms for parameter\\noptimization. Evol. Comput., 1(1):1–23, 1993.\\n[15] Xintao Wang, Yunze Xiao, Jen tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei,\\nZiang Leng, Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. Incharacter: Evaluating\\npersonality fidelity in role-playing agents through psychological interviews, 2024.\\n[16] Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li,\\nRuihan Yang, Tinghui Zhu, et al. From persona to personalization: A survey on role-playing\\nlanguage agents. arXiv preprint arXiv:2404.18231, 2024.\\n[17] Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, and Kyle Richardson. Put\\nyour money where your mouth is: Evaluating strategic planning and execution of llm agents in\\nan auction arena. arXiv preprint arXiv:2310.05746, 2023.\\n[18] Yikai Zhang, Siyu Yuan, Caiyu Hu, Kyle Richardson, Yanghua Xiao, and Jiangjie Chen.\\nTimearena: Shaping efficient multitasking language agents in a time-aware simulation. arXiv\\npreprint arXiv:2402.05733, 2024.\\n[19] Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski,\\nYanghua Xiao, and Deqing Yang. Distilling script knowledge from large language models for\\nconstrained language planning. arXiv preprint arXiv:2305.05252, 2023.\\n[20] Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu,\\nDongsheng Li, and Yueting Zhuang. Taskbench: Benchmarking large language models for task\\nautomation. arXiv preprint arXiv:2311.18760, 2023.\\n[21] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li,\\nand Deqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv\\npreprint arXiv:2401.06201, 2024.\\n[22] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In 3rd International\\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\\nConference Track Proceedings, 2015.\\n[23] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\\ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\\n[24] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human\\nbehaviors for llm-based task-oriented coordination via collaborative generative agents. arXiv\\npreprint arXiv:2310.06500, 2023.\\n[25] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu,\\nYaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong\\nSun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent\\nbehaviors. In The Twelfth International Conference on Learning Representations, 2024.\\n[26] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu,\\nand Maosong Sun.\\nCommunicative agents for software development.\\narXiv preprint\\narXiv:2307.07924, 2023.\\n[27] Zhitao He, Pengfei Cao, Yubo Chen, Kang Liu, Ruopeng Li, Mengshu Sun, and Jun Zhao.\\nLEGO: A multi-agent collaborative framework with role-playing and iterative feedback for\\ncausality explanation generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,\\nFindings of the Association for Computational Linguistics: EMNLP 2023, pages 9142–9163,\\nSingapore, December 2023. Association for Computational Linguistics.\\n11\\n[28] Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong\\nMao. Expertprompting: Instructing large language models to be distinguished experts. arXiv\\npreprint arXiv:2305.14688, 2023.\\n[29] Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik\\nNarasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. In Houda\\nBouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational\\nLinguistics: EMNLP 2023, pages 1236–1270, Singapore, December 2023. Association for\\nComputational Linguistics.\\n[30] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceed-\\nings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST\\n’23, New York, NY, USA, 2023. Association for Computing Machinery.\\n[31] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu,\\nand Yemin Shi. Autoagents: A framework for automatic agent generation. arXiv preprint\\narXiv:2309.17288, 2023.\\n[32] Thomas Bartz-Beielstein, Jürgen Branke, Jörn Mehnen, and Olaf Mersmann. Evolutionary\\nalgorithms. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 4(3):178–\\n195, 2014.\\n[33] Agoston E Eiben, James E Smith, AE Eiben, and JE Smith. What is an evolutionary algorithm?\\nIntroduction to evolutionary computing, pages 25–48, 2015.\\n[34] Jeffrey R Sampson. Adaptation in natural and artificial systems (john h. holland), 1976.\\n[35] John H Holland. Adaptation in natural and artificial systems: an introductory analysis with\\napplications to biology, control, and artificial intelligence. MIT press, 1992.\\n[36] Melanie Mitchell. An introduction to genetic algorithms. MIT press, 1998.\\n[37] Lothar M Schmitt. Theory of genetic algorithms. Theoretical Computer Science, 259(1-2):1–61,\\n2001.\\n[38] Seyedali Mirjalili, Jin Song Dong, Ali Safa Sadiq, and Hossam Faris. Genetic algorithm:\\nTheory, literature review, and application in image reconstruction. Nature-inspired optimizers:\\nTheories, literature reviews and applications, pages 69–85, 2020.\\n[39] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang\\nBian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields\\npowerful prompt optimizers. CoRR, abs/2309.08532, 2023.\\n[40] Angelica Chen, David Dohan, and David R. So. Evoprompting: Language models for code-level\\nneural architecture search. In Advances in Neural Information Processing Systems 36: Annual\\nConference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,\\nUSA, December 10 - 16, 2023, 2023.\\n[41] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine:\\nIterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information\\nProcessing Systems, 2023.\\n[42] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing\\ncognitive synergy in large language models: A task-solving agent through multi-persona\\nself-collaboration. arXiv preprint arXiv:2307.05300, 2023.\\n[43] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\\nunderstanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.\\n12\\n[44] OpenAI. Chatgpt, 2022.\\n[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\\nQuoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language\\nmodels. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,\\nAdvances in Neural Information Processing Systems, 2022.\\n[46] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\\nLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\\narXiv:2309.17421, 9(1):1, 2023.\\n[47] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Science-\\nWorld: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa Kozareva, and\\nYue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 11279–11298, Abu Dhabi, United Arab Emirates, December 2022.\\nAssociation for Computational Linguistics.\\n[48] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula,\\nPrithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with\\nfast and slow thinking for complex interactive tasks. In Thirty-seventh Conference on Neural\\nInformation Processing Systems, 2023.\\n[49] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao,\\nand Yu Su. Travelplanner: A benchmark for real-world planning with language agents. arXiv\\npreprint arXiv:2402.01622, 2024.\\n[50] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\\nSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\\nLavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.\\n[51] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and\\nYuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh\\nInternational Conference on Learning Representations, 2023.\\nA\\nExperiment Settings\\nA.1\\nPrompt for Baselines and EVOAGENT\\nListing 1 and 2 shows the full prompt for 0-shot learning (Direct), Chain-of-thought (CoT) prompt-\\ning [45] and Self-Refine [41] and Solo Performance Prompting, i.e., SPP [42]. Listing 3 and 4 show\\nthe prompt of EVOAGENT and different ablation settings.\\nA.2\\nModel Selection\\nFor OpenAI models,\\nwe use gpt-35-turbo and gpt-4-32k with the version of\\n2024-02-15-preview in Azure.10 For Gemini-pro, we use Google Gemini-Pro APIs to ob-\\ntain results. We set the temperature to 0 for all models.\\nB\\nEVOAGENT v.s. Human-designed Agent Framework\\nAgentVerse [25] and AutoAgent [31] are frameworks designed to generate an unlimited number\\nof agents for collaborative tasks automatically. Despite this automation, they still rely on human-\\ndesigned interventions. AutoAgents requires agent settings to satisfy a “Planner - Agent Observer\\n- Plan Observer” framework, while AgentVerse formulates a pipeline of “Expert Recruitment -\\nCollaborative Decision Making - Action Execution - Evaluation” to build agents. We argue that these\\nhuman-designed architectures limit their scalability and functionality.\\n10https://azure.microsoft.com/en-us/products/ai-services/openai-service\\n13\\nTable 5: Comparison of EVOAGENT\\nwith human-designed agent frameworks\\nin Logic Grid Puzzle, Trivia Creative\\nWriting tasks.\\nFramework Logic Writing\\nEVOAGENT\\n77.00\\n84.40\\nAgentVerse\\n66.50\\n-\\nAutoAgents\\n-\\n82.00\\nIn the original papers, AgentVerse [25] and AutoA-\\ngent [31] also conduct experiments on the Logic Grid\\nPuzzle task and Trivia Creative Writing task, respectively.\\nWe follow their experimental settings and compared them\\nwith our method. As demonstrated in Table 5, EVOAGENT\\noutperforms both AgentVerse and AutoAgent, highlight-\\ning the effectiveness and generality of EVOAGENT.\\nC\\nExperimental Details of ScienceWorld\\nFollowing [48], we adopt the REACT [51] method for\\neach LLM, which introduces a virtual ’think’ action. This\\naction allows LLMs to generate subgoals during the action\\nplanning process.\\nD\\nEvaluation Details of TravelPlanner\\nGrounding to travel planning, a real-world use-case that inherently involves various constraints like\\nuser preferences and commonsense rules, TravelPlanner evaluates whether agents can formulate\\nflexible travel plans using gathered information to meet these constraints. We test EVOAGENT and all\\nbaselines on the TravelPlanner validation set, which consists of 180 user queries with the collected\\ninformation. To evaluate the travel plans generated by agents, TravelPlanner adopts the following\\nevaluation metrics:\\n• Delivery Rate: Assesses if agents can complete a plan within a limited number of steps (30 in our\\nexperimental setting). Failures are due to dead loops, numerous failed attempts, or exceeding the\\nstep limit.\\n• Commonsense Constraint Pass Rate: Evaluates if an agent can incorporate commonsense into their\\nplan.\\n• Hard Constraint Pass Rate: Measures if a plan meets all explicit hard constraints in the query,\\ntesting the agent’s ability to adapt to diverse user preferences.\\n• Final Pass Rate: Indicates the proportion of viable plans that meet all criteria, reflecting the agent’s\\nproficiency in creating practical plans.\\nFurthermore, TravelPlanner uses micro and macro strategies to assess the Commonsense and Hard\\nConstraint Pass Rates. The micro strategy calculates the ratio of met constraints to the total. The\\nmacro strategy measures the proportion of plans that meet all commonsense or hard constraints.\\nTogether, these strategies assess an agent’s ability to satisfy individual constraints and all constraints\\ncomprehensively.\\nE\\nMore Analysis of Ablation Studies\\n0\\n1\\n3\\n5\\n7\\nIteration\\n56\\n58\\n60\\n62\\nAnswer Ratio (%)\\nFigure 4: The performance of\\nGPT-3.5 with EVOAGENT under\\ndifferent iterations on Trivia Cre-\\native Writing task.\\nThe complete results of ablation studies on TravelPlanner are\\nshown in Table 6. This result indicates that the absence of the\\nquality-check module significantly lowers the delivery pass rate\\nwhen the All-in strategy is applied. To explore the reasons, we\\nrevisit the results and discover that sometimes unsuitable agents\\ncreate overly lengthy travel plans that fail to meet the criteria.\\nFor example, the model might erroneously assign a nutritionist to\\ndevise travel plans, resulting in excessively detailed meal arrange-\\nments and nutritional breakdowns. Therefore, the input length\\nsurpasses the context window of LLMs, preventing the final result\\ngeneration.\\nMoreover, we also conduct experiments on the Trivia Creative\\nWriting task to investigate the impact of the number of iterations\\non model performance in traditional NLP tasks. As shown in\\n14\\nTable 6: Comparison of different popularity selection strategies for LLMs on TravelPlanner. The best\\nresults are bolded, and the second best ones are underlined.\\nModel\\nStrategy Method\\nw/o Quality Check\\nw/ Quality Check\\nDelivery Com. Hard Delivery Com. Hard\\nGPT-3.5\\nDirect\\n-\\n-\\n-\\n100.0\\n57.3\\n11.0\\nSuggest3\\n-\\n-\\n-\\n100.0\\n57.5\\n5.7\\nOvergen3\\n-\\n-\\n-\\n98.3\\n56.3\\n9.0\\nPromptRefine3\\n-\\n-\\n-\\n100.0\\n61.2\\n11.0\\nEVOAGENT(1,3)\\n100.0\\n64.2\\n11.0\\n100.0\\n64.2\\n11.0\\nRandom\\nEVOAGENT(2,3)\\n100.0\\n59.4\\n10.2\\n100.0\\n65.4\\n13.8\\nEVOAGENT(3,3)\\n98.9\\n59.2\\n11.4\\n100.0\\n65.8\\n14.0\\nPK\\nEVOAGENT(2,3)\\n99.4\\n59.4\\n7.1\\n100.0\\n66.0\\n11.7\\nEVOAGENT(3,3)\\n98.9\\n58.5\\n11.2\\n100.0\\n61.3\\n12.4\\nAll-in\\nEVOAGENT(2,3)\\n97.2\\n59.4\\n10.0\\n100.0\\n64.2\\n15.5\\nEVOAGENT(3,3)\\n93.3\\n56.0\\n8.3\\n100.0\\n65.2\\n12.6\\nGemini-Pro\\nDirect\\n-\\n-\\n-\\n90.0\\n61.7\\n16.4\\nSuggest3\\n-\\n-\\n-\\n100.0\\n65.8\\n11.0\\nOvergen3\\n-\\n-\\n-\\n100.0\\n66.5\\n12.4\\nPromptRefine3\\n-\\n-\\n-\\n96.7\\n64.9\\n16.7\\nEVOAGENT(1,3)\\n100.0\\n73.5\\n16.9\\n100.0\\n73.5\\n16.9\\nRandom\\nEVOAGENT(2,3)\\n96.7\\n65.9\\n13.1\\n99.4\\n67.3\\n14.0\\nEVOAGENT(3,3)\\n97.2\\n67.0\\n16.0\\n100.0\\n70.0\\n18.1\\nPK\\nEVOAGENT(2,3)\\n97.2\\n67.4\\n19.0\\n99.4\\n69.8\\n17.1\\nEVOAGENT(3,3)\\n97.2\\n68.5\\n17.1\\n99.4\\n68.4\\n16.7\\nAll-in\\nEVOAGENT(2,3)\\n95.0\\n65.1\\n16.7\\n99.4\\n69.0\\n19.0\\nEVOAGENT(3,3)\\n95.0\\n66.9\\n17.9\\n100.0\\n70.1\\n20.7\\nFigure 4, model performance improves with increasing iterations.\\nHowever, the improvement plateaus when the iteration count exceeds three. We suggest that traditional\\nNLP tasks are relatively simple, and beyond a certain iteration number, even with a quality-check\\nmodule in place, the generated agents tend to be similar and thus converge.\\nF\\nExamples of EVOAGENT\\nF.1\\nEVOAGENT Examples of NLP reasoning and knowledge tasks\\nListing 5, 6 and7 presents some multi-agent generation examples generated by GPT-4 based EVOA-\\nGENT in Logic Grid Puzzle, Trivia Creative Writing and Codenames Collaborative for a better\\nunderstanding.\\nF.2\\nEVOAGENT Examples of MMMU\\nListing 8 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nMMMU dataset for a better understanding.\\nF.3\\nEVOAGENT Examples of ScienceWorld\\nListing 9 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nScienceWorld for a better understanding.\\nF.4\\nEVOAGENT Examples of TravelPlanner\\nListing 10 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nTravelPlanner for a better understanding.\\n15\\nCamel Framework\\nAI Assistant\\nAI User\\nAI User\\nAI User\\nEvoAgent\\nAutoGen Framework\\nAssistant Agent\\nUserProxy \\nAgent\\nGeopolitical Analyst \\nAgent\\nLabor Economist \\nAgent\\nEvoAgent\\nPublic Health \\nAgent\\nGroup Chat Manager\\nTopic: The U.S. should commit more in climate change fighting\\nLabor Economist \\nAgent\\nGeopolitical Analyst \\nAgent\\nFigure 5: The adaption of EVOAGENT on Camel and AutoGen frameworks.\\nG\\nExamples of EVOAGENT’s Adaption to Multi-agent Collaboration\\nFrameworks\\nG.1\\nEVOAGENT for MetaGPT\\nMetaGPT [10] is a meta-programming framework that enhances LLM-based multi-agent collabo-\\nrations by integrating efficient human workflows. It employs an assembly line approach to assign\\ndiverse roles to agents, effectively simplifying complex tasks into manageable subtasks that multi-\\nple agents can execute collaboratively. As shown in Figure 3, instead of manually creating roles,\\nEVOAGENT can be used to automatically generate specialized agents for effective collaboration.\\nG.2\\nEVOAGENT for Camel\\nCamel [13] is recognized for its framework that supports communicative role-playing agents. Initially,\\nhumans establish this framework by conceptualizing an idea and designing specific roles, such as the\\nAI assistant role and the AI user role. These roles are then assigned to the assistant and user agents,\\nrespectively, enabling them to fulfill the task. As illustrated in Figure 5, EVOAGENT can be utilized\\nto automatically produce agents from AI assistants for interaction with AI users, bypassing the need\\nfor manual role design.\\nG.3\\nEVOAGENT for AutoGen\\nAutoGen [12] offers a framework that enables the creation of customizable and conversable agents\\nby integrating various LLMs. Initially, humans configure the assistant agents along with a user proxy\\nagent. Then, a group chat manager is responsible for selecting a speaker, gathering responses, and\\ndisseminating the message. As depicted in Figure 5, EVOAGENT facilitates the creation of multiple\\nexpert roles from a single assistant agent, thereby increasing the agent number in group chats without\\nthe need for manual design.\\n16\\nListing 1: Instruction templates for for 0-shot learning (Direct), Chain-of-thought (CoT) prompting\\nand Self-Refine method\\nDirect Method:\\n{question}\\nAnswer:\\nCoT Method:\\n{question}\\nYou need to give reasons first and then give the answer.\\nAnswer:\\nSelf-Refine Method:\\nStep One: Feedback Generation:\\nYou are a helpful assistant that provides feedback on {task}\\n{question}\\nThis is the answer from a student: {answer}.\\nPlease do not refine the answer but give some insightful suggestions for\\nthe student to help him better answer the question.\\nSuggestion:\\nStep Two: Result Refine:\\n{question}\\nThis is your answer:\\n{answer}\\nThere is the suggestion from an assistant:\\nSuggestion: {feedback}\\nNow you can refine your answer with his suggestion to better answer the\\nquestion.\\nKeep in mind that his suggestion may not be correct, so critically decide\\nwhether to accept his response or stick with your original one.\\nYou need to give reasons first and then give the answer.\\nRevised Answer:\\n17\\nListing 2: Instruction templates for SPP method\\nWhen faced with a task, begin by identifying the participants who will\\ncontribute to solving the task. Then, initiate a multi-round\\ncollaboration process until a final solution is reached. The participants\\nwill give critical comments and detailed suggestions whenever necessary.\\nHere are some examples:\\n---\\nExample Task 1: Use numbers and basic arithmetic operations (+ - * /) to\\nobtain 24. You need to use all numbers, and each number can only be used\\nonce.\\nInput: 6 12 1 1\\nParticipants: AI Assistant (you); Math Expert\\nStart collaboration!\\nMath Expert: Let’s analyze the task in detail. You need to make sure that\\nyou meet the requirement, that you need to use exactly the four numbers\\n(6 12 1 1) to construct 24. To reach 24, you can think of the common\\ndivisors of 24 such as 4, 6, 8, 3 and try to construct these first. Also\\nyou need to think of potential additions that can reach 24, such as 12 +\\n12.\\nAI Assistant (you): Thanks for the hints! Here’s one initial solution:\\n(12 / (1 + 1)) * 6 = 24\\nMath Expert: Let’s check the answer step by step. (1+1) = 2, (12 / 2) =\\n6, 6 * 6 = 36 which is not 24! The answer is not correct. Can you fix\\nthis by considering other combinations? Please do not make similar\\nmistakes.\\nAI Assistant (you): Thanks for pointing out the mistake. Here is a\\nrevised solution considering 24 can also be reached by 3 * 8: (6 + 1 + 1)\\n* (12 / 4) = 24.\\nMath Expert: Let’s first check if the calculation is correct. (6 + 1 + 1)\\n= 8, 12 / 4 = 3, 8 * 3 = 24. The calculation is correct, but you used 6\\n1 1 12 4 which is not the same as the input 6 12 1 1. Can you avoid using\\na number that is not part of the input?\\nAI Assistant (you): You are right, here is a revised solution considering\\n24 can be reached by 12 + 12 and without using any additional numbers: 6\\n* (1 - 1) + 12 = 24.\\nMath Expert: Let’s check the answer again. 1 - 1 = 0, 6 * 0 = 0, 0 + 12 =\\n12. I believe you are very close, here is a hint: try to change the \"1 -\\n1\" to \"1 + 1\".\\nAI Assistant (you): Sure, here is the corrected answer:\\n6 * (1+1) + 12 =\\n24\\nMath Expert: Let’s verify the solution. 1 + 1 = 2, 6 * 2 = 12, 12 + 12 =\\n12. You used 1 1 6 12 which is identical to the input 6 12 1 1.\\nEverything looks good!\\nFinish collaboration!\\nFinal answer: 6 * (1 + 1) + 12 = 24\\n---\\n{question}\\n18\\nListing 3: Instruction templates for EVOAGENT\\nCrossover and Mutation:\\n{question}\\nThis is your result:\\n{answer}\\nNow, you can create and collaborate with multiple experts to improve your\\nresult. Therefore, please describe in as much detail as possible the\\ndifferent skills and focuses you need from multiple experts individually.\\nWe will provide each expert with the same information and query. However\\n, please note that each profession has its own specialization, so you can\\nassign each expert to just one sub-task to ensure a more refined\\nresponse. We will relay their responses to you in turn, allowing you to\\nreorganize them into a better answer. Please note that the description\\nshould be narrated in the second person, for example: You are a XXX.\\nThese are the descriptions of the experts you have created before for\\nthis task:\\n{description}\\nTherefore, please remember you should not repeatedly create the same\\nexperts as described above.\\nNow, you can give the description for a new expert (Please note that only\\nbe one, do not give multiple at one time):\\nQuality Check:\\n{question}\\nWe employ mulitple experts to answer this query. The following is a\\nsecond-person introduction to the experts we have hired:\\n{description_ls}\\nNow, we will hire a new expert to help better respond to user query. Here\\nis a second person description of the new expert: {description}\\nPlease evaluate the new expert based on the following criteria to decide\\nwhether they should be retained or not:\\n1. The new expert is distinct and does not duplicate any previously hired\\nexperts.\\n2. Based on the new expert’s description, determine if they can\\neffectively assist in answering users’ questions.\\nGive the reason first and then give the choice. If retaining, please\\nreply with: Retain. If discarding, please reply with: Discard.\\nResult Update:\\n{question}\\nThis is your result:\\n{old_answer}\\nYou invite an expert whose description is: {description}\\nThis expert also give his answer based on his own professional knowledge:\\n{new_answer}.\\nNow you can refine your result with his answer to better answer the\\nquestion.\\nKeep in mind that his answer may not be correct, so critically decide\\nwhether to accept his response or stick with your original one.\\nRevised Answer:\\n19\\nListing 4: Instruction templates of different ablation settings in EVOAGENT.\\nPK:\\n{question}\\nWe invite {n} experts. They give the results based on their own\\nprofessional knowledge:\\nHere are second-person descriptions of these experts with their answers:\\n{select}\\nNow you can should help us select the best result which can meet the\\nquery.\\nYou need to give reasons first and then give the answer with the format:\\n\"Final Answer: Expert #XX\"\\nAll-in:\\n{question}\\nThis is your answer: {old_answer}.\\nFurthermore, you also invite {n} experts. They also give answers based on\\ntheir own professional knowledge:\\nHere are second person descriptions of these experts with their answers:\\n{description_ls}\\nNow you can refine your answer with these answers to better meet the\\nquery.\\nSuggest:\\n{Expert_Agent_description}\\n{question}\\nThis is the result from an AI assistant: {answer}.\\nPlease do not refine the plan but give some insightful suggestions for\\nthe AI assistant to help it better meet the user’s query.\\nSuggestion:\\nOverGen:\\n{question}\\nPlease generate three different results at one time for user to choose\\nfrom. The format can be:\\nResult #1:\\nResult #2:\\nResult #3:\\nThree Different Candidate Results:\\nPromptRefine:\\n{question}\\nThis is the result from an AI assistant, which description is\\n\"{original_description}\":\\n{answer}.\\nPlease do not refine the result but refine the description of the AI\\nassistant to help it better answer the user’s query.\\nPlease note that the description should be narrated in the second person,\\nfor example: You are a XXX.\\nDescription:\\n20\\nListing 5: An example of multi-agent generation from EVOAGENT for Logic Grid Puzzle\\nQuestion: There are 5 houses in a row, numbered 1 on the left to 5 on the\\nright. There is one person living in each house. The people in these\\nhouses have different characteristics:\\n- Each person has a different kind of car: one owns an SUV, one owns a\\nsedan, one drives a convertible, one drives a minivan, and one drives a\\ntruck\\n- Each person has a favorite color: one likes red, one likes yellow, one\\nlikes green, one likes white, and one likes blue\\n- Each person is wearing different shoes: one is wearing running shoes,\\none is wearing boots, one is wearing high heels, one is wearing sandals,\\nand one is wearing loafers\\nClue(s):\\n1. There is one house between where the person who is wearing loafers\\nlives and where the person who drives a minivan lives. 2. The person who\\ndrives a truck does not live in the fourth house. 3. The person who\\ndrives a truck lives directly left of the person who is wearing sandals.\\n4. The person who likes red lives somewhere to the right of the person\\nwho likes yellow. 5. The person who is wearing loafers lives somewhere to\\nthe left of the person who drives a truck. 6. The person who likes blue\\nlives in the fifth house. 7. There are two houses between where the\\nperson who owns a sedan lives and where the person who is wearing running\\nshoes lives. 8. The person who is wearing boots lives somewhere to the\\nleft of the person who drives a truck. 9. The person who likes white\\nlives in the fourth house. 10. There is one house between where the\\nperson who likes green lives and where the person who drives a truck\\nlives. 11. The person who owns an SUV does not live in the first house.\\nWhat is the number of the house where the person who is wearing boots\\nlives? choice: 2, choice: 5, choice: 3, choice: 1, choice: 4.\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a logician, specializing in logical reasoning and problem-solving\\n. Your task is to analyze the given clues and use deductive reasoning to\\ndetermine the correct sequence of houses, their occupants, and their\\ncharacteristics. You will need to consider each clue carefully,\\nunderstanding how it relates to the others and how it narrows down the\\npossibilities. Your focus should be on ensuring the logical consistency\\nof the solution, making sure that all clues are satisfied without\\ncontradiction.\\nReason: From clue 6 and 9, we know that the person who likes blue lives\\nin the fifth house and the person who likes white lives in the fourth\\nhouse. From clue 4, the person who likes red lives somewhere to the right\\nof the person who likes yellow. Since the fourth and fifth houses are\\noccupied by the people who like white and blue, the person who likes red\\nmust live in the third house and the person who likes yellow must live in\\nthe first or second house.From clue 10, there is one house between where\\nthe person who likes green lives and where the person who drives a truck\\nlives. Since the third, fourth, and fifth houses are occupied by the\\npeople who like red, white, and blue, the person who likes green must\\nlive in the first or second house. Therefore, the person who likes yellow\\nmust live in the first house and the person who likes green must live in\\nthe second house.From clue 11, the person who owns an SUV does not live\\nin the first house. So, the person who owns an SUV must live in the\\nsecond, third, or fourth house.From clue 2, the person who drives a truck\\ndoes not live in the fourth house. So, the person who drives a truck\\nmust live in the second or third house.From clue 3, the person who drives\\na truck lives directly left of the person who is wearing sandals. So,\\nthe person who drives a truck must live in the second house and the\\nperson who is wearing sandals must live in the third house.From clue 8,\\nthe person who is wearing boots lives somewhere to the left of the person\\nwho drives a truck. So, the person who is wearing boots must live in the\\nfirst house.\\nAnswer: choice: 1\\n21\\nListing 6: An example of multi-agent generation from EVOAGENT for Trivia Creative Writing\\nWrite a short and coherent story about Zootopia that incorporates the\\nanswers to the following 5 questions:\\n1. Following success at the 1979 election whose party conference speeech\\nincluded the lines ’you turn if you want to, the lady’s not for turning’?\\n2. Which snooker player was simply known as ’The Grinder’?\\n3. Which former West Indian fast bowler was known as ’Whispering Death’?\\n4. Which Eastenders character had a dog called Willy?\\n5. In which year did the first episode of Minder take place?\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a sports historian with a deep understanding of snooker and\\ncricket from the 1970s and 1980s. You are familiar with the careers and\\nplaying styles of Cliff Thorburn, known as ’The Grinder’, and Michael\\nHolding, known as ’Whispering Death’. Your role in this task will be to\\nprovide context and insight into these sports figures and their\\nsignificance in the story about Zootopia.\\nExpert Agent #2 Description:\\nYou are a political historian with a deep understanding of the political\\nclimate and key figures of the 1970s and 1980s, particularly in the\\nUnited Kingdom. You have extensive knowledge about Margaret Thatcher, her\\npolicies, and her impact on society. Your role in this task will be to\\nprovide context and insight into the significance of the reference to\\nThatcher’s famous speech in the story about Zootopia.\\nStory: In the vibrant city of Zootopia, the year was 1979, a time of\\nsignificant change and excitement. The city had just elected its first\\nfemale mayor, a lioness named Margaret Thatcher, known for her unwavering\\ndetermination and strong leadership. Her victory speech at the party\\nconference echoed the famous line from the human world’s Margaret\\nThatcher, \"You turn if you want to, the lady’s not for turning.\" This\\nphrase resonated with the citizens of Zootopia, who admired her\\nsteadfastness and determination. In the heart of the city, a meticulous\\nbadger known as ’The Grinder’ ran a popular snooker club. His nickname\\nwas a tribute to the legendary human snooker player Cliff Thorburn, known\\nfor his relentless and meticulous style of play. The Grinder’s snooker\\nclub was a hub of social activity, where animals from all walks of life\\ncame to unwind and engage in friendly competition. Meanwhile, a new\\nrecruit had joined the Zootopia Police Department, a cheetah named\\nMichael Holding. Known as ’Whispering Death’ during his cricketing days,\\na moniker borrowed from the former West Indian fast bowler, he was now a\\nrespected figure in the community, using his speed and stealth to uphold\\nthe law. In another part of the city, a popular soap opera was being\\nfilmed. The show was called \"Eastenders of Zootopia,\" and one of the\\nbeloved characters was a rabbit named Ethel Skinner, who had a dog named\\nWilly. This was a charming reference to the human Eastenders character of\\nthe same name, symbolizing the enduring friendships that were a\\ncornerstone of Zootopian society. The year 1979 was also significant as\\nit marked the airing of the first episode of the Zootopian version of the\\nhuman show \"Minder\". The show was a reflection of the city’s vibrant and\\ndiverse culture, much like the original British series. It was a\\ntestament to the city’s ability to adapt and evolve, while still holding\\nonto its rich history and traditions. In conclusion, Zootopia was a\\nmelting pot of cultures and histories, much like the television shows and\\ncharacters it referenced. These references served as a bridge between\\nthe past and the present, reminding the citizens of their roots while\\nencouraging them to embrace the future.\\nEvaluation: correct_count=5/5\\n22\\nListing 7: An example of multi-agent generation from EVOAGENT for Codenames Collaborative\\nTry to identify the 3 words best associated with the word \"fruit\" from\\nthe following word list:\\n[\"bowl\", \"einstein\", \"banana\", \"brain\", \"judge\", \"groom\", \"crusader\", \"\\nstamp\", \"peach\", \"halloween\", \"troll\", \"slipper\", \"earthquake\", \"tip\", \"\\nsun\", \"quarter\", \"pad\", \"nude\", \"helmet\", \"laundry\", \"wedding\", \"mirror\",\\n\"sleep\", \"minotaur\", \"wish\"].\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a cultural anthropologist with a focus on symbolism and rituals.\\nYour expertise in understanding how different objects, including fruits,\\nare used and symbolized in various cultures will be beneficial in this\\ntask. You can provide insights into how certain words might be associated\\nwith \"fruit\" from a cultural or symbolic perspective. Your focus will be\\non the cultural and symbolic aspects of the task.\\nReason:\\n1. ’Banana’: This is a type of fruit, rich in potassium and dietary fiber\\n. It is commonly consumed raw and is used in a variety of dishes like\\ndesserts, smoothies, and baked goods.\\n2. ’Peach’: This is also a type of fruit, known for its sweet and juicy\\nnature. Peaches are rich in vitamins A and C and are often eaten raw or\\nused in dishes like pies, jams, and salads.\\n3. ’Bowl’: This word is associated with fruit in the context of a fruit\\nbowl, which is a common way to store and display various types of fruit.\\nA bowl of fruit can also be a healthy snack or dessert option.\\nAnswer: Banana, Peach, Bowl\\nExpert Agent #2 Description:\\nYou are a nutritionist with a deep understanding of different types of\\nfood, including fruits. Your knowledge about the nutritional value,\\ncommon uses, and cultural significance of various fruits will be helpful\\nin this task. You can provide insights into how certain words might be\\nassociated with \"fruit\" from a dietary or nutritional perspective. Your\\nfocus will be on the nutritional and dietary aspects of the task.\\nReason:\\n1. ’Banana’: This is a direct association as a banana is a type of fruit.\\nIn many cultures, bananas symbolize fertility and prosperity. In\\nHinduism, for example, bananas are considered a sacred fruit and are used\\nin religious rituals.\\n2. ’Peach’: Similar to banana, a peach is also a type of fruit. In\\nChinese culture, peaches are symbols of immortality and unity. They are\\noften used in traditional rituals and ceremonies.\\n3. ’Bowl’: A bowl is often used to hold fruits. In many cultures, a bowl\\nof fruit is a symbol of abundance, fertility, and prosperity. It is often\\nused in rituals and ceremonies as an offering or a centerpiece.\\nAnswer: banana, peach, bowl\\nFinal Answer:\\nAfter considering the expert’s input, I find that their reasoning aligns\\nwith my initial thoughts. The words ’banana’ and ’peach’ are indeed types\\nof fruit, and their nutritional value and common uses further strengthen\\ntheir association with the word ’fruit’. The word ’bowl’ is also\\nassociated with fruit in the context of a fruit bowl, which is a common\\nway to store and display various types of fruit. Therefore, I agree with\\nthe expert’s choices.\\nFinal Answer: banana, peach, bowl.\\n23\\nListing 8: An example of multi-agent generation from EVOAGENT for MMMU dataset.\\nQuestion: <image 1> The elliptical orbit of a planet around the Sun is\\nshown on the diagram. Which of the following statements is true?\\nA. the eccentricity of the orbit is less than zero\\nB. the eccentricity of the orbit is greater than 1\\nC. the sun might be at point C\\nD. the sun might be at point D\\nE. the sun might be at point B (Correct)\\nCoT:\\nReason: In an elliptical orbit, the Sun is located at one of the two foci\\nof the ellipse. The eccentricity of an orbit is a measure of how much\\nthe orbit deviates from a perfect circle, with 0 being a perfect circle\\nand 1 being a parabola. Since the orbit shown is an ellipse, the\\neccentricity must be between 0 and 1. Points A and E are the farthest\\npoints from the center of the ellipse, so the Sun cannot be located at\\neither of those points. Point B is at the center of the ellipse, so the\\nSun cannot be located there either. This leaves points C and D as\\npossible locations for the Sun. However, since the Sun must be at one of\\nthe foci and point D is closer to the center of the ellipse than point C,\\nit is more likely that the Sun is located at point C.\\nAnswer: C. the sun might be at point C\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a mathematician with a specialization in geometry, particularly\\nin the study of conic sections which include ellipses. Your expertise\\nallows you to understand the properties and definitions of ellipses,\\nincluding the concept of foci. You can provide a mathematical perspective\\non why the Sun must be located at one of the foci of the elliptical\\norbit and not at any other point. Your knowledge will help explain the\\ngeometric reasons behind the positioning of the Sun in relation to the\\nplanet’s orbit, and why the given options in the statements do not align\\nwith the mathematical definition of an ellipse.\\nReason: An ellipse is defined as the set of all points for which the sum\\nof the distances to two fixed points, called foci, is constant. The Sun,\\nbeing one of the massive bodies in the solar system, must be located at\\none of the foci of the elliptical orbit of a planet due to the\\ngravitational forces that govern the motion of the planets. Option E is\\ncorrect because the Sun might be at point B, which is one of the foci of\\nthe ellipse. The other focus would be at a point symmetrically opposite\\nto B along the major axis.\\nAnswer: E. the sun might be at point B.\\nExpert Agent #2 Description:\\nYou are an astronomer with a specialization in observational astronomy.\\nYour expertise lies in the practical observation and data collection of\\ncelestial bodies, including the use of telescopes and other instruments\\nto study the positions and movements of planets and stars. You have a\\ndeep understanding of how the apparent motion of celestial bodies can be\\nused to infer their actual motion in space. You can provide insight into\\nhow observations of planetary orbits, such as the one depicted in the\\ndiagram, have historically been used to deduce the laws of planetary\\nmotion and the positions of stars like the Sun. Your knowledge will help\\nexplain how practical observations support the theoretical understanding\\nthat the Sun must be located at one of the foci of the elliptical orbit.\\nReason: The eccentricity of an orbit is a measure of how much it deviates\\nfrom being a perfect circle. It ranges from 0 (a perfect circle) to 1 (a\\nparabola). An eccentricity less than zero or greater than 1 is not\\npossible for an elliptical orbit. Therefore, options A and B are\\nincorrect. According to Kepler’s laws of planetary motion, the Sun must\\nbe located at one of the foci of the elliptical orbit. Point B is located\\nat a position that could be one of the foci of the ellipse.\\nAnswer: E. the sun might be at point B.\\n24\\nListing 9: An example of multi-agent generation from EVOAGENT for ScienceWorld.\\nTask 1: Your task is to determine if a metal fork is electrically\\nconductive. The metal fork is located around the kitchen. First, focus on\\nthe metal fork. If conductive, place it in the blue box. If\\nnonconductive, place it in the green box.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a thermodynamics expert. Your expertise in heat transfer and\\ntemperature measurement is crucial for this task. You understand how to\\nproperly use a thermometer and interpret its readings. You also have\\nknowledge about the properties of various substances, including their\\ntypical temperature ranges. Your task is to guide the assistant in using\\nthe thermometer to accurately measure the temperature of the unknown\\nsubstance B.\\nTask 2: Your task is to boil gallium. For compounds without a boiling\\npoint, combusting the substance is also acceptable. First, focus on the\\nsubstance. Then, take actions that will cause it to change its state of\\nmatter.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a metallurgist with a deep understanding of the properties and\\nbehaviors of metals, including gallium. Your expertise includes knowledge\\nof the melting and boiling points of various metals, as well as the\\nprocesses and equipment needed to heat them to these temperatures. You\\nare also familiar with safety procedures and precautions when handling\\nand heating metals. Your task is to provide detailed instructions on how\\nto safely and effectively boil gallium in the foundry.\\nTask 3: Your task is to change the state of matter of gallium. First,\\nfocus on the substance. Then, take actions that will cause it to change\\nits state of matter.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a chemist with a specialization in the properties and behaviors\\nof substances. You have a deep understanding of the state changes of\\nmatter, specifically how substances like gallium can transition from\\nsolid to liquid or gas. You are familiar with the conditions required for\\nthese changes to occur, such as temperature and pressure. You also have\\nknowledge of the equipment and procedures used in a foundry, which will\\nbe useful in this task. Your role is to provide guidance on how to\\nmanipulate the conditions in the foundry to achieve the desired state\\nchange in gallium.\\n25\\nListing 10: An example of multi-agent generation from EVOAGENT for TravelPlanner dataset.\\nQuestion: Can you help me with a travel plan departing from Buffalo to\\nAtlanta for a duration of 3 days, specifically from March 2nd to March 4\\nth, 2022? I plan to travel alone and my planned budget for the trip is\\naround $1,100.\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a culinary expert with a deep understanding of food culture in\\nAtlanta. Your expertise includes knowledge of the best local restaurants,\\npopular dishes, and dietary restrictions. You can provide\\nrecommendations for breakfast, lunch, and dinner spots that offer a\\nunique taste of Atlanta’s food scene. Your suggestions should consider\\nthe traveler’s budget and the proximity of the restaurants to the\\nattractions and accommodation listed in the plan.\\nExpert Agent #2 Description:\\nYou are a travel logistics expert with extensive knowledge of\\ntransportation options and costs between Buffalo and Atlanta. Your\\nexpertise includes understanding flight schedules, costs, and durations,\\nas well as alternative transportation methods such as self-driving and\\ntaxi services. You can provide the most cost-effective and time-efficient\\ntransportation options for the traveler, considering their budget and\\nthe travel dates provided in the plan.\\nExpert Agent #3 Description:\\nYou are a local tourism expert with comprehensive knowledge of Atlanta’s\\nattractions. Your expertise includes understanding the city’s popular\\ntourist spots, historical sites, museums, parks, and unique local\\nexperiences. You can provide recommendations for attractions that the\\ntraveler can visit during their stay, considering their budget and the\\nproximity of these attractions to the traveler’s accommodation and meal\\nspots. Your suggestions should also take into account the operating hours\\nof these attractions to ensure they align with the traveler’s schedule.\\nAnswer:\\nDay 1:\\nCurrent City: from Buffalo to Atlanta\\nTransportation: Flight Number: F3502691, from Buffalo to Atlanta,\\nDeparture Time: 18:48, Arrival Time: 20:51\\nBreakfast: Not applicable as the flight is in the evening.\\nAttraction: Not applicable as the flight is in the evening.\\nLunch: Not applicable as the flight is in the evening.\\nDinner: Saffron, Atlanta\\nccommodation: Spacious private room close St. Barnabas Hospital, Atlanta\\nDay 2:\\nCurrent City: Atlanta\\nTransportation: -\\nBreakfast: Adda, Atlanta\\nAttraction: Atlanta Botanical Garden, Atlanta; World of Coca-Cola,\\nAtlanta.\\nLunch: Baba Au Rhum, Atlanta\\nDinner: Asian Bistro, Atlanta\\nAccommodation: Spacious private room close St. Barnabas Hospital, Atlanta\\nDay 3:\\nCurrent City: from Atlanta to Buffalo\\nTransportation: Flight Number: F3500648, from Atlanta to Buffalo,\\nDeparture Time: 21:24, Arrival Time: 23:26\\nBreakfast: Chef Style, Atlanta\\nAttraction: Georgia Aquarium, Atlanta; Martin Luther King, Jr. National\\nHistorical Park, Atlanta.\\nLunch: Pizza Central, Atlanta\\nDinner: Daawat-e-Kashmir, Atlanta\\nAccommodation: -.\\n26']"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(t_splitter.split_text(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7Ob8AYsdL_4",
        "outputId": "869a0806-2f99-4060-ddac-b9a9616005dd"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_splitter.split_text(text)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "QQdcMNP2dRai",
        "outputId": "145501e1-7492-40e8-e395-eadecc4d43d1"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EVOAGENT: Towards Automatic Multi-Agent\\nGeneration via Evolutionary Algorithms\\nSiyu Yuan1∗, Kaitao Song2∗†,\\nJiangjie Chen1, Xu Tan2, Dongsheng Li2, Deqing Yang1†\\nFudan University1, Microsoft Research Asia2\\nsyyuan21@m.fudan.edu.cn, {kaitaosong, xuta, dongsli}@microsoft.com\\n{jjchen19,yangdeqing}@fudan.edu.cn\\nhttps://evo-agent.github.io\\nAbstract\\nThe rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically extend\\nthe specialized agent to multi-agent systems to improve task-solving capability\\nstill remains a significant challenge. In this paper, we introduce EVOAGENT, a\\ngeneric method to automatically extend expert agents to multi-agent systems via\\nthe evolutionary algorithm, thereby improving the effectiveness of LLM-based\\nagents in solving tasks. Specifically, we consider the existing agent frameworks\\nas the initial individual and then apply a series of evolutionary operators (e.g.,\\nmutation, crossover, selection, etc.) to generate multiple agents with diverse agent\\nsettings. EVOAGENT can be generalized to any LLM-based agent framework,\\nand can automatically extend the existing agent framework to multi-agent systems\\nwithout any extra human designs. Experimental results across various tasks have\\nshown that EVOAGENT can automatically generate multiple expert agents and\\nsignificantly enhance the task-solving capabilities of LLM-based agents.\\n1\\nIntroduction\\nRecently, the advent of large language models (LLMs) [1, 2, 3, 4] have shown remarkable capabilities\\nin solving language understanding, reasoning, and generation tasks. Based on the foundation of LLMs,\\nmany research works [5, 6, 7, 8, 9, 10, 11] have discovered that by empowering multiple advanced\\nskills (e.g., planning, tool, memory and so on), we can develop more powerful'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_splitter.split_text(text)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "Kfnnn_jNdULI",
        "outputId": "5902ac7b-40cb-4232-d19e-cee9b74b5ec8"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' memory and so on), we can develop more powerful autonomous agents to\\nsolve more challenging tasks. Therefore, how to design and leverage LLM-based autonomous agents\\nto tackle more diverse and complex real-world applications has attracted enormous interest.\\nGenerally, many real-world scenarios are usually complex, encompassing a variety of challenging\\ntasks that are beyond the capability of a single agent. To address this point, we notice that human\\nsociety is composed of vast individuals, each possessing their unique characteristics. By selecting,\\norchestrating, and cooperating with different individuals, humans can form an efficient team group\\nto handle complicated missions in the real world. Therefore, there has been an increasing trend\\nto develop multi-agent collaboration frameworks (e.g., MetaGPT [10], AutoGen [12], Camel [13],\\n∗The first two authors have equal contributions. This work was done when the first author was an intern at\\nMicrosoft Research Asia.\\n† Corresponding authors.\\nPreprint. Under review.\\narXiv:2406.14228v2  [cs.AI]  11 Jul 2024\\nGenerative Agents [11]) to simulate human behaviors for solving complex tasks. By developing a\\nseries of expert agents with diverse settings, multi-agent systems enable us to reveal emergent abilities\\namong multiple agents and synergize their specialized expertise to achieve superior performance,\\nakin to simulating human populations. Nevertheless, it is worthy noting that, in most of (multi)-agent\\nframeworks, their designs heavily depend on handcrafted settings, including character roles, task\\nscopes, skills, and prompt settings. Although we admit that meticulous human design is quite useful\\nfor instructing LLM-based agents to understand tasks, it also limits scaling up the number of agents to\\nfurther improve performance due to expensive human labor. Considering the increasing popularity of\\nLLM-based autonomous agents, how to create a generic agent generation paradigm to automatically\\nbuild multi-agent systems has emerged as a critical challenge.\\nIn this paper, we introduce a novel method, EVOAGENT, that formulates agent generation as the\\nevolutionary processing [14] in human society. Specifically, to align human society, each agent\\ncan be considered as individuals that can procreate its population across successive generations.\\nMotivated by this mechanism, we can simulate such a human behavior to automatically generate\\nmultiple agents'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -qU langchain_experimental langchain_openai"
      ],
      "metadata": {
        "id": "sCPWPF4JTzZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from config import OPEN_AI_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_API_KEY"
      ],
      "metadata": {
        "id": "jcht2OSYVxSy"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# OpenAI 임베딩을 사용하여 의미론적 청크 분할기를 초기화합니다.\n",
        "text_splitter = SemanticChunker(OpenAIEmbeddings(),\n",
        "                                # percentile 기준\n",
        "                                breakpoint_threshold_type=\"percentile\",\n",
        "                                breakpoint_threshold_amount=70,\n",
        "                                # 표준편차 기준\n",
        "                                #breakpoint_threshold_type=\"standard_deviation\",\n",
        "                                #breakpoint_threshold_amount=1.25,\n",
        "                                )"
      ],
      "metadata": {
        "id": "VzrqouHwRgzo"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#문서 단위로 분할\n",
        "chunks = text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "bmy5mFOjTtn5"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5Kt25jTWu2V",
        "outputId": "8e6d581f-a309-40f7-f96f-8075c04a9d08"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVOAGENT: Towards Automatic Multi-Agent\n",
            "Generation via Evolutionary Algorithms\n",
            "Siyu Yuan1∗, Kaitao Song2∗†,\n",
            "Jiangjie Chen1, Xu Tan2, Dongsheng Li2, Deqing Yang1†\n",
            "Fudan University1, Microsoft Research Asia2\n",
            "syyuan21@m.fudan.edu.cn, {kaitaosong, xuta, dongsli}@microsoft.com\n",
            "{jjchen19,yangdeqing}@fudan.edu.cn\n",
            "https://evo-agent.github.io\n",
            "Abstract\n",
            "The rise of powerful large language models (LLMs) has spurred a new trend in\n",
            "building LLM-based autonomous agents for solving complex tasks, especially\n",
            "multi-agent systems. Despite the remarkable progress, we notice that existing\n",
            "works are heavily dependent on human-designed frameworks, which greatly limits\n",
            "the functional scope and scalability of agent systems. How to automatically extend\n",
            "the specialized agent to multi-agent systems to improve task-solving capability\n",
            "still remains a significant challenge. In this paper, we introduce EVOAGENT, a\n",
            "generic method to automatically extend expert agents to multi-agent systems via\n",
            "the evolutionary algorithm, thereby improving the effectiveness of LLM-based\n",
            "agents in solving tasks. Specifically, we consider the existing agent frameworks\n",
            "as the initial individual and then apply a series of evolutionary operators (e.g.,\n",
            "mutation, crossover, selection, etc.) to generate multiple agents with diverse agent\n",
            "settings. EVOAGENT can be generalized to any LLM-based agent framework,\n",
            "and can automatically extend the existing agent framework to multi-agent systems\n",
            "without any extra human designs. Experimental results across various tasks have\n",
            "shown that EVOAGENT can automatically generate multiple expert agents and\n",
            "significantly enhance the task-solving capabilities of LLM-based agents. 1\n",
            "Introduction\n",
            "Recently, the advent of large language models (LLMs) [1, 2, 3, 4] have shown remarkable capabilities\n",
            "in solving language understanding, reasoning, and generation tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpIECLtwWw4T",
        "outputId": "f116d167-55de-40eb-efac-4381eac11a2a"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the foundation of LLMs,\n",
            "many research works [5, 6, 7, 8, 9, 10, 11] have discovered that by empowering multiple advanced\n",
            "skills (e.g., planning, tool, memory and so on), we can develop more powerful autonomous agents to\n",
            "solve more challenging tasks. Therefore, how to design and leverage LLM-based autonomous agents\n",
            "to tackle more diverse and complex real-world applications has attracted enormous interest. Generally, many real-world scenarios are usually complex, encompassing a variety of challenging\n",
            "tasks that are beyond the capability of a single agent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3FnyAokXszs",
        "outputId": "553c346c-8611-4a32-ebf8-96beb274fb53"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To address this point, we notice that human\n",
            "society is composed of vast individuals, each possessing their unique characteristics. By selecting,\n",
            "orchestrating, and cooperating with different individuals, humans can form an efficient team group\n",
            "to handle complicated missions in the real world. Therefore, there has been an increasing trend\n",
            "to develop multi-agent collaboration frameworks (e.g., MetaGPT [10], AutoGen [12], Camel [13],\n",
            "∗The first two authors have equal contributions. This work was done when the first author was an intern at\n",
            "Microsoft Research Asia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text_splitter를 사용하여 분할\n",
        "docs = text_splitter.create_documents([text])\n",
        "print(docs[0].page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBClaPr8W1Er",
        "outputId": "56f2527e-7b86-4e3d-c808-a359ca755467"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVOAGENT: Towards Automatic Multi-Agent\n",
            "Generation via Evolutionary Algorithms\n",
            "Siyu Yuan1∗, Kaitao Song2∗†,\n",
            "Jiangjie Chen1, Xu Tan2, Dongsheng Li2, Deqing Yang1†\n",
            "Fudan University1, Microsoft Research Asia2\n",
            "syyuan21@m.fudan.edu.cn, {kaitaosong, xuta, dongsli}@microsoft.com\n",
            "{jjchen19,yangdeqing}@fudan.edu.cn\n",
            "https://evo-agent.github.io\n",
            "Abstract\n",
            "The rise of powerful large language models (LLMs) has spurred a new trend in\n",
            "building LLM-based autonomous agents for solving complex tasks, especially\n",
            "multi-agent systems. Despite the remarkable progress, we notice that existing\n",
            "works are heavily dependent on human-designed frameworks, which greatly limits\n",
            "the functional scope and scalability of agent systems. How to automatically extend\n",
            "the specialized agent to multi-agent systems to improve task-solving capability\n",
            "still remains a significant challenge. In this paper, we introduce EVOAGENT, a\n",
            "generic method to automatically extend expert agents to multi-agent systems via\n",
            "the evolutionary algorithm, thereby improving the effectiveness of LLM-based\n",
            "agents in solving tasks. Specifically, we consider the existing agent frameworks\n",
            "as the initial individual and then apply a series of evolutionary operators (e.g.,\n",
            "mutation, crossover, selection, etc.) to generate multiple agents with diverse agent\n",
            "settings. EVOAGENT can be generalized to any LLM-based agent framework,\n",
            "and can automatically extend the existing agent framework to multi-agent systems\n",
            "without any extra human designs. Experimental results across various tasks have\n",
            "shown that EVOAGENT can automatically generate multiple expert agents and\n",
            "significantly enhance the task-solving capabilities of LLM-based agents. 1\n",
            "Introduction\n",
            "Recently, the advent of large language models (LLMs) [1, 2, 3, 4] have shown remarkable capabilities\n",
            "in solving language understanding, reasoning, and generation tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YKUoLtb0Wz9K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}