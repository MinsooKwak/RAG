{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ36jyxbVbC+RYEHfuNlXn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinsooKwak/RAG/blob/main/test/deploy/RAG_paper_assistant_chatbot_for_deploy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install cohere\n",
        "#!pip install langchain\n",
        "#!pip install openai\n",
        "#!pip install -U langchain-community\n",
        "#!pip install pypdf\n",
        "#!pip install tiktoken\n",
        "#!pip install faiss-cpu\n",
        "#!pip install -U langchain-openai\n",
        "#!pip install streamlit"
      ],
      "metadata": {
        "id": "YkV-qIKWGUsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thao6qkuF6Te"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# PDF 가져오기\n",
        "\n",
        "loader = PyPDFLoader(\"2306.05685v4.pdf\")\n",
        "\n",
        "\"\"\"\n",
        "loaders = [\n",
        "    PyPDFLoader(\"2306.05685v4.pdf\")\n",
        "]\n",
        "\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "  docs.extend(loader.load())\n",
        "\"\"\"\n",
        "\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from config import OPEN_AI_API_KEY, COHERE_API_KEY, NGROK_TOKEN_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_API_KEY\n",
        "os.environ[\"NGROK_TOKEN\"] = NGROK_TOKEN_KEY"
      ],
      "metadata": {
        "id": "jCM14xPFgnEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking"
      ],
      "metadata": {
        "id": "lZ7UeqwHQkPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap  = 50,\n",
        ")\n",
        "\n",
        "data = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxWqEtS8RZXT",
        "outputId": "b1e26ea8-3a23-4252-d988-7122769f453d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2602, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1127, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 622, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1204, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1188, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1398, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1097, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3863, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3203, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1284, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1721, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 757, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1522, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1946, which is longer than the specified 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding"
      ],
      "metadata": {
        "id": "rIB994xfRmsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=OPEN_AI_API_KEY)"
      ],
      "metadata": {
        "id": "nsWK7-V0Rab1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FAISS database"
      ],
      "metadata": {
        "id": "YFwiRROzSJuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "vectorstore = FAISS.from_documents(data, embeddings)"
      ],
      "metadata": {
        "id": "j0orBkzPRxBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM"
      ],
      "metadata": {
        "id": "R-OrwJYZSN3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- memory 부분에 대화 내용도 함께 넘어가므로 token size가 중요하다"
      ],
      "metadata": {
        "id": "DP2qspIgUJ4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", api_key=OPEN_AI_API_KEY)"
      ],
      "metadata": {
        "id": "BBsvn0GKSCpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 대화 내용 기억하도록\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
      ],
      "metadata": {
        "id": "vm1o4pU7SEes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(),     # 검색 결과 넘겨줌\n",
        "    memory=memory                             # 대화 내용도 같이 전달 (token따라 구현x)\n",
        ")"
      ],
      "metadata": {
        "id": "XZ7Xr_lmS-i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"기존에 활용된 평가 방식과는 어떤 차이점이 있어?\"\n",
        "result = conversational_chain({\"question\": query})  # 질문 넘겨줌\n",
        "answer = result[\"answer\"]\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "akqu8Wo2Tu65",
        "outputId": "8613f327-875c-4598-d0c2-69bc7edf2d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-579eb2af6986>:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "  result = conversational_chain({\"question\": query})  # 질문 넘겨줌\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'이 논문에서는 기존 평가 방식과 비교하여 LLM-as-a-judge 방식의 잠재적 한계를 연구하고 있습니다. 이에 대한 몇 가지 예상한 한계는 위치 편향, 장황 편향, 자아 강화 편향 및 제한된 추론 능력입니다. 이러한 한계들은 인간 평가의 골드 표준과 비교하여 분석되고 있습니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"LLM-as-a-judge 방법은 어떻게 작동이 돼?\"\n",
        "result = conversational_chain({\"question\": query})  # 질문 넘겨줌\n",
        "answer = result[\"answer\"]\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "yj2GE2raUhaP",
        "outputId": "9032490d-bd7b-4cac-8740-942d40b4616e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LLM-as-a-judge 방법은 Large Language Models (LLMs)를 판단 기준으로 사용하여 텍스트나 대화 모델의 성능을 평가하는 방법입니다. 이 방법은 LLM이 사람과의 대조를 통해 응답을 판단하고 사람의 선호도와 일치하는지 확인하는 것을 목표로 합니다. LLM-as-a-judge의 세 가지 변형이 제안되었으며, 이는 독립적으로 또는 결합하여 구현될 수 있습니다. 이러한 방법은 여러 가지 편향과 한계를 가지고 있지만, 해결책을 제안하고 이러한 한계를 극복하는 방법을 연구하고 있습니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"변형에 대해 자세히 알려줘.\"\n",
        "result = conversational_chain({\"question\": query})  # 질문 넘겨줌\n",
        "answer = result[\"answer\"]\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ZYQ2HLc3VMQh",
        "outputId": "4b992139-8325-4f61-d9f0-a89916787e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LLM-as-a-judge 방법의 세 가지 변형은 다음과 같습니다:\\n\\n1. Position bias: LLM이 특정 위치를 다른 위치보다 선호하는 경향을 나타내는 바이어스입니다. 이러한 편향은 인간의 의사결정에서도 볼 수 있으며, 다른 기계 학습 분야에서도 관측되었습니다.\\n\\n2. Verbosity bias: LLM이 말이 많은 응답을 선호하는 경향을 나타내는 바이어스입니다.\\n\\n3. Self-enhancement bias: LLM이 자아 강화를 위한 성향을 나타내는 바이어스입니다.\\n\\n이러한 세 가지 변형에 대한 제한 사항과 해결책에 대해 논의되고 있습니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"제시한 방법이 어떤 장점이 있고 어떤 한계가 있는지 알려줘.\"\n",
        "result = conversational_chain({\"question\": query})  # 질문 넘겨줌\n",
        "answer = result[\"answer\"]\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "srSVhdAnVW41",
        "outputId": "5f6598cb-f5ed-4dd1-c188-30eb4f13becc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LLM-as-a-judge의 장점은 확장성과 설명가능성입니다. 이는 인간의 개입을 줄여 확장 가능한 벤치마크와 빠른 반복을 가능케 합니다. 게다가 LLM 판정자는 점수 뿐만 아니라 설명도 제공하여 그 결과를 해석하기 쉽게 만들어줍니다. \\n\\n한편 LLM-as-a-judge의 한계는 위치 편향(position bias)과 같은 편향이 있습니다. 이러한 편향은 우리의 맥락에서만 발생하는 것이 아니며, 인간의 의사결정에서도 보여지고 다른 기계학습 분야에서도 관찰되었습니다. 이러한 한계들에도 불구하고, 나중에 해결책을 제시하고 LLM 판정자와 인간 사이의 일치가 높음을 보여줄 것입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "XYU0EKTecLeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e37516-d957-466c-a31a-57d68a5d069d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ngrok 사이트 : https://dashboard.ngrok.com/signup"
      ],
      "metadata": {
        "id": "Kho7WU9Sfgtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ~/.streamlit/cache"
      ],
      "metadata": {
        "id": "24mRIE1t2bdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 캐시 삭제\n",
        "import streamlit as st\n",
        "st.cache_data.clear()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_1TyvaLxVxD",
        "outputId": "3ef740c8-9d89-4832-8f54-486ca9a55539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-09-07 14:51:39.881 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- rag 미활용 app"
      ],
      "metadata": {
        "id": "vSgiIUE8GfVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "import tempfile  # 임시 파일 생성에 사용\n",
        "\n",
        "# 캐시 삭제\n",
        "st.cache_data.clear()\n",
        "\n",
        "# LLM Prompt 정의\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"combined_input\"],\n",
        "    template=\"\"\"\n",
        "    You are an AI assistant based on research papers.\n",
        "    Answer the following question based on the content of the provided papers.\n",
        "\n",
        "    {combined_input}\n",
        "\n",
        "    AI:\"\"\"\n",
        ")\n",
        "\n",
        "# OpenAI 모델과 메모리 설정\n",
        "llm = ChatOpenAI(model_name='gpt-4')  # 또는 'gpt-3.5-turbo' 사용 가능\n",
        "memory = ConversationBufferWindowMemory(memory_key='chat_history', k=4)  # 최근 4개의 대화만 기억\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    prompt=prompt\n",
        ")\n",
        "\n",
        "# Create embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "st.title(\"ChatGPT AI Assistant\")\n",
        "st.write(\"논문 파일들을 입력해주세요.\")\n",
        "\n",
        "# 여러 PDF 파일 업로드 기능 추가\n",
        "uploaded_files = st.file_uploader(\"논문 PDF 파일들을 업로드하세요\", type=[\"pdf\"], accept_multiple_files=True)\n",
        "\n",
        "if uploaded_files:\n",
        "    all_text = \"\"\n",
        "\n",
        "    for uploaded_file in uploaded_files:\n",
        "        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "            temp_file.write(uploaded_file.read())\n",
        "            temp_file_path = temp_file.name\n",
        "\n",
        "        # PyPDFLoader로 임시 파일을 로드\n",
        "        loader = PyPDFLoader(temp_file_path)\n",
        "        documents = loader.load()\n",
        "        pdf_text = \"\\n\".join([doc.page_content for doc in documents])\n",
        "        all_text += pdf_text + \"\\n\"\n",
        "\n",
        "    st.write(\"### 업로드한 논문들 내용 미리보기:\")\n",
        "    st.write(all_text[:1000])\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [\n",
        "            {\"role\": \"system\", \"content\": \"안녕하세요, 저는 AI Assistant입니다. 업로드한 논문들을 바탕으로 질문에 답변해 드립니다.\"}\n",
        "        ]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message['content'])\n",
        "\n",
        "    user_prompt = st.chat_input(\"논문에 대한 질문을 입력하세요...\")\n",
        "\n",
        "    if user_prompt and user_prompt.strip():\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(user_prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        if all_text.strip():  # 논문 내용이 비어 있지 않도록 확인\n",
        "                            # 질문과 논문 내용을 하나의 텍스트로 결합\n",
        "                            combined_input = f\"Question: {user_prompt}\\n\\nPapers content:\\n{all_text}\"\n",
        "\n",
        "                            # llm_chain.predict()로 단일 입력값 전달\n",
        "                            ai_response = llm_chain.predict(combined_input=combined_input)\n",
        "\n",
        "                            if ai_response and ai_response.strip():\n",
        "                                st.write(ai_response)\n",
        "                                new_ai_message = {\"role\": \"assistant\", \"content\": ai_response}\n",
        "                                st.session_state.messages.append(new_ai_message)\n",
        "                            else:\n",
        "                                st.write(\"AI가 응답하지 못했습니다. 다시 시도해주세요.\")\n",
        "                        else:\n",
        "                            st.write(\"업로드된 논문에서 내용을 찾을 수 없습니다.\")\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"오류 발생: {str(e)}\")\n",
        "    else:\n",
        "        st.write(\"질문을 입력해주세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2DOboNIb7Ns",
        "outputId": "77e35402-3124-4efc-9a40-2117b39a1917"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- rag 활용 app for deploy"
      ],
      "metadata": {
        "id": "ZOew5poqGi3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_paper_chat_assistant_app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "import tempfile  # 임시 파일 생성에 사용\n",
        "\n",
        "# 캐시 삭제\n",
        "st.cache_data.clear()\n",
        "\n",
        "# LLM Prompt 정의\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "    You are an AI assistant. Based on the research papers provided, answer the following question.\n",
        "\n",
        "    Context: {context}\n",
        "    Question: {question}\n",
        "\n",
        "    AI:\"\"\"\n",
        ")\n",
        "\n",
        "# 사용자로부터 API키 입력 받음\n",
        "api_key = st.text_input(\"OpenAI API 키를 입력하세요:\", type=\"password\")\n",
        "\n",
        "if api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # openai 모델 초기화\n",
        "    llm = ChatOpenAI(model_name=\"gpt-4\", api_key=api_key)\n",
        "    memory = ConversationBufferWindowMemory(memory_key='chat_history', k=4)\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "\n",
        "  st.title(\"ChatGPT AI Assistant with RAG\")\n",
        "\n",
        "  #### 여러 PDF 파일 업로드 기능 추가\n",
        "  uploaded_files = st.file_uploader(\"논문 PDF 파일들을 업로드하세요\", type=[\"pdf\"], accept_multiple_files=True)\n",
        "\n",
        "\n",
        "  if uploaded_files:\n",
        "      all_text = \"\"\n",
        "      documents = []\n",
        "\n",
        "      for uploaded_file in uploaded_files:\n",
        "          with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "              temp_file.write(uploaded_file.read())\n",
        "              temp_file_path = temp_file.name\n",
        "\n",
        "          # PyPDFLoader로 임시 파일을 로드\n",
        "          loader = PyPDFLoader(temp_file_path)\n",
        "          docs = loader.load()\n",
        "          documents.extend(docs)\n",
        "          # 업로드 문서 보기 위한 작업\n",
        "          pdf_text = \"\\n\".join([doc.page_content for doc in documents])\n",
        "          all_text += pdf_text + \"\\n\"\n",
        "\n",
        "      st.write(\"### 업로드한 논문들 내용 미리보기:\")\n",
        "      st.write(all_text[:1000])\n",
        "\n",
        "  #### Chunking\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "          chunk_size=750,\n",
        "          chunk_overlap=50,\n",
        "          separators=[\"\\n\\n\"]\n",
        "      )\n",
        "\n",
        "      chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "      # FAISS vectorDB 생성\n",
        "      vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "      #### Chains\n",
        "      # RetrievalQA chain을 사용할 때\n",
        "      qa_chain = RetrievalQA.from_chain_type(\n",
        "          llm=llm,\n",
        "          chain_type=\"stuff\",  # \"stuff\" simply concatenates the retrieved chunks\n",
        "          retriever=vectorstore.as_retriever(),\n",
        "          return_source_documents=True  # This returns the documents that were retrieved as well\n",
        "      )\n",
        "\n",
        "      # ConversationalRetrievalChain을 사용할 때\n",
        "      conversational_chain = ConversationalRetrievalChain.from_llm(\n",
        "          llm=llm,\n",
        "          chain_type=\"stuff\",\n",
        "          retriever=vectorstore.as_retriever(),     # 검색 결과 넘겨줌\n",
        "          #memory=memory                            # 대화 내용도 같이 전달 (token 소비에 따라 구현x)\n",
        "      )\n",
        "\n",
        "      if \"messages\" not in st.session_state.keys():\n",
        "          st.session_state.messages = [\n",
        "              {\"role\": \"system\", \"content\": \"안녕하세요, 저는 AI Assistant입니다. 업로드한 논문들을 바탕으로 질문에 답변해 드립니다.\"}\n",
        "          ]\n",
        "\n",
        "      for message in st.session_state.messages:\n",
        "          with st.chat_message(message[\"role\"]):\n",
        "              st.write(message['content'])\n",
        "\n",
        "      user_prompt = st.chat_input(\"논문에 대한 질문을 입력하세요...\")\n",
        "\n",
        "      if user_prompt and user_prompt.strip():\n",
        "          st.session_state.messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "          with st.chat_message(\"user\"):\n",
        "              st.write(user_prompt)\n",
        "\n",
        "          if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "              with st.chat_message(\"assistant\"):\n",
        "                  with st.spinner(\"Retrieving relevant sections...\"):\n",
        "                      try:\n",
        "                          if chunks:\n",
        "                              #response = qa_chain({\"query\": user_prompt})                  # qa_chain 사용할 때\n",
        "                              response = conversational_chain({\"question\": user_prompt})    # conversational_chain 사용할 때\n",
        "\n",
        "                              ai_response = response[\"result\"]\n",
        "                              source_docs = response[\"source_documents\"]\n",
        "\n",
        "                              if ai_response and ai_response.strip():\n",
        "                                  st.write(ai_response)\n",
        "                                  st.write(\"\\n참조된 문서들:\")\n",
        "                                  for doc in source_docs:\n",
        "                                      st.write(doc.page_content[:500])                      # 참조된 doc의 첫 500자까지 보여줌\n",
        "\n",
        "                                  new_ai_message = {\"role\": \"assistant\", \"content\": ai_response}\n",
        "                                  st.session_state.messages.append(new_ai_message)\n",
        "                              else:\n",
        "                                  st.write(\"AI가 응답하지 못했습니다. 다시 시도해주세요.\")\n",
        "                          else:\n",
        "                              st.write(\"업로드된 논문에서 내용을 찾을 수 없습니다.\")\n",
        "                      except Exception as e:\n",
        "                          st.error(f\"오류 발생: {str(e)}\")\n",
        "      else:\n",
        "          st.write(\"질문을 입력해주세요.\")\n",
        "else:\n",
        "    st.warning(\"API 키를 입력해야 논문 assistant 사용이 가능합니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSZ3DRKnvmGi",
        "outputId": "7fc8d104-5c1a-42bf-8294-82dc40d002f9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_paper_chat_assistant_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 현재 실행 중인 모든 터널 종료\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "GnQ5hgPojSpW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- rag 활용 앱 실행"
      ],
      "metadata": {
        "id": "Woy2-X38GZsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from config import OPEN_AI_API_KEY, COHERE_API_KEY, NGROK_TOKEN_KEY\n",
        "os.environ[\"NGROK_TOKEN\"] = NGROK_TOKEN_KEY"
      ],
      "metadata": {
        "id": "R-Cw1Jo0K8Zt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(os.environ[\"NGROK_TOKEN\"])\n",
        "\n",
        "!streamlit run rag_paper_chat_assistant_app.py &>/dev/null&\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDKxrbD4vtMw",
        "outputId": "207155b3-acd5-4dd6-a7e8-bc39de4ecc3f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://f66d-34-172-211-3.ngrok-free.app\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}