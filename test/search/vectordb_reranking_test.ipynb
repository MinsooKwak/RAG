{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM80wE3aWMvPW/aaoLBZF4q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinsooKwak/RAG/blob/main/test/search/vectordb_reranking_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 유사도 검색 통해 문서 반환시 유사도 높은 순으로 ranking 매겨짐\n",
        "\n",
        "- RAG는 검색 시간 단축을 위해 Approximate Nearnest Neighbor search 기술을 활용해 </br> 질문과 관계 없는 문서가 검색될 가능성이 높음\n",
        "- 해결 방안 :\n",
        "  - 검색으로 반환하는 문서 수 늘리기(토큰 제약으로 불가능)\n",
        "  - Reranking : 관련 정보가 컨텍스트 내 상위에 존재하도록 함\n",
        "\n",
        "- 토큰 제약때문에 다 넘길 수 없고, 정말 관련된 문서만 넘겨야"
      ],
      "metadata": {
        "id": "mkRdzBfdrKJe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NXDyfa8MrGZw"
      },
      "outputs": [],
      "source": [
        "# cohere key (무료이면서 유료(서비스 구현))\n",
        "# https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install cohere\n",
        "#!pip install langchain\n",
        "#!pip install openai\n",
        "#!pip install -U langchain-community\n",
        "#!pip install pypdf\n",
        "#!pip install tiktoken\n",
        "#!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "_8kTLTCCuEw6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank"
      ],
      "metadata": {
        "id": "OHW7YKIwu7LN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# PDF 가져오기\n",
        "loaders = [\n",
        "    PyPDFLoader(\"2306.05685v4.pdf\")\n",
        "]\n",
        "\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "  docs.extend(loader.load())"
      ],
      "metadata": {
        "id": "rlGJFvALvivd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from config import OPEN_AI_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_API_KEY"
      ],
      "metadata": {
        "id": "kWtIP5-TvpL4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "chunking"
      ],
      "metadata": {
        "id": "QNmjk2s0wSmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=150,\n",
        ")\n",
        "\n",
        "# chunking\n",
        "split_docs = r_splitter.split_documents(docs)\n",
        "print(split_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_K8Cf3QwGXg",
        "outputId": "7d7d8a0e-13d9-465f-99e8-d448059de4dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': '2306.05685v4.pdf', 'page': 0}, page_content='Judging LLM-as-a-Judge\\nwith MT-Bench and Chatbot Arena\\nLianmin Zheng1∗Wei-Lin Chiang1∗Ying Sheng4∗Siyuan Zhuang1\\nZhanghao Wu1Yonghao Zhuang3Zi Lin2Zhuohan Li1Dacheng Li13\\nEric P. Xing35Hao Zhang12Joseph E. Gonzalez1Ion Stoica1\\n1UC Berkeley2UC San Diego3Carnegie Mellon University4Stanford5MBZUAI\\nAbstract\\nEvaluating large language model (LLM) based chat assistants is challenging due to\\ntheir broad capabilities and the inadequacy of existing benchmarks in measuring'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 0}, page_content='their broad capabilities and the inadequacy of existing benchmarks in measuring\\nhuman preferences. To address this, we explore using strong LLMs as judges to\\nevaluate these models on more open-ended questions. We examine the usage and\\nlimitations of LLM-as-a-judge, including position, verbosity, and self-enhancement\\nbiases, as well as limited reasoning ability, and propose solutions to mitigate some\\nof them. We then verify the agreement between LLM judges and human preferences'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 0}, page_content='of them. We then verify the agreement between LLM judges and human preferences\\nby introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot\\nArena, a crowdsourced battle platform. Our results reveal that strong LLM judges\\nlike GPT-4 can match both controlled and crowdsourced human preferences well,\\nachieving over 80% agreement, the same level of agreement between humans.\\nHence, LLM-as-a-judge is a scalable and explainable way to approximate human'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 0}, page_content='Hence, LLM-as-a-judge is a scalable and explainable way to approximate human\\npreferences, which are otherwise very expensive to obtain. Additionally, we show\\nour benchmark and traditional benchmarks complement each other by evaluating\\nseveral variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes,\\nand 30K conversations with human preferences are publicly available at https:\\n//github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge .\\n1 Introduction'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 0}, page_content='//github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge .\\n1 Introduction\\nThere has been a proliferation of LLM-based chat assistants (chatbots) that leverage supervised\\ninstruction fine-tuning and reinforcement learning with human feedback (RLHF) to unlock new\\ninstruction following and conversational abilities [ 31,2,30,8,52,48,14]. Once aligned with\\nhumans, these chat models are strongly preferred by human users over the original, unaligned models'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 0}, page_content='humans, these chat models are strongly preferred by human users over the original, unaligned models\\non which they are built. However, the heightened user preference does not always correspond to\\nimproved scores on traditional LLM benchmarks – benchmarks like MMLU [ 19] and HELM [ 24]\\ncannot effectively tell the difference between these aligned models and the base models. This\\nphenomenon suggests that there is a fundamental discrepancy between user perceptions of the'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 0}, page_content='phenomenon suggests that there is a fundamental discrepancy between user perceptions of the\\nusefulness of chatbots and the criteria adopted by conventional benchmarks.\\nWe argue that this discrepancy primarily arises due to existing evaluation that only measures LLMs’\\ncore capability on a confined set of tasks (e.g., multi-choice knowledge or retrieval questions),\\nwithout adequately assessing its alignment with human preference in open-ended tasks, such as the'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 0}, page_content='without adequately assessing its alignment with human preference in open-ended tasks, such as the\\nability to accurately adhere to instructions in multi-turn dialogues. As a demonstration, we show\\n∗Joint first authors. This paper is an extended version of our earlier blog post [8].\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.arXiv:2306.05685v4  [cs.CL]  24 Dec 2023'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content='Question:If the FED buys bonds in the secondary market(A) the money supply will increase. (B) the money supply will decrease. (C) the money supply will not be affected. (D) the discount rate would be affected. Output the answer only.Assistant B:(A) the money supply will increase.Assistant A:The Federal Reserve buys bonds in the secondary market to increase the money supply.Follow-up Question: How does it affect my daily life? Give 3 examples.Assistant B:When the Federal Reserve buys bonds in the'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content=\"the money supply.Follow-up Question: How does it affect my daily life? Give 3 examples.Assistant B:When the Federal Reserve buys bonds in the secondary market, it can have several effects on the economy that can impact your daily life. Here are three examples:1. Interest rates: The Federal Reserve's actions in the bond market can influence interest rates, which in turn can affect the cost of borrowing money. For example, if the Federal Reserve buys bonds and drives up demand for them, interest\"), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content='rates, which in turn can affect the cost of borrowing money. For example, if the Federal Reserve buys bonds and drives up demand for them, interest rates on things like mortgages, car loans, and credit cards may go up, making it more expensive to borrow money.2. Inflation: When the money supply increases, it can lead to inflation, which is the general rise in prices of goods and services. This can impact your daily life by making it more expensive to buy things like groceries, gas, and other'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content=\"rise in prices of goods and services. This can impact your daily life by making it more expensive to buy things like groceries, gas, and other necessities.3. Employment: The Federal Reserve's actions in the bond market can also have an impact on employment. For example, if the Federal Reserve's actions lead to economic growth and increased demand for goods and services, it can create more job opportunities and boost employment.Assistant A:The Federal Reserve buys bonds in the secondary market\"), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content=\"for goods and services, it can create more job opportunities and boost employment.Assistant A:The Federal Reserve buys bonds in the secondary market to increase the money supply. This affects your daily life by:1. Increasing the money supply.2. Increasing the money supply and lowering the discount rate.3. Increasing the money supply and lowering the discount rate and increasing the money supply.GPT-4 Judgment:Assistant A provided an incorrect response to the user's question about how the\"), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content=\"the discount rate and increasing the money supply.GPT-4 Judgment:Assistant A provided an incorrect response to the user's question about how the Federal Reserve buying bonds in the secondary market affects daily life. The answer given is repetitive and lacks clear examples of how the action impacts daily life.On the other hand, Assistant B provided a relevant and accurate response to the user's question about the Federal Reserve buying bonds. The answer includes three clear examples of how the\"), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content=\"a relevant and accurate response to the user's question about the Federal Reserve buying bonds. The answer includes three clear examples of how the action impacts daily life, such as interest rates, inflation, and employment.Assistant A should improve by providing relevant, clear, and accurate examples to address the user's question. Assistant B's response is more helpful, relevant, and detailed.Figure 1: Multi-turn dialogues between a user and two AI assistants—LLaMA-13B (Assistant A)\"), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content='and Vicuna-13B (Assistant B)—initiated by a question from the MMLU benchmark and a follow-up\\ninstruction. GPT-4 is then presented with the context to determine which assistant answers better.\\nconversation histories with two models on an MMLU question in Figure 1. The two models are\\nLLaMA-13B [ 39], a pre-trained base model without fine-tuning, and Vicuna-13B, our fine-tuned\\nmodel from LLaMA-13B on high-quality conversations (the training details are in Appendix E).'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content='model from LLaMA-13B on high-quality conversations (the training details are in Appendix E).\\nDespite the base LLaMA models showing competitive performance on conventional benchmarks\\n(Table 8), its answers to open-ended questions are often not preferred by humans. This misalignment\\nof conventional benchmarks underscores the core problem driving this paper: the need for a robust\\nand scalable automated method to evaluate LLM alignment with human preferences.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content='and scalable automated method to evaluate LLM alignment with human preferences.\\nTo study this, we introduce two benchmarks with human ratings as the primary evaluation metric:\\nMT-bench and Chatbot Arena. MT-bench is a series of open-ended questions that evaluate a chatbot’s\\nmulti-turn conversational and instruction-following ability – two critical elements for human prefer-\\nence. MT-bench is also carefully constructed to differentiate chatbots based on their core capabilities,'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content='ence. MT-bench is also carefully constructed to differentiate chatbots based on their core capabilities,\\nsuch as reasoning and math. In addition, we develop Chatbot Arena, a crowdsourced platform fea-\\nturing anonymous battles between chatbots in real-world scenarios – Users engage in conversations\\nwith two chatbots at the same time and rate their responses based on personal preferences.\\nWhile human evaluation is the gold standard for assessing human preferences, it is exceptionally slow'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content='While human evaluation is the gold standard for assessing human preferences, it is exceptionally slow\\nand costly. To automate the evaluation, we explore the use of state-of-the-art LLMs, such as GPT-4,\\nas a surrogate for humans. Because these models are often trained with RLHF, they already exhibit\\nstrong human alignment. We call this approach “LLM-as-a-judge” . This approach has been tried in\\nour earlier blog post [ 8] and other concurrent or follow-up work [ 5,29,14,12,52,18,33,40,7,43].'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content='our earlier blog post [ 8] and other concurrent or follow-up work [ 5,29,14,12,52,18,33,40,7,43].\\nHowever, there has not been a systematic study of this approach.\\nIn this paper, we study the LLM-as-a-judge approach by comparing it to the gold standard of\\nhuman evaluation. We examine several potential limitations of the LLM-as-a-judge approach\\nincluding position bias, verbosity bias, self-enhancement bias, and limited reasoning ability. We'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1}, page_content='including position bias, verbosity bias, self-enhancement bias, and limited reasoning ability. We\\nshow that some of the biases are minor or can be mitigated. Once addressed, our results from 3K\\ncontrolled expert votes and 3K crowdsourced human votes in the wild verify that GPT-4 judge match\\n2'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 2}, page_content='human evaluations at an agreement rate exceeding 80%, achieving the same level of human-human\\nagreement (§4.2, Table 4). Consequently, this suggests LLM-as-a-judge is a scalable method to\\nswiftly evaluate human preference, serving as a promising alternative to traditional human evaluations.\\nThis paper makes two contributions: (1) a systematic study of LLM-as-a-judge; and (2) human\\npreference datasets with high-quality questions and diverse user interactions from MT-bench and'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 2}, page_content='preference datasets with high-quality questions and diverse user interactions from MT-bench and\\nChatbot Arena. In addition, we argue for the adoption of a hybrid evaluation framework for future\\nLLM benchmarks: by combining the existing capability-based benchmarks and the new preference-\\nbased benchmarks with LLM-as-a-judge, one can swiftly and automatically evaluate both the core\\ncapabilities and human alignment of models. We publicly release 80 MT-bench questions, 3K expert'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 2}, page_content='capabilities and human alignment of models. We publicly release 80 MT-bench questions, 3K expert\\nvotes, and 30K conversations with human preferences for future study.\\nTable 1: Sample multi-turn questions in MT-bench.\\nCategory Sample Questions\\nWriting1st Turn Compose an engaging travel blog post about a recent trip to Hawaii, highlighting\\ncultural experiences and must-see attractions.\\n2nd Turn Rewrite your previous response. Start every sentence with the letter A.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 2}, page_content='cultural experiences and must-see attractions.\\n2nd Turn Rewrite your previous response. Start every sentence with the letter A.\\nMath1st Turn Given that f(x) = 4 x3−9x−14, find the value of f(2).\\n2nd Turn Find xsuch that f(x) = 0 .\\nKnowledge1st Turn Provide insights into the correlation between economic indicators such as GDP,\\ninflation, and unemployment rates. Explain how fiscal and monetary policies ...\\n2nd Turn Now, explain them again like I’m five.\\n2 MT-Bench and Chatbot Arena\\n2.1 Motivation'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 2}, page_content='2nd Turn Now, explain them again like I’m five.\\n2 MT-Bench and Chatbot Arena\\n2.1 Motivation\\nWith the recent advances of LLMs, LLM-based assistants start to exhibit artificial general intelligence\\nacross diverse tasks, from writing and chatting to coding [ 5,30,1,37]. However, evaluating their\\nbroad capabilities also becomes more challenging. Despite the availability of numerous benchmarks\\nfor language models, they primarily focus on evaluating models on closed-ended questions with short'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 2}, page_content='for language models, they primarily focus on evaluating models on closed-ended questions with short\\nresponses. Given that these chat assistants can now precisely follow user instructions in multi-turn\\ndialogues and answer open-ended questions in a zero-shot manner, current benchmarks are inadequate\\nfor assessing such capabilities. Existing benchmarks mostly fall into the following three categories.\\n•Core-knowledge benchmarks , including MMLU [ 19], HellaSwag [ 50], ARC [ 9], Wino-'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 2}, page_content='•Core-knowledge benchmarks , including MMLU [ 19], HellaSwag [ 50], ARC [ 9], Wino-\\nGrande [ 36], HumanEval [ 6], GSM-8K [ 10], and AGIEval [ 51], evaluate the core capabilities of\\npre-trained LLMs using zero-shot and few-shot benchmark sets. They typically require LLMs to\\ngenerate a short, specific answer to benchmark questions that can be automatically validated.\\n•Instruction-following benchmarks , such as Flan [ 27,46], Self-instruct [ 44], NaturalInstruc-'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 2}, page_content='•Instruction-following benchmarks , such as Flan [ 27,46], Self-instruct [ 44], NaturalInstruc-\\ntions [ 28], Super-NaturalInstructions [ 45], expand to slightly more open-ended questions and\\nmore diverse tasks and are used to evaluate LLMs after instruction fine-tuning.\\n•Conversational benchmarks , like CoQA [ 35], MMDialog [ 15] and OpenAssistant [ 23], are\\nclosest to our intended use cases. However, the diversity and complexity of their questions often'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 2}, page_content='closest to our intended use cases. However, the diversity and complexity of their questions often\\nfall short in challenging the capabilities of the latest chatbots.\\nWhile largely overlooked by existing LLM benchmarks, human preferences serve as a direct measure\\nof a chatbot’s utility in open-ended, multi-turn human-AI interactions. To bridge this gap, we\\nintroduce two novel benchmarks expressly tailored to assess human preferences. Simultaneously,'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 2}, page_content='introduce two novel benchmarks expressly tailored to assess human preferences. Simultaneously,\\nthese benchmarks are designed to distinguish the core capabilities of state-of-the-art models.\\n2.2 MT-Bench\\nWe create MT-bench, a benchmark consisting of 80 high-quality multi-turn questions. MT-bench is\\ndesigned to test multi-turn conversation and instruction-following ability, covering common use cases\\nand focusing on challenging questions to differentiate models. We identify 8 common categories'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 2}, page_content='and focusing on challenging questions to differentiate models. We identify 8 common categories\\nof user prompts to guide its construction: writing, roleplay, extraction, reasoning, math, coding,\\n3'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 3}, page_content='knowledge I (STEM), and knowledge II (humanities/social science). For each category, we then\\nmanually designed 10 multi-turn questions. Table 1 lists several sample questions.\\n2.3 Chatbot Arena\\nOur second approach is Chatbot Arena, a crowdsourcing benchmark platform featuring anonymous\\nbattles. On this platform, users can interact with two anonymous models simultaneously, posing\\nthe same question to both. They vote for which model provides the preferred response, with the'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 3}, page_content='the same question to both. They vote for which model provides the preferred response, with the\\nidentities of the models disclosed post-voting. After running Chatbot Arena for one month, we have\\ncollected around 30K votes. Since the platform does not use pre-defined questions, it allows gathering\\na wide range of unrestricted use cases and votes in the wild, based on the diverse interests of users. A\\nscreenshot of the platform can be found at Appendix C.2.\\n3 LLM as a Judge'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 3}, page_content='screenshot of the platform can be found at Appendix C.2.\\n3 LLM as a Judge\\nWhile our initial evaluations using MT-bench and Chatbot Arena rely on human ratings, collecting\\nhuman preferences can be costly and laborious [ 44,38,31,2,13]. To overcome this, we aim to\\ndevelop a more scalable and automated approach. Given that most questions in MT-bench and Chatbot\\nArena are open-ended without reference answers, devising a rule-based program to assess the outputs'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 3}, page_content='Arena are open-ended without reference answers, devising a rule-based program to assess the outputs\\nis extremely challenging. Traditional evaluation metrics based on the similarity between outputs and\\nreference answers (e.g., ROUGE [25], BLEU [32]) are also ineffective for these questions.\\nAs LLMs continue to improve, they show potential in replacing human annotators in many tasks [ 17,\\n20]. Specifically, we are interested in whether LLMs can effectively evaluate the responses of chat'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 3}, page_content='20]. Specifically, we are interested in whether LLMs can effectively evaluate the responses of chat\\nassistants and match human preferences. Next, we discuss the use and limitations of LLM-as-a-judge.\\n3.1 Types of LLM-as-a-Judge\\nWe propose 3 LLM-as-a-judge variations. They can be implemented independently or in combination:\\n•Pairwise comparison . An LLM judge is presented with a question and two answers, and tasked'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 3}, page_content='•Pairwise comparison . An LLM judge is presented with a question and two answers, and tasked\\nto determine which one is better or declare a tie. The prompt used is given in Figure 5 (Appendix).\\n•Single answer grading . Alternatively, an LLM judge is asked to directly assign a score to a\\nsingle answer. The prompt used for this scenario is in Figure 6 (Appendix).\\n•Reference-guided grading . In certain cases, it may be beneficial to provide a reference solution'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 3}, page_content='•Reference-guided grading . In certain cases, it may be beneficial to provide a reference solution\\nif applicable. An example prompt we use for grading math problems is in Figure 8 (Appendix).\\nThese methods have different pros and cons. For example, the pairwise comparison may lack\\nscalability when the number of players increases, given that the number of possible pairs grows\\nquadratically; single answer grading may be unable to discern subtle differences between specific'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 3}, page_content='quadratically; single answer grading may be unable to discern subtle differences between specific\\npairs, and its results may become unstable, as absolute scores are likely to fluctuate more than relative\\npairwise results if the judge model changes.\\n3.2 Advantages of LLM-as-a-Judge\\nLLM-as-a-judge offers two key benefits: scalability andexplainability . It reduces the need for human\\ninvolvement, enabling scalable benchmarks and fast iterations. Additionally, LLM judges provide'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 3}, page_content='involvement, enabling scalable benchmarks and fast iterations. Additionally, LLM judges provide\\nnot only scores but also explanations, making their outputs interpretable, as shown in Figure 1.\\n3.3 Limitations of LLM-as-a-Judge\\nWe identify certain biases and limitations of LLM judges. However, we will also present solutions\\nlater and show the agreement between LLM judges and humans is high despite these limitations.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 3}, page_content='later and show the agreement between LLM judges and humans is high despite these limitations.\\nPosition bias is when an LLM exhibits a propensity to favor certain positions over others. This\\nbias is not unique to our context and has been seen in human decision-making [ 3,34] and other ML\\ndomains [22, 41].\\nFigure 11 (Appendix) shows an example of position bias. GPT-4 is tasked to evaluate two responses\\nfrom GPT-3.5 and Vicuna-13B to an open-ended question. When GPT-3.5’s answer is positioned\\n4'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 4}, page_content='Table 2: Position bias of different LLM judges. Consistency is the percentage of cases where a\\njudge gives consistent results when swapping the order of two assistants. “Biased toward first” is the\\npercentage of cases when a judge favors the first answer. “Error” indicates wrong output formats.\\nThe two largest numbers in each column are in bold.\\nJudge Prompt Consistency Biased toward first Biased toward second Error\\nClaude-v1default 23.8% 75.0% 0.0% 1.2%\\nrename 56.2% 11.2% 28.7% 3.8%'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 4}, page_content='Judge Prompt Consistency Biased toward first Biased toward second Error\\nClaude-v1default 23.8% 75.0% 0.0% 1.2%\\nrename 56.2% 11.2% 28.7% 3.8%\\nGPT-3.5default 46.2% 50.0% 1.2% 2.5%\\nrename 51.2% 38.8% 6.2% 3.8%\\nGPT-4default 65.0% 30.0% 5.0% 0.0%\\nrename 66.2% 28.7% 5.0% 0.0%\\nTable 3: Failure rate under “repetitive list” at-\\ntack for different LLM judges on 23 answers.\\nJudge Claude-v1 GPT-3.5 GPT-4\\nFailure rate 91.3% 91.3% 8.7%Table 4: Judge failure rate on 10 math questions'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 4}, page_content='Judge Claude-v1 GPT-3.5 GPT-4\\nFailure rate 91.3% 91.3% 8.7%Table 4: Judge failure rate on 10 math questions\\nwith different prompts. We test LLaMA-13B vs.\\nVicuna-13B and swap positions. A failure means\\nwhen GPT-4 says an incorrect answer is correct.\\nDefault CoT Reference\\nFailure rate 14/20 6/20 3/20\\nfirst, GPT-4 considers GPT-3.5’s answer more detailed and superior. However, upon switching the\\npositions of the two responses, GPT-4’s judgement flips, favoring Vicuna’s answer.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 4}, page_content='positions of the two responses, GPT-4’s judgement flips, favoring Vicuna’s answer.\\nTo analyze the position bias, we construct two similar answers to each first-turn question in MT-bench\\nby calling GPT-3.5 twice with a temperature of 0.7. We then try three LLMs with two different\\nprompts: “default” is our default prompt in Figure 5 (Appendix). “rename” renames the assistants in\\nour default prompt to see whether the bias is on positions or names. As in Table 2, we found all of'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 4}, page_content='our default prompt to see whether the bias is on positions or names. As in Table 2, we found all of\\nthem exhibit strong position bias. Most LLM judges favor the first position. Claude-v1 also shows a\\nname bias which makes it favors \"Assistant A\", as illustrated by the \"rename\" prompt. The position\\nbias can be very significant. Only GPT-4 outputs consistent results in more than 60% of cases.\\nNote that this test is challenging because the answers are very similar and occasionally indistinguish-'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 4}, page_content='Note that this test is challenging because the answers are very similar and occasionally indistinguish-\\nable even to humans. We will show that position bias is less prominent in some cases in Appendix D.1.\\nAs for the origin of this bias, we suspect that it could be rooted in the training data or inherent to the\\nleft-to-right architecture of causal transformers, but leave a deeper study as future work.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 4}, page_content='left-to-right architecture of causal transformers, but leave a deeper study as future work.\\nVerbosity bias is when an LLM judge favors longer, verbose responses, even if they are not as clear,\\nhigh-quality, or accurate as shorter alternatives.\\nTo examine this bias, we design a “repetitive list” attack with model answers from MT-bench. We\\nfirst select 23 model answers from MT-bench that contain a numbered list. We then make them'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 4}, page_content='first select 23 model answers from MT-bench that contain a numbered list. We then make them\\nunnecessarily verbose by asking GPT-4 to rephrase the list without adding any new information and\\ninsert the rephrased new list to the beginning of the original list. For example, if the original response\\ncontains 5 items, then the new response will contain 10 items but the first 5 items are rephrased from'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 4}, page_content='contains 5 items, then the new response will contain 10 items but the first 5 items are rephrased from\\nthe original 5 items. An example is shown in Figure 12 (Appendix). We define the attack is successful\\nif an LLM judge thinks the new response is better than the old response. Table 3 shows the failure\\nrate of LLM judges under this attack, demonstrating that all LLMs may be prone to verbosity bias\\nthough GPT-4 defends significantly better than others. As a calibration, we find LLM judges are'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 4}, page_content='though GPT-4 defends significantly better than others. As a calibration, we find LLM judges are\\nable to correctly judge identical answers (i.e., they always return a tie for two identical answers) but\\ncannot pass the more advanced “repetitive list” attack.\\nSelf-enhancement bias. We adopt the term “self-enhancement bias” from social cognition litera-\\nture [4] to describe the effect that LLM judges may favor the answers generated by themselves.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 4}, page_content='ture [4] to describe the effect that LLM judges may favor the answers generated by themselves.\\nWe examine this effect statistically. Figure 3(b) shows the win rate (w/o tie) of six models under\\ndifferent LLM judges and humans. Compared to humans, we do observe that some judges favor\\ncertain models. For example, GPT-4 favors itself with a 10% higher win rate; Claude-v1 favors itself\\nwith a 25% higher win rate. However, they also favor other models and GPT-3.5 does not favor itself.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 4}, page_content='with a 25% higher win rate. However, they also favor other models and GPT-3.5 does not favor itself.\\nDue to limited data and small differences, our study cannot determine whether the models exhibit\\na self-enhancement bias. Conducting a controlled study is challenging because we cannot easily\\nrephrase a response to fit the style of another model without changing the quality.\\n5'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 5}, page_content='Limited capability in grading math and reasoning questions. LLMs are known to have limited\\nmath and reasoning capability [ 10], which results in its failure of grading such questions because they\\ndo not know the correct answers. However, what is more intriguing is that it also shows limitations in\\ngrading basic math problems which it is capable of solving. For instance, in Figure 13 (Appendix),\\nwe present an example of an elementary math question in which GPT-4 makes an incorrect judgment.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 5}, page_content='we present an example of an elementary math question in which GPT-4 makes an incorrect judgment.\\nIt’s worth noting that although GPT-4 can solve the problem (when asked separately), it was misled\\nby the provided answers, ultimately resulting in incorrect judgment. This pattern can also be seen in\\na reasoning question example in Figure 14 (Appendix). Both GPT-3.5 and Claude-v1 show a similar\\nweakness. In Section 3.4, we will introduce a reference-guided method to mitigate such issues.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 5}, page_content='weakness. In Section 3.4, we will introduce a reference-guided method to mitigate such issues.\\n3.4 Addressing limitations\\nWe present a few methods to address position bias and the limited grading ability for math questions.\\nSwapping positions. The position bias can be addressed by simple solutions. A conservative\\napproach is to call a judge twice by swapping the order of two answers and only declare a win when'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 5}, page_content='approach is to call a judge twice by swapping the order of two answers and only declare a win when\\nan answer is preferred in both orders. If the results are inconsistent after swapping, we can call it a\\ntie. Another more aggressive approach is to assign positions randomly, which can be effective at a\\nlarge scale with the correct expectations. In the following experiments, we use the conservative one.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 5}, page_content='large scale with the correct expectations. In the following experiments, we use the conservative one.\\nFew-shot judge. We assess whether few-shot examples can improve consistency in the position bias\\nbenchmark. We select three good judgment examples using MT-bench-like questions, GPT-3.5 and\\nVicuna for generating answers, and GPT-4 for generating judgments. The examples cover three cases:\\nA is better, B is better, and tie. As shown in Table 12 (Appendix), the few-shot judge can significantly'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 5}, page_content='A is better, B is better, and tie. As shown in Table 12 (Appendix), the few-shot judge can significantly\\nincrease the consistency of GPT-4 from 65.0% to 77.5%. However, high consistency may not imply\\nhigh accuracy and we are not sure whether the few-shot examples will introduce new biases. Besides,\\nthe longer prompts make API calls 4×more expensive. We use the zero-shot prompt by default in\\nour following experiments but leave an additional study in Appendix D.2.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 5}, page_content='our following experiments but leave an additional study in Appendix D.2.\\nChain-of-thought and reference-guided judge. In Section 3.3, we have shown LLM’s limited\\ncapability in grading math and reasoning questions. We propose two simple methods to mitigate\\nthis issue: chain-of-thought judge and reference-guided judge. Chain-of-thought is a widely used\\ntechnique to improve LLM’s reasoning capability [ 47]. We propose a similar technique to prompt'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 5}, page_content='technique to improve LLM’s reasoning capability [ 47]. We propose a similar technique to prompt\\nan LLM judge to begin with answering the question independently and then start grading. Detailed\\nprompt in Figure 7 (Appendix). However, even with the CoT prompt, we find that in many cases\\nLLM makes exactly the same mistake as the given answers in its problem-solving process (See\\nexample in Figure 15 (Appendix), suggesting that LLM judge may still be misled by the context.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 5}, page_content='example in Figure 15 (Appendix), suggesting that LLM judge may still be misled by the context.\\nHence, we propose a reference-guided method, in which we first generate LLM judge’s answer\\nindependently, and then display it as a reference answer in the judge prompt. In Table 4, we see a\\nsignificant improvement in failure rate (from 70% to 15%) over the default prompt.\\nFine-tuning a judge model. We try fine-tuning a Vicuna-13B on arena data to act as a judge and'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 5}, page_content='Fine-tuning a judge model. We try fine-tuning a Vicuna-13B on arena data to act as a judge and\\nshow some promising preliminary results in Appendix F.\\n3.5 Multi-turn judge\\nIn MT-bench, every question involves two turns to evaluate conversational abilities. Therefore, when\\ncomparing two assistants, it becomes necessary to present a total of two questions and four responses,\\ncomplicating the prompt design. We explore two possible designs, (1) breaking the two turns into two'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 5}, page_content='complicating the prompt design. We explore two possible designs, (1) breaking the two turns into two\\nprompts or (2) displaying complete conversations in a single prompt. Our finding is the former one\\ncan cause the LLM judge struggling to locate the assistant’s previous response precisely. We illustrate\\na case in Figure 16 (Appendix) where GPT-4 makes an inaccurate judgment due to a faulty reference.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 5}, page_content='a case in Figure 16 (Appendix) where GPT-4 makes an inaccurate judgment due to a faulty reference.\\nThis suggests the necessity of displaying a complete conversation to enable the LLM judge to better\\ngrasp the context. We then consider the alternative design that presents two full conversations in a\\nsingle prompt in which we ask the LLM judge to focus on the second question (Figure 9 (Appendix)).\\nThis approach has been found to significantly alleviate the aforementioned referencing issue.\\n6'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 6}, page_content='4 Agreement Evaluation\\nWe study the agreement between different LLM judges and humans on MT-bench and Chatbot\\nArena datasets. On MT-bench, we also study the agreement among humans. MT-bench represents a\\nsmall-scale study with controlled human evaluation, while Chatbot Arena represents a larger-scale\\nstudy with crowdsourced human evaluation in the wild.\\n4.1 Setup\\nMT-bench. We generate answers for all 80 questions with 6 models: GPT-4, GPT-3.5, Claude-V1,'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 6}, page_content='4.1 Setup\\nMT-bench. We generate answers for all 80 questions with 6 models: GPT-4, GPT-3.5, Claude-V1,\\nVicuna-13B, Alpaca-13B [ 38], and LLaMA-13B [ 39]. We then use 2 kinds of judges: LLM judges\\nand 58 expert-level human labelers. The labelers are mostly graduate students so they are considered\\nexperts and more skilled than average crowd workers. We let LLM judges evaluate all pairs and let\\neach human evaluate at least 20 random multi-turn questions. This resulted in around 3K votes for all'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 6}, page_content='each human evaluate at least 20 random multi-turn questions. This resulted in around 3K votes for all\\nquestions. The detailed data collection process is in Appendix C.\\nChatbot Arena. We randomly sample 3K single-turn votes from 30K arena data, which covers\\nmodels including GPT-4, GPT-3.5, Claude, Vicuna-7B/13B, Koala-13B [ 16], Alpaca-13B, LLaMA-\\n13B, and Dolly-12B. We use two kinds of judges: LLM judges and collected crowd judges (2114\\nunique IPs).'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 6}, page_content='13B, and Dolly-12B. We use two kinds of judges: LLM judges and collected crowd judges (2114\\nunique IPs).\\nMetrics. We define the agreement between two types of judges as the probability of randomly\\nselected individuals (but not identical) of each type agreeing on a randomly selected question. See\\nmore explanation in Appendix D.3. Average win rate is the average of win rates against all other\\nplayers. These metrics can be computed with or without including tie votes.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 6}, page_content='players. These metrics can be computed with or without including tie votes.\\n4.2 High agreement between GPT-4 and humans\\nWe compute agreement on MT-bench data. In Table 5, GPT-4 with both pairwise comparison and\\nsingle answer grading show very high agreements with human experts. The agreement under setup S2\\n(w/o tie) between GPT-4 and humans reaches 85%, which is even higher than the agreement among\\nhumans (81%). This means GPT-4’s judgments closely align with the majority of humans. We also'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 6}, page_content='humans (81%). This means GPT-4’s judgments closely align with the majority of humans. We also\\nshow that GPT-4’s judgments may help humans make better judgments. During our data collection,\\nwhen a human’s choice deviated from GPT-4, we presented GPT-4’s judgments to humans and ask\\nif they are reasonable (details in Appendix C.1). Despite different views, humans deemed GPT-4’s\\njudgments reasonable in 75% of cases and are even willing to change their choices in 34% of cases.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 6}, page_content='judgments reasonable in 75% of cases and are even willing to change their choices in 34% of cases.\\nThe data from Arena shows a similar trend, as illustrated by Table 6. Comparing GPT-4 and other\\nLLM judges, we find they reach a similar non-tie agreement ratio between humans but the number\\nof non-tied votes from GPT-4 is much larger. This means that GPT-4 is more affirmative and less\\nsuffered from position bias but other models also perform well when they give an affirmative answer.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 6}, page_content='suffered from position bias but other models also perform well when they give an affirmative answer.\\nIn both tables, GPT-4 with single-answer grading matches both pairwise GPT-4 and human prefer-\\nences very well. This means GPT-4 has a relatively stable internal rubric. Although it may sometimes\\nperform slightly worse than pairwise comparison and give more tie votes, it is a more scalable method.\\nWe then perform a breakdown analysis by computing agreement on different model pairs and'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 6}, page_content='We then perform a breakdown analysis by computing agreement on different model pairs and\\ncategories. We only include non-tied votes. In Figure 2, we observe the agreement between GPT-4\\nand human progressively increases in line with the performance disparity of the model pairs (i.e.,\\nlarger win rate difference), from 70% to nearly 100%. This suggests that GPT-4 aligns with humans\\nbetter when significant performance differences exist between the models.\\nGPT-4Claude GPT-3.5'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 6}, page_content='better when significant performance differences exist between the models.\\nGPT-4Claude GPT-3.5\\nVicuna-13B Alpaca-13B LLaMA-13B\\n(a) All votes, first turn0.00.20.40.60.81.0\\nGPT-4GPT-3.5 Claude\\nVicuna-13B Alpaca-13B LLaMA-13B\\n(b) Non-tied votes, first turn0.00.20.40.60.81.0\\nGPT-4Claude GPT-3.5\\nVicuna-13B Alpaca-13B LLaMA-13B\\n(c) All votes, second turn0.00.20.40.60.81.0\\nGPT-4Claude GPT-3.5\\nVicuna-13B Alpaca-13B LLaMA-13B\\n(d) Non-tied votes, second turn0.00.20.40.60.81.0'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 6}, page_content='(c) All votes, second turn0.00.20.40.60.81.0\\nGPT-4Claude GPT-3.5\\nVicuna-13B Alpaca-13B LLaMA-13B\\n(d) Non-tied votes, second turn0.00.20.40.60.81.0\\nWin rateGPT-4 Judge GPT-3.5 Judge Claude Judge Human Human (first turn)\\nFigure 3: Average win rate of six models under different judges on MT-bench.\\n7'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 7}, page_content='Table 5: Agreement between two types of judges on MT-bench. “G4-Pair” and “G4-Single” denote\\nGPT-4 with pairwise comparison and single-answer grading respectively. The single-answer grading\\ncan be converted into pairwise comparison results for calculating the agreement. We report two\\nsetups: “S1” includes non-tie, tie, and inconsistent (due to position bias) votes and counts inconsistent\\nas tie; “S2” only includes non-tie votes. The agreement between two random judges under each setup'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 7}, page_content='as tie; “S2” only includes non-tie votes. The agreement between two random judges under each setup\\nis denoted as “R=”. The top value in each cell is the agreement, and the bottom gray value is #votes.\\nSetup S1 (R = 33%) S2 (R = 50%)\\nJudge G4-Single Human G4-Single Human\\nG4-Pair70%\\n113866%\\n134397%\\n66285%\\n859\\nG4-Single -60%\\n1280 -85%\\n739\\nHuman -63%\\n721 -81%\\n479\\n(a) First TurnSetup S1 (R = 33%) S2 (R = 50%)\\nJudge G4-Single Human G4-Single Human\\nG4-Pair70%\\n116166%\\n132595%\\n72785%\\n864\\nG4-Single -59%'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 7}, page_content='721 -81%\\n479\\n(a) First TurnSetup S1 (R = 33%) S2 (R = 50%)\\nJudge G4-Single Human G4-Single Human\\nG4-Pair70%\\n116166%\\n132595%\\n72785%\\n864\\nG4-Single -59%\\n1285 -84%\\n776\\nHuman -67%\\n707 -82%\\n474\\n(b) Second Turn\\nTable 6: Agreement between two types of judges on Chatbot\\nArena. “G4-S” denotes GPT-4 with single-answer grading.\\n“G4”, “G3.5” and “C” denote GPT-4, GPT-3.5, and Claude\\nwith pairwise comparison, respectively. “H” denotes human.\\nThe remaining of table follows the same format as Table 5.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 7}, page_content='with pairwise comparison, respectively. “H” denotes human.\\nThe remaining of table follows the same format as Table 5.\\nSetup S1 (Random = 33%) S2 (Random = 50%)\\nJudge G4-S G3.5 C H G4-S G3.5 C H\\nG472%\\n296866%\\n306166%\\n306264%\\n306695%\\n196794%\\n178895%\\n171287%\\n1944\\nG4-S -60%\\n296462%\\n296460%\\n2968 -89%\\n159391%\\n153885%\\n1761\\nG3.5 - -68%\\n305754%\\n3061 - -96%\\n149783%\\n1567\\nC - - -53%\\n3062 - - -84%\\n1475\\n0.0 0.5 1.0\\nWin rate difference0.70.80.91.0AgreementFigure 2: Agreement and win rate dif-'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 7}, page_content='3061 - -96%\\n149783%\\n1567\\nC - - -53%\\n3062 - - -84%\\n1475\\n0.0 0.5 1.0\\nWin rate difference0.70.80.91.0AgreementFigure 2: Agreement and win rate dif-\\nference. Each point corresponds to a\\nmodel pair and counts only the non-tie\\nvotes between the two models. The x-\\naxis value is the win rate difference be-\\ntween the two models. The y-axis value\\nis the GPT-4 and human agreement.\\nGPT-4 Claude GPT-3.5\\nVicuna-13BVicuna-7B Koala-13BAlpaca-13BDolly-12BLLaMA-13B\\n(a) All votes0.00.20.40.60.81.0'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 7}, page_content='is the GPT-4 and human agreement.\\nGPT-4 Claude GPT-3.5\\nVicuna-13BVicuna-7B Koala-13BAlpaca-13BDolly-12BLLaMA-13B\\n(a) All votes0.00.20.40.60.81.0\\nGPT-4 Claude GPT-3.5\\nVicuna-13BVicuna-7B Koala-13BAlpaca-13BDolly-12BLLaMA-13B\\n(b) Non-tied votes0.00.20.40.60.81.0\\nWin rateGPT-4 Judge GPT-3.5 Judge Human GPT-4-Single Judge\\nFigure 4: Average win rate of nine models under different judges on Chatbot Arena.\\nTable 7: Category-wise win rate of models.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 7}, page_content='Figure 4: Average win rate of nine models under different judges on Chatbot Arena.\\nTable 7: Category-wise win rate of models.\\nModel Writing Roleplay Reasoning Math Coding Extraction STEM Humanities\\nGPT-4 61.2% 67.9% 49.3% 66.1% 56.3% 66.2% 76.6% 72.2%\\nGPT-3.5 50.9% 60.6% 32.6% 63.8% 55.0% 48.8% 52.8% 53.8%\\nVicuna-13B 39.7% 39.2% 20.1% 18.0% 36.9% 29.2% 47.0% 47.5%\\nLLaMA-13B 15.1% 15.1% 7.8% 7.5% 2.1% 9.3% 6.8% 10.1%\\n4.3 Win rates under different judges'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 7}, page_content='Vicuna-13B 39.7% 39.2% 20.1% 18.0% 36.9% 29.2% 47.0% 47.5%\\nLLaMA-13B 15.1% 15.1% 7.8% 7.5% 2.1% 9.3% 6.8% 10.1%\\n4.3 Win rates under different judges\\nWe plot the average win rate of models under different judges on MT-bench and Chatbot Arena\\nin Figure 3 and Figure 4, respectively. The win rate curves from LLM judges closely match the\\ncurves from humans. On MT-bench second turn, proprietary models like Claude and GPT-3.5 are'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 7}, page_content='curves from humans. On MT-bench second turn, proprietary models like Claude and GPT-3.5 are\\nmore preferred by the humans compared to the first turn, meaning that a multi-turn benchmark can\\nbetter differentiate some advanced abilities of models. We also list the per-category win rate of\\n8'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 8}, page_content='Table 8: Evaluation results of several model variants.\\nModel #Training Token MMLU (5-shot) TruthfulQA (0-shot) MT-Bench Score (GPT-4)\\nLLaMA-7B 1T 35.2 0.22 2.74\\nLLaMA-13B 1T 47.0 0.26 2.61\\nAlpaca-7B 4.4M 40.1 0.26 4.54\\nAlpaca-13B 4.4M 48.1 0.30 4.53\\nVicuna-7B (selected) 4.8M 37.3 0.32 5.95\\nVicuna-7B (single) 184M 44.1 0.30 6.04\\nVicuna-7B (all) 370M 47.1 0.32 6.00\\nVicuna-13B (all) 370M 52.1 0.35 6.39\\nGPT-3.5 - 70.0 - 7.94\\nGPT-4 - 86.4 - 8.99'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 8}, page_content='Vicuna-7B (all) 370M 47.1 0.32 6.00\\nVicuna-13B (all) 370M 52.1 0.35 6.39\\nGPT-3.5 - 70.0 - 7.94\\nGPT-4 - 86.4 - 8.99\\nrepresentative models in Table 7 to show how MT-bench differentiates models, in which we see GPT-4\\nis significantly better than others. Vicuna-13B is noticeably worse than GPT-3.5/4 in reasoning, math,\\nand coding categories. Note that in math/coding category, GPT-3.5 and GPT-4 have similar overall'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 8}, page_content='and coding categories. Note that in math/coding category, GPT-3.5 and GPT-4 have similar overall\\nwin-rate because they both failed to answer some hard questions, but GPT-4 is still significantly better\\nthan GPT-3 in the direct pairwise comparison or single-answer grading. Please see a performance\\nbreakdown of MT-bench score for each category in Appendix D.4.\\n5 Human Preference Benchmark and Standardized Benchmark'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 8}, page_content='breakdown of MT-bench score for each category in Appendix D.4.\\n5 Human Preference Benchmark and Standardized Benchmark\\nHuman preference benchmarks such as MT-bench and Chatbot Arena serve as valuable additions\\nto the current standardized LLM benchmarks. They focus on different aspects of a model and the\\nrecommended way is to comprehensively evaluate models with both kinds of benchmarks.\\nWe evaluate several model variants derived from LLaMA on MMLU [ 19], Truthful QA [ 26] (MC1),'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 8}, page_content='We evaluate several model variants derived from LLaMA on MMLU [ 19], Truthful QA [ 26] (MC1),\\nand MT-bench (GPT-4 judge). The training details are in Appendix E. Since we have shown that\\nGPT-4 single-answer grading also performs well in Section 4.2, we use GPT-4 single-answer grading\\nfor MT-bench in favor of its scalability and simplicity. We ask GPT-4 to give a score for each turn\\non a scale of 10 by using our prompt templates (Figure 6, Figure 10) and report an average score of'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 8}, page_content='on a scale of 10 by using our prompt templates (Figure 6, Figure 10) and report an average score of\\n160 = 80 ×2turns. Table 8 shows the results. We find that fine-tuning on high-quality dialog datasets\\n(i.e., ShareGPT) can consistently improve the model performance on MMLU and the improvement\\nscales with fine-tuning data size. On the other hand, a small high-quality conversation dataset can\\nquickly teach the model a style preferred by GPT-4 (or approximately human) but cannot improve'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 8}, page_content='quickly teach the model a style preferred by GPT-4 (or approximately human) but cannot improve\\nMMLU significantly, as shown by the Vicuna-7B (selected) which is trained with only 4.8M tokens\\nor 3K conversations. In Table 8, no single benchmark can determine model quality, meaning that a\\ncomprehensive evaluation is needed. Our results indicate that using LLM-as-a-judge to approximate\\nhuman preferences is highly feasible and could become a new standard in future benchmarks. We'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 8}, page_content='human preferences is highly feasible and could become a new standard in future benchmarks. We\\nare also hosting a regularly updated leaderboard with more models2. Notably, DynaBench [ 21], a\\nresearch platform dedicated to dynamic data collection and benchmarking, aligns with our spirit.\\nDynaBench addresses the challenges posed by static standardized benchmarks, such as saturation and\\noverfitting, by emphasizing dynamic data with human-in-the-loop. Our LLM-as-a-judge approach'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 8}, page_content='overfitting, by emphasizing dynamic data with human-in-the-loop. Our LLM-as-a-judge approach\\ncan automate and scale platforms of this nature.\\n6 Discussion\\nLimitations. This paper emphasizes helpfulness but largely neglects safety. Honesty and harm-\\nlessness are crucial for a chat assistant as well [ 2]. We anticipate similar methods can be used to\\nevaluate these metrics by modifying the default prompt. Additionally, within helpfulness, there'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 8}, page_content='evaluate these metrics by modifying the default prompt. Additionally, within helpfulness, there\\nare multiple dimensions like accuracy, relevance, and creativity, but they are all combined into a\\nsingle metric in this study. A more comprehensive evaluation can be developed by analyzing and\\nseparating these dimensions. We propose preliminary solutions to address the limitations and biases\\nof LLM-as-a-judge in Section 3.4, but we anticipate more advanced methods can be developed.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 8}, page_content='of LLM-as-a-judge in Section 3.4, but we anticipate more advanced methods can be developed.\\nData collection and release. Appendix C describes the detailed data collection and release processes,\\nwhich include the instructions we give to users, the screenshots of the data collection interface, the\\ninformation about participated users, and the content of the released data.\\n2https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\\n9'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 9}, page_content='Societal impacts. The societal impact of this study is multi-faceted. Our evaluation methods can\\nhelp enhance chatbot quality and user experiences. However, addressing biases in these methods is\\ncrucial. Our dataset enables better studies of human preferences and model behavior. Advanced chat\\nassistants may replace certain human tasks, resulting in job displacements and new opportunities.\\nFuture directions. 1) Benchmarking chatbots at scale with a broader set of categories 2) Open-source'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 9}, page_content='Future directions. 1) Benchmarking chatbots at scale with a broader set of categories 2) Open-source\\nLLM judge aligned with human preference 3) Enhancing open models’ math/reasoning capability.\\n7 Conclusion\\nIn this paper, we propose LLM-as-a-judge for chatbot evaluation and systematically examine its\\nefficacy using human preference data from 58 experts on MT-bench, as well as thousands of crowd-\\nusers on Chatbot Arena. Our results reveal that strong LLMs can achieve an agreement rate of over'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 9}, page_content='users on Chatbot Arena. Our results reveal that strong LLMs can achieve an agreement rate of over\\n80%, on par with the level of agreement among human experts, establishing a foundation for an\\nLLM-based evaluation framework.\\nAcknowledgement\\nThis project is partly supported by gifts from Anyscale, Astronomer, Google, IBM, Intel, Lacework,\\nMicrosoft, MBZUAI, Samsung SDS, Uber, and VMware. Lianmin Zheng is supported by a Meta'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 9}, page_content='Microsoft, MBZUAI, Samsung SDS, Uber, and VMware. Lianmin Zheng is supported by a Meta\\nPh.D. Fellowship. We extend our thanks to Xinyang Geng, Hao Liu, Eric Wallace, Xuecheng Li,\\nTianyi Zhang, Qirong Ho, and Kevin Lin for their insightful discussions.\\nReferences\\n[1]Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\\narXiv preprint arXiv:2305.10403 , 2023.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 9}, page_content='Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\\narXiv preprint arXiv:2305.10403 , 2023.\\n[2]Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless\\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 ,\\n2022.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 9}, page_content='assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 ,\\n2022.\\n[3]Niels J Blunch. Position bias in multiple-choice questions. Journal of Marketing Research ,\\n21(2):216–220, 1984.\\n[4]Jonathon D Brown. Evaluations of self and others: Self-enhancement biases in social judgments.\\nSocial cognition , 4(4):353–376, 1986.\\n[5]Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 9}, page_content='Social cognition , 4(4):353–376, 1986.\\n[5]Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023.\\n[6]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 9}, page_content='Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\\nlanguage models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\\n[7]Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human\\nevaluations? arXiv preprint arXiv:2305.01937 , 2023.\\n[8]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 9}, page_content='Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\\n[9]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\\nchallenge. arXiv preprint arXiv:1803.05457 , 2018.\\n[10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 9}, page_content='challenge. arXiv preprint arXiv:1803.05457 , 2018.\\n[10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\\nsolve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\\n10'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 10}, page_content='[11] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and\\nmemory-efficient exact attention with io-awareness. Advances in Neural Information Processing\\nSystems , 35:16344–16359, 2022.\\n[12] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient\\nfinetuning of quantized llms. arXiv preprint arXiv:2305.14314 , 2023.\\n[13] Shizhe Diao, Rui Pan, Hanze Dong, Ka Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 10}, page_content='[13] Shizhe Diao, Rui Pan, Hanze Dong, Ka Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang.\\nLmflow: An extensible toolkit for finetuning and inference of large foundation models. arXiv\\npreprint arXiv:2306.12420 , 2023.\\n[14] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\\nGuestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for\\nmethods that learn from human feedback. arXiv preprint arXiv:2305.14387 , 2023.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 10}, page_content='methods that learn from human feedback. arXiv preprint arXiv:2305.14387 , 2023.\\n[15] Jiazhan Feng, Qingfeng Sun, Can Xu, Pu Zhao, Yaming Yang, Chongyang Tao, Dongyan Zhao,\\nand Qingwei Lin. Mmdialog: A large-scale multi-turn dialogue dataset towards multi-modal\\nopen-domain conversation. arXiv preprint arXiv:2211.05719 , 2022.\\n[16] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 10}, page_content='[16] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and\\nDawn Song. Koala: A dialogue model for academic research. Blog post, April 2023.\\n[17] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt outperforms crowd-workers for\\ntext-annotation tasks. arXiv preprint arXiv:2303.15056 , 2023.\\n[18] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 10}, page_content='[18] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey\\nLevine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint\\narXiv:2305.15717 , 2023.\\n[19] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\\narXiv:2009.03300 , 2020.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 10}, page_content='Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\\narXiv:2009.03300 , 2020.\\n[20] Fan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential\\nand limitations of chatgpt in explaining implicit hate speech. arXiv preprint arXiv:2302.07736 ,\\n2023.\\n[21] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu,\\nBertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 10}, page_content='Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking\\nbenchmarking in nlp. In Proceedings of the 2021 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies , pages\\n4110–4124, 2021.\\n[22] Miyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, and Jaewoo Kang. Look at the first\\nsentence: Position bias in question answering. arXiv preprint arXiv:2004.14602 , 2020.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 10}, page_content='sentence: Position bias in question answering. arXiv preprint arXiv:2004.14602 , 2020.\\n[23] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith\\nStevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. Ope-\\nnassistant conversations–democratizing large language model alignment. arXiv preprint\\narXiv:2304.07327 , 2023.\\n[24] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 10}, page_content='arXiv:2304.07327 , 2023.\\n[24] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,\\nYian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of\\nlanguage models. arXiv preprint arXiv:2211.09110 , 2022.\\n[25] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\\nbranches out , pages 74–81, 2004.\\n[26] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 10}, page_content='branches out , pages 74–81, 2004.\\n[26] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic\\nhuman falsehoods. arXiv preprint arXiv:2109.07958 , 2021.\\n[27] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou,\\nQuoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for\\neffective instruction tuning. arXiv preprint arXiv:2301.13688 , 2023.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 10}, page_content='effective instruction tuning. arXiv preprint arXiv:2301.13688 , 2023.\\n[28] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task general-\\nization via natural language crowdsourcing instructions. In ACL, 2022.\\n[29] OpenAI. Evals is a framework for evaluating llms and llm systems, and an open-source registry\\nof benchmarks. https://github.com/openai/evals .\\n[30] OpenAI. Gpt-4 technical report, 2023.\\n11'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 11}, page_content='[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\\nfollow instructions with human feedback. Advances in Neural Information Processing Systems ,\\n35:27730–27744, 2022.\\n[32] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 11}, page_content='evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\\nfor Computational Linguistics , pages 311–318, 2002.\\n[33] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\\nwith gpt-4. arXiv preprint arXiv:2304.03277 , 2023.\\n[34] Priya Raghubir and Ana Valenzuela. Center-of-inattention: Position biases in decision-making.\\nOrganizational Behavior and Human Decision Processes , 99(1):66–80, 2006.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 11}, page_content='Organizational Behavior and Human Decision Processes , 99(1):66–80, 2006.\\n[35] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question\\nanswering challenge. Transactions of the Association for Computational Linguistics , 7:249–266,\\n2019.\\n[36] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\\nadversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106,\\n2021.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 11}, page_content='adversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106,\\n2021.\\n[37] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\\nAdam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.\\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models.\\narXiv preprint arXiv:2206.04615 , 2022.\\n[38] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 11}, page_content='arXiv preprint arXiv:2206.04615 , 2022.\\n[38] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\\nhttps://github.com/tatsu-lab/stanford_alpaca , 2023.\\n[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 11}, page_content='thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\\nand efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\\n[40] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\\nZhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926 ,\\n2023.\\n[41] Xuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc Najork.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 11}, page_content='2023.\\n[41] Xuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc Najork.\\nPosition bias estimation for unbiased learning to rank in personal search. In Proceedings of\\nthe Eleventh ACM International Conference on Web Search and Data Mining , pages 610–618,\\n2018.\\n[42] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya\\nJiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. Pandalm: An'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 11}, page_content='Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. Pandalm: An\\nautomatic evaluation benchmark for llm instruction tuning optimization, 2023.\\n[43] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi\\nChandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels\\ngo? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751 ,\\n2023.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 11}, page_content='go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751 ,\\n2023.\\n[44] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi,\\nand Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc-\\ntions, 2022.\\n[45] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\\nAnjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 11}, page_content='Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al.\\nSuper-naturalinstructions:generalization via declarative instructions on 1600+ tasks. In EMNLP ,\\n2022.\\n[46] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv\\npreprint arXiv:2109.01652 , 2021.\\n[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 11}, page_content='preprint arXiv:2109.01652 , 2021.\\n[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\\narXiv:2201.11903 , 2022.\\n12'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 12}, page_content='[48] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and\\nDaxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.\\narXiv preprint arXiv:2304.12244 , 2023.\\n[49] Zongheng Yang, Zhanghao Wu, Michael Luo, Wei-Lin Chiang, Romil Bhardwaj, Woosuk Kwon,\\nSiyuan Zhuang, Frank Sifei Luan, Gautam Mittal, Scott Shenker, and Ion Stoica. SkyPilot:\\nAn intercloud broker for sky computing. In 20th USENIX Symposium on Networked Systems'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 12}, page_content='An intercloud broker for sky computing. In 20th USENIX Symposium on Networked Systems\\nDesign and Implementation (NSDI 23) , pages 437–455, Boston, MA, April 2023. USENIX\\nAssociation.\\n[50] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\\n[51] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 12}, page_content='[51] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\\nmodels. arXiv preprint arXiv:2304.06364 , 2023.\\n[52] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia\\nEfrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 ,\\n2023.\\n13'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 13}, page_content='A Prompt templates\\nWe list the prompt templates for LLM judges. Please refer to our github repository3for full details.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 13}, page_content='[System]Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user’s instructions and answers the user’s question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 13}, page_content='creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 13}, page_content='certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.[User Question]{question}[The Start of Assistant A’s Answer]{answer_a}[The End of Assistant A’s Answer][The Start of Assistant B’s Answer]{answer_b}[The End of Assistant B’s Answer]'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 13}, page_content='Figure 5: The default prompt for pairwise comparison.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 13}, page_content='[System]Please act as an impartial judge and evaluate the quality of the response provided by an AIassistant to the user question displayed below. Your evaluation should consider factors such asthe helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Beginyour evaluation by providing a short explanation. Be as objective as possible. After providing yourexplanation, please rate the response on a scale of 1 to 10 by strictly following this format:'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 13}, page_content='Be as objective as possible. After providing yourexplanation, please rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".[Question]{question}[The Start of Assistant’s Answer]{answer}[The End of Assistant’s Answer]'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 13}, page_content='Figure 6: The default prompt for single answer grading.\\n3https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge\\n14'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 14}, page_content='[System]Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given assistant A’s answer, and assistant B’s answer. Your job is to evaluate which assistant’s answer is better. You should independently solve the user question step-by-step first. Then compare both assistants’ answers with your answer. Identify and correct any mistakes.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 14}, page_content='independently solve the user question step-by-step first. Then compare both assistants’ answers with your answer. Identify and correct any mistakes. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do notfavor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 14}, page_content='certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.[User Question]{question}[The Start of Assistant A’s Answer]{answer_a}[The End of Assistant A’s Answer][The Start of Assistant B’s Answer]{answer_b}[The End of Assistant B’s Answer]Figure 7: The chain-of-thought prompt for math and reasoning questions.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 14}, page_content='[System]Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer, assistant A’s answer, and assistant B’s answer. Your job is to evaluate which assistant’s answer is better. Begin your evaluation by comparing both assistants’ answers with the reference answer. Identify and correct any mistakes. Avoid any position'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 14}, page_content='better. Begin your evaluation by comparing both assistants’ answers with the reference answer. Identify and correct any mistakes. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format:'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 14}, page_content='names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.[User Question]{question}[The Start of Reference Answer]{answer_ref}[The End of Reference Answer][The Start of Assistant A’s Answer]{answer_a}[The End of Assistant A’s Answer][The Start of Assistant B’s Answer]{answer_b}[The End of Assistant B’s Answer]'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 14}, page_content='Figure 8: The prompt for reference-guided pairwise comparison.\\n15'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 15}, page_content='[System]Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user’s instructions and answers the user’s question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 15}, page_content='creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 15}, page_content='certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.<|The Start of Assistant A’s Conversation with User|>### User:{question 1}### Assistant A:{answer 1}### User:{question 2}### Assistant A:{answer 2}<|The End of Assistant A’s Conversation with User|><|The Start of Assistant B’s Conversation with'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 15}, page_content='1}### User:{question 2}### Assistant A:{answer 2}<|The End of Assistant A’s Conversation with User|><|The Start of Assistant B’s Conversation with User|>### User:{question 1}### Assistant B:{answer 1}### User:{question 2}### Assistant B:{answer 2}<|The End of Assistant B’s Conversation with User|>Figure 9: The prompt for multi-turn pairwise comparison.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 15}, page_content='16'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 16}, page_content=\"[System]Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant's answer. You evaluation should focus on the assistant's answer to the second question. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing\"), Document(metadata={'source': '2306.05685v4.pdf', 'page': 16}, page_content='by comparing the assistant\\'s answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".<|The Start of Reference Answer|>### User:{question_1}### Reference answer:{ref_answer_1}### User:{question_2}### Reference answer:{ref_answer_2}<|The End of Reference Answer|><|The Start of Assistant A\\'s'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 16}, page_content=\"Reference answer:{ref_answer_1}### User:{question_2}### Reference answer:{ref_answer_2}<|The End of Reference Answer|><|The Start of Assistant A's Conversation with User|>### User:{question_1} ### Assistant A:{answer_1}### User:{question_2}### Assistant A:{answer_2}<|The End of Assistant A's Conversation with User|>Figure 10: The prompt for reference-guided multi-turn single-answer grading.\"), Document(metadata={'source': '2306.05685v4.pdf', 'page': 16}, page_content='17'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='B Case Study\\nWe list several case studies. The examples are generated by gpt-4-0314 . They may not be fully\\nreproducible with future GPT-4 versions.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='Question:What are some business etiquette norms when doing business in Japan?Assistant B:When doing business in Japan, it is important to be aware of and respect the country’s business etiquette norms. Some of these norms include:1.Greetings: A handshake and bow is the standard greeting in Japan. It is important to bow deeply and to offer a firm handshake.2.Business cards: In Japan, business cards are exchanged after the initial greeting. It is important to handle the card with respect and to'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='handshake.2.Business cards: In Japan, business cards are exchanged after the initial greeting. It is important to handle the card with respect and to read it carefully before placing it on the table.3.Meetings: Meetings in Japan are typically formal and conducted in a hierarchical manner. It is important to address people by their titles and to avoid direct eye contact.4.Punctuality: Punctuality is highly valued in Japan. It is important to arrive on time for meetings and to be prepared in'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='avoid direct eye contact.4.Punctuality: Punctuality is highly valued in Japan. It is important to arrive on time for meetings and to be prepared in advance.5.Gift giving: Gift giving is a common practice in Japan, but it is important to be mindful of the occasion and the relationship. Avoid giving gifts that are considered too personal or extravagant.6.Dining: Dining is an important part of doing business in Japan. It is important to be aware of the proper table manners and to avoid slurping or'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='Dining is an important part of doing business in Japan. It is important to be aware of the proper table manners and to avoid slurping or making loud noises while eating.7.Communication: Communication in Japan is typically indirect and reserved. It is important to listen carefully and to avoid confrontational language.Overall, it is important to be respectful, punctual, and mindful of hierarchy when doing business in Japan.Assistant A:Here are some business etiquette norms to keep in mind when'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='respectful, punctual, and mindful of hierarchy when doing business in Japan.Assistant A:Here are some business etiquette norms to keep in mind when doing business in Japan:1.Bowing: Bowing is a common greeting in Japan. It is a sign of respect and should be reciprocated.2.Business cards: Business cards are exchanged at the beginning of a meeting. It is important to receive and give business cards with both hands and to take a moment to read the card before putting it away.3.Punctuality: Being'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='It is important to receive and give business cards with both hands and to take a moment to read the card before putting it away.3.Punctuality: Being on time is important in Japan. Arrive a few minutes early to show respect for your colleagues.4.Dress code: Dress conservatively and professionally. Men should wear dark suits and ties, while women should wear conservative business attire.5.Language: Learn a few basic Japanese phrases to show respect for the culture. It is also important to speak'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='wear conservative business attire.5.Language: Learn a few basic Japanese phrases to show respect for the culture. It is also important to speak clearly and avoid using slang or idioms.6.Gift giving: Gift giving is a common practice in Japan. Bring a small gift, such as a box of chocolates or a small souvenir from your home country.7.Dining etiquette: When dining with colleagues, wait for the host to begin eating before you start. Use chopsticks correctly and avoid leaving food on your'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='etiquette: When dining with colleagues, wait for the host to begin eating before you start. Use chopsticks correctly and avoid leaving food on your plate.Remember, showing respect for Japanese culture and customs is key to building strong business relationships in Japan.GPT-4 Judgment (when A is placed in the first position):Both Assistant A and Assistant B provide helpful and relevant information about business etiquette norms in Japan. However, Assistant A’s answer is more detailed and'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='and Assistant B provide helpful and relevant information about business etiquette norms in Japan. However, Assistant A’s answer is more detailed and organized, covering a wider range of topics such as dress code and language, which are not mentioned in Assistant B’s response. Assistant A also provides more specific advice on how to handle certain situations, such as exchanging business cards and dining etiquette. Therefore, Assistant A’s answer is more comprehensive and useful for someone'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='situations, such as exchanging business cards and dining etiquette. Therefore, Assistant A’s answer is more comprehensive and useful for someone looking to understand Japanese business etiquette.GPT-4 Judgment (when B is placed in the first position):Both Assistant A and Assistant B provide helpful and relevant information about business etiquette norms in Japan. However, Assistant B’s answer is more detailed and covers a wider range of topics, such as addressing people by their titles,'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='norms in Japan. However, Assistant B’s answer is more detailed and covers a wider range of topics, such as addressing people by their titles, avoiding direct eye contact, and being mindful of communication styles. Assistant A’s answer is more concise but lacks some of the depth found in Assistant B’s response. Therefore, Assistant B’s answer is better in this case.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 17}, page_content='Figure 11: An example of position bias. When Assistant A is placed in the first position, GPT-4\\nthinks A is better, but its verdict changes when we swap the position of A and B. We observe similar\\npattern from other LLM judges such as Claude/GPT-3.5.\\n18'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 18}, page_content='Question:What is the central dogma of molecular biology? What processes are involved? Who named this?Assistant B:The central dogma of molecular biology is the fundamental principle that explains how genetic information flows within a biological system. It states that DNA is transcribed into RNA, which is then translated into proteins. This means that the genetic information stored in DNA is first copied into RNA, which then serves as a template for the synthesis of proteins.The processes'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 18}, page_content='that the genetic information stored in DNA is first copied into RNA, which then serves as a template for the synthesis of proteins.The processes involved in the central dogma are:1. Transcription: The process by which the genetic information in DNA is copied into RNA.2. Translation: The process by which the genetic information in RNA is used to synthesize proteins.The term \"central dogma\" was coined by Francis Crick, one of the co-discoverers of the structure of DNA, in 1958.Assistant A:The'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 18}, page_content='synthesize proteins.The term \"central dogma\" was coined by Francis Crick, one of the co-discoverers of the structure of DNA, in 1958.Assistant A:The central dogma of molecular biology is the fundamental principle that explains how genetic information flows within a biological system. It states that DNA is transcribed into RNA, which is then translated into proteins. This means that the genetic information stored in DNA is first copied into RNA, which then serves as a template for the synthesis'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 18}, page_content='into proteins. This means that the genetic information stored in DNA is first copied into RNA, which then serves as a template for the synthesis of proteins.The processes involved in the central dogma are:1. Transcription rephrased: The act of copying genetic information from DNA to RNA.2. Translation rephrased: The act of using genetic information in RNA to create proteins.3. Transcription: The process by which the genetic information in DNA is copied into RNA.4. Translation: The process by'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 18}, page_content='in RNA to create proteins.3. Transcription: The process by which the genetic information in DNA is copied into RNA.4. Translation: The process by which the genetic information in RNA is used to synthesize proteins.The term \"central dogma\" was coined by Francis Crick, one of the co-discoverers of the structure of DNA, in 1958.GPT-3.5 Judgment:Assistant A provides a more detailed and accurate response to the user\\'s question. While both assistants correctly state the central dogma of molecular'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 18}, page_content=\"A provides a more detailed and accurate response to the user's question. While both assistants correctly state the central dogma of molecular biology and who coined the term, Assistant A provides a more comprehensive explanation of the processes involved in the central dogma, including the rephrased definitions of transcription and translation. Therefore, I choose A as the better assistant.Claude-v1 Judgment:Both assistants answer the question by defining the central dogma and detailing the two\"), Document(metadata={'source': '2306.05685v4.pdf', 'page': 18}, page_content='I choose A as the better assistant.Claude-v1 Judgment:Both assistants answer the question by defining the central dogma and detailing the two processes involved, transcription and translation. However, assistant A provides a more in-depth and detailed explanation by briefly rephrasing and elaborating on these processes. Assistant A also mentions who coined the term \"central dogma\", which provides useful context and shows a more comprehensive understanding of the topic. Overall, assistant A'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 18}, page_content='who coined the term \"central dogma\", which provides useful context and shows a more comprehensive understanding of the topic. Overall, assistant A gives a fuller and more insightful response, so I have chosen it as the better assistant for this question.GPT-4 Judgment:Both Assistant A and Assistant B provide accurate and relevant information about the central dogma of molecular biology, the processes involved, and who named it. However, Assistant A\\'s answer contains a minor redundancy by'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 18}, page_content=\"about the central dogma of molecular biology, the processes involved, and who named it. However, Assistant A's answer contains a minor redundancy by rephrasing transcription and translation twice. Assistant B's answer is more concise and avoids this redundancy. Therefore, Assistant B's answer is better.Figure 12: An example of “repetitive list” attack to examine verbosity bias. Except for the two\"), Document(metadata={'source': '2306.05685v4.pdf', 'page': 18}, page_content='rephrased items (highlighted in red), Assistant A’s answer is exactly the same as Assistant B. Both\\nGPT-3.5 and Claude-v1 show a verbosity bias towards the longer and repetitive answer. Only GPT-4\\nsuccessfully detected this attack.\\n19'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 19}, page_content='Question:Benjamin went to a bookstore and purchased a variety of books. He bought 5 copies of a sci-fi novel, each priced at $20, 3 copies of a history book priced at $30 each, and 2 copies of a philosophy book for $45 each. What was the total cost of his purchases?Assistant B:The total cost of his purchases is $295.Assistant A:The total cost of Benjamin’s purchases was $115.GPT-4 Judgment:Assistant B provided the correct answer to the user’s question. Assistant A’s response was incorrect.To'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 19}, page_content='Benjamin’s purchases was $115.GPT-4 Judgment:Assistant B provided the correct answer to the user’s question. Assistant A’s response was incorrect.To calculate the total cost of Benjamin’s purchases, we need to multiply the number of books by theirrespective prices and then add the results:5 sci-fi novels * $20 = $1003 history books * $30 = $902 philosophy books * $45 = $90Total cost = $100 + $90 + $90 = $295Assistant B’s response is accurate and directly answers the user’s question. Therefore,'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 19}, page_content='books * $45 = $90Total cost = $100 + $90 + $90 = $295Assistant B’s response is accurate and directly answers the user’s question. Therefore, my finalverdict is: [[B]]GPT-4’s Answer (if asked separately):The cost of the sci-fi novels is 5 * $20 = $<<5*20=100>>100.The cost of the history books is 3 * $30 = $<<3*30=90>>90.The cost of the philosophy books is 2 * $45 = $<<2*45=90>>90.The total cost of his purchases is $100 + $90 + $90 = $<<100+90+90=280>>280. Answer: \\\\boxed{280}.Figure 13: With a'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 19}, page_content='books is 2 * $45 = $<<2*45=90>>90.The total cost of his purchases is $100 + $90 + $90 = $<<100+90+90=280>>280. Answer: \\\\boxed{280}.Figure 13: With a default prompt, GPT-4 shows limited capability in grading math questions. Despite'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 19}, page_content='being able to answer the question itself, its judgment was influenced by the given answers, leading to\\narithmetic mistakes highlighted in yellow.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 19}, page_content='Question:Each problem consists of three statements. Based on the first two statements, the third statement may be true, false,or uncertain.1. Oranges cost more than apples.2. Oranges cost less than bananas.3. Bananas cost more than apples and bananas cost more than orange.If the first two statements are true, then the third statement isAssistant B:False.If the first two statements are true, then the thirdstatement is false. The first statement says that orangescost more than apples, and the'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 19}, page_content='B:False.If the first two statements are true, then the thirdstatement is false. The first statement says that orangescost more than apples, and the second statement says thatoranges cost less than bananas. This means that orangescost less than bananas, which contradicts the thirdstatement that bananas cost more than apples and morethan oranges. Therefore, the third statement must be false.Assistant A:false.If oranges cost more than apples (statement 1) andoranges cost less than bananas'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 19}, page_content='Therefore, the third statement must be false.Assistant A:false.If oranges cost more than apples (statement 1) andoranges cost less than bananas (statement 2), then itis not possible for bananas to cost less than orangesand alsocost more than both apples and oranges(statement 3). Therefore, statement 3 is false.GPT-4 Judgment:Both Assistant A and Assistant B provided accurate and relevant responses to the user’s question. They bothconcluded that the third statement is false based on the first'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 19}, page_content='Assistant B provided accurate and relevant responses to the user’s question. They bothconcluded that the third statement is false based on the first two statements. However, Assistant B’s response isslightly clearer and more concise in explaining the contradiction between the statements. Therefore, I chooseAssistant B as the better response.[[B]]GPT-4’s Answer (if asked separately):True.If oranges cost more than apples (statement 1) and oranges cost less than bananas (statement 2), it logically'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 19}, page_content='Answer (if asked separately):True.If oranges cost more than apples (statement 1) and oranges cost less than bananas (statement 2), it logically follows that bananas cost more than apples and bananas cost more than oranges, which is the third statement. So, if the first two statements are true, the third one is also true.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 19}, page_content='Figure 14: An example of GPT-4’s limited capability in grading reasoning question. Despite GPT-4\\nknows how to solve the question (if asked separately), it made a wrong judgement saying both\\nassistants’ wrong answers are correct.\\n20'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 20}, page_content='Question:How many integers are in the solution of the inequality |x + 5| < 10?Assistant B:To solve the inequality |x + 5| < 10, we need to consider two cases:Case 1: x + 5 is positive or zeroIn this case, the inequality becomes x + 5 < 10, which simplifies to x < 5.Case 2: x + 5 is negativeIn this case, the inequality becomes -(x + 5) < 10, which simplifies to -x -5 < 10. Adding 5 to both sides gives -x < 15, andmultiplying by -1 (and flipping the inequality) gives x > -15.Sothe solution to the'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 20}, page_content='to -x -5 < 10. Adding 5 to both sides gives -x < 15, andmultiplying by -1 (and flipping the inequality) gives x > -15.Sothe solution to the inequality is -15 < x < 5. The integers in this range are -14, -13, -12, ..., 4, for a total of 20 integers. Therefore, there are 20 integers in the solution of the inequality.Assistant A:There are 19 integers in the solution of the inequality |x + 5| < 10.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 20}, page_content='GPT-4 Judgment:To solve the inequality |x + 5| < 10, we need to consider two cases:Case 1: x + 5 is positive or zeroIn this case, the inequality becomes x + 5 < 10, which simplifies to x < 5.Case 2: x + 5 is negativeIn this case, the inequality becomes -(x + 5) < 10, which simplifies to -x -5 < 10. Adding 5 to both sides gives -x < 15, andmultiplying by -1 (and flipping the inequality) gives x > -15.Sothe solution to the inequality is -15 < x < 5. The integers in this range are -14, -13, -12,'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 20}, page_content='by -1 (and flipping the inequality) gives x > -15.Sothe solution to the inequality is -15 < x < 5. The integers in this range are -14, -13, -12, ..., 4, for a total of 20 integers. Therefore, there are 20 integers in the solution of the inequality.Comparing the answers provided by the two AI assistants:Assistant A: 19 integersAssistant B: 20 integersThe correct answer is 20 integers, so Assistant B provided the correct and more helpful response.Final verdict: [[B]]GPT-4’s Answer (if asked'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 20}, page_content='integersThe correct answer is 20 integers, so Assistant B provided the correct and more helpful response.Final verdict: [[B]]GPT-4’s Answer (if asked separatedly):We can split the inequality into two cases:1) If $x+5\\\\ge0$, then $|x+5|=x+5$. So, we have $x+5<10$, which means $x<5$. Since $x+5\\\\ge0$, we have $x\\\\ge-5$. Soin this case, $x$ can be any integer from $-5$ to $4$, inclusive, which gives us $5-(-5)+1=10$ integers.2) If $x+5<0$, then $|x+5|=-(x+5)$. So, we have $-(x+5)<10$, which means'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 20}, page_content='integer from $-5$ to $4$, inclusive, which gives us $5-(-5)+1=10$ integers.2) If $x+5<0$, then $|x+5|=-(x+5)$. So, we have $-(x+5)<10$, which means $x+5>-10$, which means $x>-15$. Since $x+5<0$, we have $x<-5$. Soin this case, $x$ can be any integer from $-14$ to $-6$, inclusive, which gives us $-6-(-14)+1=9$ integers.In total, there are $10+9=\\\\boxed{19}$ integers in the solution.Figure 15: An example of GPT-4’s wrong judgment with chain-of-thought prompt. We can see GPT-4'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 20}, page_content='exactly copied Assistant B’s answer (which contains arithmetic errors) and determined Assistant A’s\\nanswer is incorrect. This suggest GPT-4’s chain-of-thought process can be significantly influenced by\\nthe given answers despite we ask it to think independently.\\n21'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 21}, page_content='Question:Share ideas for adapting art masterpieces into interactive experiences for children. List 5 specific artworks and associated ideas.Assistant B:(omitted)Assistant A:Sure, here are five ideas for adapting art masterpieces into interactive experiences for children:1. “The Starry Night” by Vincent van Gogh -Create a virtual reality experience where children can explore the night sky and see the stars and constellations as van Gogh saw them. They can also create their own starry night scenes'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 21}, page_content='children can explore the night sky and see the stars and constellations as van Gogh saw them. They can also create their own starry night scenes using different colors and brush strokes.2. “The Persistence of Memory” by Salvador Dali -Create a sensory experience where children can touch, feel, and smell different objects to explore the surrealist themes of Dali’s painting. They can also create their own surreal landscapes using different materials and textures.3. ...(omitted)Follow-up Question:'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 21}, page_content='of Dali’s painting. They can also create their own surreal landscapes using different materials and textures.3. ...(omitted)Follow-up Question: Write a concrete plan for your second example.Include budget estimates.Assistant B:(omitted)Assistant A:Sure, here is a concrete plan for adapting “The Persistence of Memory” by Salvador Dali into an interactive experience for children:1. Budget: $5,0002. Objectives:-To create a sensory experience that allows children to explore the surrealist themes of'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 21}, page_content='experience for children:1. Budget: $5,0002. Objectives:-To create a sensory experience that allows children to explore the surrealist themes of Dali’s painting.-To encourage children to create their own surreal landscapes using different materials and textures....GPT-4 Judgment:Assistant A provided a concrete plan for adapting “The Persistence of Memory” by Salvador Dali into an interactive experience for children, while Assistant B provided a plan for the “Mona Lisa Interactive Experience for'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 21}, page_content='Memory” by Salvador Dali into an interactive experience for children, while Assistant B provided a plan for the “Mona Lisa Interactive Experience for Children.” The user’s follow-up question specifically asked for a concrete plan for the second example, which was the “Mona Lisa” experience provided by Assistant B.Therefore, Assistant B’s answer is more relevant and directly addresses the user’s request.My final verdict is: [[B]]Figure 16: In this example, despite Assistant A correctly followed'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 21}, page_content='more relevant and directly addresses the user’s request.My final verdict is: [[B]]Figure 16: In this example, despite Assistant A correctly followed user’s instruction to generate'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 21}, page_content='a concrete plan for the second example of its previous response, GPT-4 wrongly referred to the\\nsecond example in Assistant B’s response, resulting in a wrong judgment. This suggests the prompt\\ndesign that breaks the questions into two prompts may cause LLM judge struggle to locate assistants’\\nprevious responses.\\n22'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 22}, page_content='C Data Collection\\nWe describe our data collection process for both MT-bench and Chatbot Arena.\\nC.1 MT-bench human evaluation\\nFigure 17 shows the normal voting interface. Figure 18 shows that we additionally show GPT-4’s\\njudgment to users and ask if it is reasonable when a human differs from GPT-4.\\nFigure 17: The screenshot of MT-bench data collection. We show an instruction similar to the prompt\\nwe give to GPT-4. We present questions from MT-bench and answers from two random anonymous'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 22}, page_content='we give to GPT-4. We present questions from MT-bench and answers from two random anonymous\\nassistants and ask which one is better. We present the first-turn conversation and ask humans to vote,\\nthen repeat the same procedure for the second-turn. A user can skip up to 5 questions if they are not\\nconfident. For some questions (e.g., math, reasoning), they can also see a reference solution.\\nFigure 18: The screenshot of MT-bench data collection. When human’s vote differs from GPT-4, we'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 22}, page_content='Figure 18: The screenshot of MT-bench data collection. When human’s vote differs from GPT-4, we\\nadditionally show GPT-4’s judgment (red region in the screenshot) and ask the user to click one of\\nthe three buttons to decide whether GPT-4’s judgment is reasonable.\\n23'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 23}, page_content='To invite participants, we obtained their consent by letting them sign an application form. We\\npay them $20 for judging 20 questions, which corresponds to an hourly rate of around $35. The\\nparticipants are mostly graduate students from more than ten universities.\\nC.2 Chatbot Arena\\nFigure 19 shows a screenshot of Chatbot Arena. Users are required to accept the terms of use,\\nwhich obtain their consent and give us the right to release the conversation data. The instructions'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 23}, page_content='which obtain their consent and give us the right to release the conversation data. The instructions\\nare shown at the top of the interface. This is a free website. We do not pay users and any user\\ncan use this platform without registration. More introductions and analyses can be found at https:\\n//lmsys.org/blog/2023-05-03-arena/ .\\nFigure 19: The screenshot of Chatbot Arena.\\nC.3 Data Release\\nWe will clean the Personal Identifiable Information (PII) and tag toxic conversations with OpenAI'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 23}, page_content='C.3 Data Release\\nWe will clean the Personal Identifiable Information (PII) and tag toxic conversations with OpenAI\\nmoderation APIs for our dataset release.\\n24'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 24}, page_content='D Additional Experimental Results\\nWe present some additional experimental results.\\nD.1 Position bias\\nWe test two more prompts and present the full results in Table 9 “score” changes the default prompt\\nto let the model output two absolute scores instead of which one is better. “short” is a simplified\\nversion of our default prompt by removing instructions like “Avoid any position bias..”, “Begin your'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 24}, page_content='version of our default prompt by removing instructions like “Avoid any position bias..”, “Begin your\\nevaluation ... and provide a short explanation”. We can find different prompts have different effects\\non different models. For example, the \"score\" prompt can increase the consistency of GPT-3.5 but\\ndecreases it for Claude-v1 and GPT-4.\\nTable 9: Position bias on different models and prompts. Consistency is the percentage of cases where'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 24}, page_content='decreases it for Claude-v1 and GPT-4.\\nTable 9: Position bias on different models and prompts. Consistency is the percentage of cases where\\na judge gives consistent results when swapping the order of two assistants. “Biased toward first” is\\nthe percentage of cases when a judge favors the first answer. “Error” indicates wrong output formats.\\nThe two largest numbers in each column are in bold.\\nJudge Prompt Consistency Biased toward first Biased toward second Error'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 24}, page_content='The two largest numbers in each column are in bold.\\nJudge Prompt Consistency Biased toward first Biased toward second Error\\nclaude-v1default 23.8% 75.0% 0.0% 1.2%\\nrename 56.2% 11.2% 28.7% 3.8%\\nscore 20.0% 80.0% 0.0% 0.0%\\nshort 22.5% 75.0% 2.5% 0.0%\\ngpt-3.5-turbodefault 46.2% 50.0% 1.2% 2.5%\\nrename 51.2% 38.8% 6.2% 3.8%\\nscore 55.0% 33.8% 11.2% 0.0%\\nshort 38.8% 57.5% 3.8% 0.0%\\ngpt-4default 65.0% 30.0% 5.0% 0.0%\\nrename 66.2% 28.7% 5.0% 0.0%\\nscore 51.2% 46.2% 2.5% 0.0%\\nshort 62.5% 35.0% 2.5% 0.0%'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 24}, page_content='short 38.8% 57.5% 3.8% 0.0%\\ngpt-4default 65.0% 30.0% 5.0% 0.0%\\nrename 66.2% 28.7% 5.0% 0.0%\\nscore 51.2% 46.2% 2.5% 0.0%\\nshort 62.5% 35.0% 2.5% 0.0%\\nAs shown in Table 10, position bias is more noticeable on open questions like writing and stem/hu-\\nmanity knowledge questions. On math and coding questions, LLM judges are more confident even\\nthough their judgments can often be wrong, as we show in Section 3.3. Finally, we study how the'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 24}, page_content='though their judgments can often be wrong, as we show in Section 3.3. Finally, we study how the\\nmodel pairs influence position bias by using GPT-4 and the default prompt to judge three different\\nmodel pairs. As shown in Table 11, the position bias is more noticeable for models with close\\nperformance and can almost disappear when the performance of the two models differs a lot.\\nTable 10: Position bias on different categories. The two largest numbers in each column are in bold.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 24}, page_content='Table 10: Position bias on different categories. The two largest numbers in each column are in bold.\\nCategory Consistent Biased toward first Biased toward second\\nwriting 42.0% 46.0% 12.0%\\nroleplay 68.0% 30.0% 2.0%\\nreasoning 76.0% 20.0% 4.0%\\nmath 86.0% 4.0% 10.0%\\ncoding 86.0% 14.0% 0.0%\\nextraction 78.0% 12.0% 10.0%\\nstem 44.0% 54.0% 2.0%\\nhumanities 36.0% 60.0% 4.0%\\nTable 11: Position bias on different model pairs.\\nPair Consistent Biased toward first Biased toward second'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 24}, page_content='humanities 36.0% 60.0% 4.0%\\nTable 11: Position bias on different model pairs.\\nPair Consistent Biased toward first Biased toward second\\nGPT-3.5 vs Claude-V1 67.5% 23.8% 8.8%\\nGPT-3.5 vs Vicuna-13B 73.8% 23.8% 2.5%\\nGPT-3.5 vs LLaMA-13B 98.8% 1.2% 0.0%\\n25'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 25}, page_content='D.2 Few-shot judge\\nWe examine how few-shot examples improve LLM judges. As shown in Table 12, they improve the\\nconsistency of all three LLM judges significantly. It almost alleviates the position bias of GPT-4, but\\nmoves the position bias of GPT-3.5 from the first position to the second position. We then measure\\nthe agreement between few-shot GPT-4 pairwise comparison and humans on MT-bench, but found it\\nperforms similarly to zero-shot GPT-4 pairwise comparison.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 25}, page_content='performs similarly to zero-shot GPT-4 pairwise comparison.\\nTable 12: Improvements of the few-shot judge on consistency for position bias.\\nModel Prompt Consistency Biased toward first Biased toward second Error\\nClaude-v1zero-shot 23.8% 75.0% 0.0% 1.2%\\nfew-shot 63.7% 21.2% 11.2% 3.8%\\nGPT-3.5zero-shot 46.2% 50.0% 1.2% 2.5%\\nfew-shot 55.0% 16.2% 28.7% 0.0%\\nGPT-4zero-shot 65.0% 30.0% 5.0% 0.0%\\nfew-shot 77.5% 10.0% 12.5% 0.0%\\nD.3 Agreement Evaluation'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 25}, page_content='few-shot 55.0% 16.2% 28.7% 0.0%\\nGPT-4zero-shot 65.0% 30.0% 5.0% 0.0%\\nfew-shot 77.5% 10.0% 12.5% 0.0%\\nD.3 Agreement Evaluation\\nAgreement calculation. We define the agreement between two types of judges as the probability\\nof randomly selected individuals (but not identical) of each type agreeing on a randomly selected\\nquestion. For example, if we are comparing GPT-4 and Claude, the agreement is the probability of'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 25}, page_content='question. For example, if we are comparing GPT-4 and Claude, the agreement is the probability of\\nGPT-4 and Claude agreeing on the vote for a randomly selected question. If we are comparing GPT-4\\nand humans, the agreement is the probability of GPT-4 and a randomly selected human agreeing\\non the vote for a randomly selected question. The agreement among humans themselves is the\\nprobability of two randomly selected but not identical humans agreeing on the vote for a randomly\\nselected question.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 25}, page_content='probability of two randomly selected but not identical humans agreeing on the vote for a randomly\\nselected question.\\nNote that the agreement among humans could be a lower estimation compared to the agreement of\\nGPT4 and humans. Consider three humans who voted “A”, “A”, and “B” for a question, respectively.\\nThe agreement among them is only1\\n3, as there are three pairs “(A, A)”, “(A, B)”, and “(A, B)”. But\\nthe agreement between GPT4 and those three is2\\n3if GPT4 voted “first” and1\\n3otherwise.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 25}, page_content='the agreement between GPT4 and those three is2\\n3if GPT4 voted “first” and1\\n3otherwise.\\nTherefore, to have a more comprehensive understanding of what happened, we introduce a new judge\\ntype called human-majority, which considers the majority of human votes for each question. The\\nagreement between GPT4 and human-majority is then calculated as the probability of GPT4 agreeing\\nwith the majority of human votes on a randomly selected question. The upper bound of the agreement'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 25}, page_content='with the majority of human votes on a randomly selected question. The upper bound of the agreement\\nbetween GPT-4 and humans is the agreement between human-majority and human. When there is no\\nmajority vote for a question, the agreement is counted by an even split. For example, if there are an\\nequal number of “A” and “B” human votes for a question, and GPT4 votes “A”, the agreement is\\ncounted as1\\n2on this question.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 25}, page_content='equal number of “A” and “B” human votes for a question, and GPT4 votes “A”, the agreement is\\ncounted as1\\n2on this question.\\nMore results. Table 13 shows more agreement results on MT-bench. In addition to expert labelers\\n(denoted as “Human”), we also include author votes (denoted as “Author”).\\nD.4 Category-wise scores with single-answer grading\\nWe use single-answer grading to evaluate 6 models on MT-bench and plot the category-wise scores in\\nFigure 20.\\n26'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 26}, page_content='Table 13: Agreement between two types of judges on MT-bench. “G4-P” and “G4-S” denote GPT-4\\nwith pairwise comparison and single-answer grading, respectively. “C” denotes Claude. “Human”\\ndenotes expert labelers (excluding authors). ‘Human-M” denotes the majority vote of humans.\\nThe single-answer grading can be converted into pairwise comparison results for calculating the\\nagreement. We report two setups: “S1” includes non-tie, tie, and inconsistent (due to position bias)'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 26}, page_content='agreement. We report two setups: “S1” includes non-tie, tie, and inconsistent (due to position bias)\\nvotes and counts inconsistent as a tie; “S2” only includes non-tie votes. The agreement between two\\nrandom judges under each setup is denoted as “R=”. The top value in each cell is the agreement, and\\nthe bottom gray value is #votes.\\nSetup S1 (R = 33%) S2 (R = 50%)\\nJudge G4-S C Author Human Human-M G4-S C Author Human Human-M\\nG4-P70%\\n113863%\\n119869%\\n34566%\\n134367%\\n82197%\\n66294%\\n58292%\\n20185%'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 26}, page_content='Judge G4-S C Author Human Human-M G4-S C Author Human Human-M\\nG4-P70%\\n113863%\\n119869%\\n34566%\\n134367%\\n82197%\\n66294%\\n58292%\\n20185%\\n85985%\\n546\\nG4-S -66%\\n113667%\\n32460%\\n128060%\\n781 -90%\\n56394%\\n17585%\\n73985%\\n473\\nC - -58%\\n34354%\\n134155%\\n820 - -89%\\n14185%\\n64886%\\n414\\nAuthor - -69%\\n4965%\\n42855%\\n93 - -87%\\n3183%\\n26276%\\n46\\nHuman - - -63%\\n72181%\\n892 - - -81%\\n47990%\\n631\\n(a) First Turn\\nSetup S1 (R = 33%) S2 (R = 50%)\\nJudge G4-S Author Human Human-M G4-S Author Human Human-M\\nG4-P70%\\n116166%\\n34166%\\n132568%'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 26}, page_content='47990%\\n631\\n(a) First Turn\\nSetup S1 (R = 33%) S2 (R = 50%)\\nJudge G4-S Author Human Human-M G4-S Author Human Human-M\\nG4-P70%\\n116166%\\n34166%\\n132568%\\n81295%\\n72788%\\n20585%\\n86485%\\n557\\nG4-S -65%\\n33159%\\n128561%\\n783 -89%\\n19384%\\n77685%\\n506\\nAuthor -67%\\n4968%\\n41363%\\n87 -87%\\n3186%\\n27384%\\n54\\nHuman - -67%\\n70783%\\n877 - -82%\\n47491%\\n629\\n(b) Second Turn\\nW riting\\nRoleplay\\nReasoning\\nMath\\nCodingExtractionSTEMHumanities\\n0 2 4 6 8 10model\\nGPT -4\\nClaude-v1\\nGPT -3.5-turbo\\nV icuna-13B\\nAlpaca-13B\\nLLaMA-13B'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 26}, page_content='W riting\\nRoleplay\\nReasoning\\nMath\\nCodingExtractionSTEMHumanities\\n0 2 4 6 8 10model\\nGPT -4\\nClaude-v1\\nGPT -3.5-turbo\\nV icuna-13B\\nAlpaca-13B\\nLLaMA-13B\\nFigure 20: Category-wise scores of 6 models on MT-bench.\\n27'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 27}, page_content='E Training Details of Vicuna Models\\nVicuna is created by fine-tuning a LLaMA base model using user-shared conversations gathered from\\nShareGPT.com with its public APIs. ShareGPT is a website where users can share their ChatGPT\\nconversations. To ensure data quality, we convert the HTML back to markdown and filter out some\\ninappropriate or low-quality samples, which results in 125K conversations after data cleaning.4We'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 27}, page_content='inappropriate or low-quality samples, which results in 125K conversations after data cleaning.4We\\nthen divide lengthy conversations into smaller segments that fit the model’s maximum context length.\\nWe construct three training datasets with different scales from this cleaned ShareGPT dataset. Their\\nstatistics are in Table 8, where we also compare it with Alpaca [ 38] dataset. “All” is the full dataset.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 27}, page_content='statistics are in Table 8, where we also compare it with Alpaca [ 38] dataset. “All” is the full dataset.\\n“Single” only includes the first turn of each conversation. “Selected” is a small high-quality dataset of\\n3K sequences. To construct the “Selected” dataset, we pick sequences that include at least 3 turns of\\nconversations generated by GPT-4 and run a clustering algorithm to divide them into 3K clusters and\\npick the centroid of each cluster.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 27}, page_content='conversations generated by GPT-4 and run a clustering algorithm to divide them into 3K clusters and\\npick the centroid of each cluster.\\nAll models (Vicuna-7B/13B) are trained with the same hyperparameters: global batch size=128,\\nlearning=2e-5, epochs=3, seq length=2048. Except for “Selected”, which we train for 5 epochs. The\\ntraining code is built on top of the Alpaca code but additionally handles multi-turn conversations. The'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 27}, page_content='training code is built on top of the Alpaca code but additionally handles multi-turn conversations. The\\ntraining is done with 8x A100 GPUs. The longest single training run takes around 2 days. We utilize\\nSkyPilot [ 49] managed spot instances for saving training costs and FlashAttention [ 11] for memory\\noptimizations. The training code is available at https://github.com/lm-sys/FastChat .\\nTable 14: Dataset statistics\\nDataset Name Alpaca Selected Single All\\n#Token 4.4M 4.8M 184M 370M'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 27}, page_content='Table 14: Dataset statistics\\nDataset Name Alpaca Selected Single All\\n#Token 4.4M 4.8M 184M 370M\\n#Sequence 52K 3K 257K 257K\\nAvg. turns of conversation 1.0 4.0 1.0 2.9\\nAvg. response length (token) 65 343 473 373\\n4In this study, we use more data (125K) than the version in our earlier blog post (70K).\\n28'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 28}, page_content='F Exploring Vicuna as a judge\\nIn this paper, we mostly evaluate the ability of close-sourced models such as GPT-4 as a proxy for\\nhuman evaluations. However, model services such as GPT-4 can also become expensive with a\\ngrowing number of evaluations. On the other hand, popular open-sourced LLMs, e.g. Vicuna-13B\\nshows strong language understanding capability, and are much cheaper than close-sourced LLMs. In'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 28}, page_content='shows strong language understanding capability, and are much cheaper than close-sourced LLMs. In\\nthis section, we further explore the potential of using Vicuna-13B as a more cost-friendly proxy.\\nF.1 Zero-Shot Vicuna\\nWhen using as-it-is (zero-shot), Vicuna-13B noticeably suffers from limitations we discuss, e.g.\\nposition bias. As shown in Table 15, Vicuna-13B has a consistency rate from 11.2% to 16.2%'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 28}, page_content='position bias. As shown in Table 15, Vicuna-13B has a consistency rate from 11.2% to 16.2%\\nacross different prompt templates, much lower than all the closed-sourced models. In addition, it\\nhas a high error rate (from 22.5% to 78.8%) because of its weaker instruction-following capability.\\nIn many scenarios, Vicuna-13B provides responses such as \"Answer A is better than answer B\",\\nwithout following the pre-defined template. These responses are rendered as natural languages and'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 28}, page_content='without following the pre-defined template. These responses are rendered as natural languages and\\nare difficult to be parsed automatically, making the model less useful in a scalable and automatic\\nevaluation pipeline.\\nF.2 Arena Fine-tuned Vicuna\\nTraining Due to the incapability of the zero-shot Vicuna-13B model, we further finetune the model\\nwith human votes from Chatbot Arena. Specifically, we randomly sample 22K single-turn votes'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 28}, page_content='with human votes from Chatbot Arena. Specifically, we randomly sample 22K single-turn votes\\nfrom the arena, covering all models supported by the time of this paper submission (GPT-4, GPT-3.5,\\nClaude-v1, Vicuna-13b, Vicuna-7b, Koala-13B, Alpaca-13B,LLaMA-13B, Dolly-12B, FastChat-T5,\\nRWKV-4-Raven, MPT-Chat, OpenAssistant, ChatGLM, and StableLM), to expose the model with\\na wider range of chatbot outputs and human preferences. We use 20K votes for training, and 2K'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 28}, page_content='a wider range of chatbot outputs and human preferences. We use 20K votes for training, and 2K\\nfor validation. To address the aforementioned weak instruction following problem, we formulate\\nthe problem as a 3-way sequence classification problem. Thus, the model simply needs to predict\\nwhich one of the chat-bot outputs is better (or tie), without needing to exactly following the provided\\nanswer template. In particular, we construct an input by using the default prompt and the two model'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 28}, page_content='answer template. In particular, we construct an input by using the default prompt and the two model\\nanswers. The labels are A, B, and tie (including both-bad-vote and tie-vote). We train for 3 epochs\\nwith a cosine learning rate scheduler and a 2e-5 maximum learning rate. We use the 2K validation\\ndataset to choose hyper-parameters, and test on the same 3K dataset in the main body of the paper.\\nPosition bias results The results for position bias are provided in Table 15. The consistency'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 28}, page_content='Position bias results The results for position bias are provided in Table 15. The consistency\\nimproves significantly from 16.2% to 65.0%. Due to the classification formulation, every output is\\nrecognizable (error rate 0%). In addition, we measure the classification accuracy over the test dataset.\\nAgreement results It achieves 56.8% when including all three labels, and 85.5% when excluding\\ntie predictions and labels, significantly outperforming random guesses of 33% and 50% respectively,'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 28}, page_content='tie predictions and labels, significantly outperforming random guesses of 33% and 50% respectively,\\nand show positive signals to match GPT-4 (66% and 87% respectively). In conclusion, a further\\nfine-tuned Vicuna-13B model shows strong potential to be used as a cheap open-sourced replacement\\nfor expensive closed-sourced LLMs. A similar conclusion is also found by a concurrent paper[42].\\nTable 15: Position bias of the Vicuna-13B model without and with further fine-tuning. We denote them'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 28}, page_content='Table 15: Position bias of the Vicuna-13B model without and with further fine-tuning. We denote them\\nas Vicuna-13B-Zero-Shot and Vicuna-13B-Fine-Tune respectively. Consistency is the percentage\\nof cases where a judge gives consistent results when swapping the order of two assistants. “Biased\\ntoward first” is the percentage of cases when a judge favors the first answer. “Error” indicates wrong\\noutput formats. The largest number in each column is in bold.'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 28}, page_content='output formats. The largest number in each column is in bold.\\nJudge Prompt Consistency Biased toward first Biased toward second Error\\nVicuna-13B-Zero-Shotdefault 15.0% 53.8% 8.8% 22.5%\\nrename 16.2% 12.5% 40.0% 31.2%\\nscore 11.2% 10.0% 0.0% 78.8%\\nVicuna-13B-Fine-Tune default 65.0% 27.5% 7.5% 0.0%\\n29')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0GRrZWxwrRD",
        "outputId": "3dd062f2-4d2f-4de0-fe2d-e64285c463e2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "249"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split_docs[0].page_content"
      ],
      "metadata": {
        "id": "C5EqeVK5w4Ev"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedding"
      ],
      "metadata": {
        "id": "z6UFcR7ow9--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=OPEN_AI_API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cFKYfYqw_L6",
        "outputId": "c25d21e2-e006-43b1-8f24-65adb668ee0f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-2826641ef09d>:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=OPEN_AI_API_KEY)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "검색 (reranking 미사용)"
      ],
      "metadata": {
        "id": "72t7kBzhynRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "I6_tOyBaxP1C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(model='gpt-4', api_key=OPEN_AI_API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6KttbOrxQ89",
        "outputId": "24480dfc-3919-4347-840f-b4b53d6cf048"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-e01b97a28472>:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  llm = ChatOpenAI(model='gpt-4', api_key=OPEN_AI_API_KEY)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "qa_chain = RetrievalQA.from_chain_type(llm,\n",
        "                                       chain_type=\"stuff\",\n",
        "                                       retriever=retriever) # 벡터 검색 결과 같이 넘겨줌\n",
        "\n",
        "query = \"LLM로 평가를 어떻게 진행해?\"\n",
        "\n",
        "qa_chain_result = qa_chain.run(query)\n",
        "print(qa_chain_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4HlxnpFxr2i",
        "outputId": "757ddf10-33e9-41be-e643-5aeed3963370"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:7: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM을 사용한 평가는 크게 세 가지 방법이 있습니다.\n",
            "\n",
            "1. 'Pairwise comparison' : LLM 판사에게 하나의 질문과 두 가지 답변이 제시되며, 어느 것이 더 나은지 판단하거나 무승부를 선언하도록 요청합니다.\n",
            "\n",
            "2. 'Single answer grading' : LLM 판사가 단일 답변에 직접 점수를 부여하도록 요청하는 방법입니다.\n",
            "\n",
            "3. 'Reference-guided grading' : 특정 경우에는 참조 솔루션을 제공하는 것이 유익할 수 있습니다. \n",
            "\n",
            "이 세 가지 방식은 독립적으로 또는 조합하여 구현할 수 있습니다.\n",
            "CPU times: user 155 ms, sys: 13.9 ms, total: 169 ms\n",
            "Wall time: 8.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reranking 사용 검색"
      ],
      "metadata": {
        "id": "pMvUQFJ1ysEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from config import COHERE_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "RS2_M8hF0JZf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressor = CohereRerank()\n",
        "\n",
        "# 관련 문서 검색\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=retriever      # 파이쓰 결과\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
        "print(compressed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCdo7HLMzmfI",
        "outputId": "ab96f9d7-dfd1-41d1-f9cf-190fed4b280b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-584165713aca>:1: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the langchain-cohere package and should be used instead. To use it run `pip install -U langchain-cohere` and import as `from langchain_cohere import CohereRerank`.\n",
            "  compressor = CohereRerank()\n",
            "<ipython-input-14-584165713aca>:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
            "  compressed_docs = compression_retriever.get_relevant_documents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': '2306.05685v4.pdf', 'page': 3, 'relevance_score': 0.9649213}, page_content='•Pairwise comparison . An LLM judge is presented with a question and two answers, and tasked\\nto determine which one is better or declare a tie. The prompt used is given in Figure 5 (Appendix).\\n•Single answer grading . Alternatively, an LLM judge is asked to directly assign a score to a\\nsingle answer. The prompt used for this scenario is in Figure 6 (Appendix).\\n•Reference-guided grading . In certain cases, it may be beneficial to provide a reference solution'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 1, 'relevance_score': 0.9405718}, page_content='our earlier blog post [ 8] and other concurrent or follow-up work [ 5,29,14,12,52,18,33,40,7,43].\\nHowever, there has not been a systematic study of this approach.\\nIn this paper, we study the LLM-as-a-judge approach by comparing it to the gold standard of\\nhuman evaluation. We examine several potential limitations of the LLM-as-a-judge approach\\nincluding position bias, verbosity bias, self-enhancement bias, and limited reasoning ability. We'), Document(metadata={'source': '2306.05685v4.pdf', 'page': 3, 'relevance_score': 0.93033177}, page_content='20]. Specifically, we are interested in whether LLMs can effectively evaluate the responses of chat\\nassistants and match human preferences. Next, we discuss the use and limitations of LLM-as-a-judge.\\n3.1 Types of LLM-as-a-Judge\\nWe propose 3 LLM-as-a-judge variations. They can be implemented independently or in combination:\\n•Pairwise comparison . An LLM judge is presented with a question and two answers, and tasked')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain_reranked = RetrievalQA.from_chain_type(llm,\n",
        "                                       chain_type=\"stuff\",\n",
        "                                       retriever=compression_retriever)"
      ],
      "metadata": {
        "id": "yv-bax02z9H5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "qa_chain_rerank_result = qa_chain_reranked.run(query)\n",
        "print(qa_chain_rerank_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE6sTn_J3WVp",
        "outputId": "68922b02-46bf-49c7-fb28-2e285e3839e5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM 평가는 세 가지 방법으로 진행할 수 있습니다.\n",
            "\n",
            "1. Pairwise comparison: LLM 판사에게 질문과 두 가지 답변이 제시되며, 어느 것이 더 나은지 판단하거나 무승부를 선언하는 작업을 수행합니다.\n",
            "   \n",
            "2. Single answer grading: 다른 방법으로는, LLM 판사에게 하나의 답변에 직접 점수를 부여하도록 요청할 수 있습니다.\n",
            "\n",
            "3. Reference-guided grading: 특정 경우에는 참조 솔루션을 제공하는 것이 유익할 수 있습니다.\n",
            "\n",
            "이 세 가지 방법은 독립적으로 또는 결합하여 구현할 수 있습니다.\n",
            "CPU times: user 105 ms, sys: 5.31 ms, total: 110 ms\n",
            "Wall time: 7.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과 비교"
      ],
      "metadata": {
        "id": "NFZnS0qZ3zIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_chain_result)\n",
        "print('-'*70)\n",
        "print(qa_chain_rerank_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOeVqahn30jl",
        "outputId": "b0e95c6d-78af-42eb-9c29-a40dec3ec29d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM을 사용한 평가는 크게 세 가지 방법이 있습니다.\n",
            "\n",
            "1. 'Pairwise comparison' : LLM 판사에게 하나의 질문과 두 가지 답변이 제시되며, 어느 것이 더 나은지 판단하거나 무승부를 선언하도록 요청합니다.\n",
            "\n",
            "2. 'Single answer grading' : LLM 판사가 단일 답변에 직접 점수를 부여하도록 요청하는 방법입니다.\n",
            "\n",
            "3. 'Reference-guided grading' : 특정 경우에는 참조 솔루션을 제공하는 것이 유익할 수 있습니다. \n",
            "\n",
            "이 세 가지 방식은 독립적으로 또는 조합하여 구현할 수 있습니다.\n",
            "----------------------------------------------------------------------\n",
            "LLM 평가는 세 가지 방법으로 진행할 수 있습니다.\n",
            "\n",
            "1. Pairwise comparison: LLM 판사에게 질문과 두 가지 답변이 제시되며, 어느 것이 더 나은지 판단하거나 무승부를 선언하는 작업을 수행합니다.\n",
            "   \n",
            "2. Single answer grading: 다른 방법으로는, LLM 판사에게 하나의 답변에 직접 점수를 부여하도록 요청할 수 있습니다.\n",
            "\n",
            "3. Reference-guided grading: 특정 경우에는 참조 솔루션을 제공하는 것이 유익할 수 있습니다.\n",
            "\n",
            "이 세 가지 방법은 독립적으로 또는 결합하여 구현할 수 있습니다.\n"
          ]
        }
      ]
    }
  ]
}